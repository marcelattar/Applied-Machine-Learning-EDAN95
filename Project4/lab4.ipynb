{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a Dataset\n",
    "1. You will use a dataset from the CoNLL conferences that benchmark natural language processing systems and tasks. There were two conferences on named entity recognition: CoNLL 2002 (Spanish and Dutch) and CoNLL 2003 (English and German). In this assignment, you will work on the English dataset. Read the description of the task.\n",
    "2. The datasets are protected by a license and you need to obtain it to reconstruct the data. Alternatively, you can use a local copy or try to find one on github (type conll2003 in the search box) or use the Google dataset search: https://toolbox.google.com/datasetsearch. You can find a local copy in the /usr/local/cs/EDAN95/datasets/NER-data folder.\n",
    "3. The dataset comes in the form of three files: a training set, a development set, and a test set. You will use the test set to evaluate your models. For this, you will apply the conlleval script that will compute the harmonic mean of the precision and recall: F1. You have a local copy of this script in /usr/local/cs/EDAN95/datasets/ner/bin. conlleval is written in Perl. Be sure to have it on your machine to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files in directory conll2003, train.txt, valid.txt and test.txt\n",
    "# https://github.com/ningshixian/NER-CONLL2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Embeddings\n",
    "1. Download the GloVe embeddings 6B from https://nlp.stanford.edu/projects/glove/ and keep the 100d vectors.\n",
    "2. Write a function that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embeddings.\n",
    "3. Using a cosine similarity, compute the 5 closest words to the words table, france, and sweden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "glove_dir = '/Users/Marcel/Documents/Python/edan95/project_4/glove.6b'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.strip().split()\n",
    "    word = values[0]\n",
    "    vector = np.array(values[1:], dtype='float32') \n",
    "    embeddings_index[word] = vector\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to run this in order for my kernel not to crash\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france\n",
      "[('belgium', 0.8076423), ('french', 0.8004377), ('britain', 0.79505277), ('spain', 0.7557464), ('paris', 0.74815863)]\n",
      "sweden\n",
      "[('denmark', 0.8624401), ('norway', 0.80732495), ('finland', 0.7906495), ('netherlands', 0.74684644), ('austria', 0.74668366)]\n",
      "table\n",
      "[('tables', 0.80211616), ('place', 0.6582379), ('bottom', 0.65597206), ('room', 0.65436906), ('side', 0.6433667)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "def top5(word, embd):\n",
    "    cdict = {}\n",
    "    for w in embd:\n",
    "        cdict[w] = np.dot(embd[w],embd[word])/(np.linalg.norm(embd[w])*np.linalg.norm(embd[word]))\n",
    "    sorted_dict = sorted(cdict.items(), key = operator.itemgetter(1),reverse=True)\n",
    "    return sorted_dict[1:6]\n",
    "\n",
    "words =['france','sweden','table']\n",
    "for w in words:\n",
    "    print(w)\n",
    "    print(top5(w,embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Corpus and Building Indices\n",
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries.\n",
    "1. Write a function that for each sentence returns the X and Y lists of symbols consisting of words and NER tags.\n",
    "2. Create a vocabulary of all the words observed in the training set and the words in GloVe.\n",
    "3. Create indices and inverted indices for the words and the NER: i.e. you will associate each word with a number. You will use index 0 for the padding symbol and 1 for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/Marcel/Documents/Python/edan95/project_4/conll003-englishversion/'\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + 'train.txt'\n",
    "    dev_file = BASE_DIR + 'valid.txt'\n",
    "    test_file = BASE_DIR + 'test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "import re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "print(train_dict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='pos', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = [word[key_x] for word in sentence]\n",
    "        y = [word[key_y] for word in sentence]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "First sentence, NER ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Build the words and NER sequence tags\n",
    "X_words, Y_ner = build_sequences(train_dict, key_x='form', key_y='ner')\n",
    "print('First sentence, words', X_words[1])\n",
    "print('First sentence, NER', Y_ner[1])\n",
    "# Extract the list of unique words and NER and vocab including glove \n",
    "word_set = sorted(list(set([item for sublist in X_words for item in sublist])))\n",
    "ner_set = sorted(list(set([item for sublist in Y_ner for item in sublist])))\n",
    "\n",
    "glove_set = sorted([key for key in embeddings_index.keys()])\n",
    "vocab = sorted(list(set(glove_set + word_set)))\n",
    "\n",
    "# Building the indices \n",
    "rev_word_idx = dict(enumerate(vocab, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner_set, start=2))\n",
    "rev_word_idx[0]=0\n",
    "rev_word_idx[1]='-unknown-'\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the words and NER sequence tags \n",
    "X_words_dev, Y_ner_dev = build_sequences(dev_dict, key_x='form', key_y='ner')\n",
    "\n",
    "# Extract the list of unique words and NER and vocab including glove \n",
    "word_set_dev = sorted(list(set([item for sublist in X_words_dev for item in sublist])))\n",
    "ner_set_dev = sorted(list(set([item for sublist in Y_ner_dev for item in sublist])))\n",
    "\n",
    "# Building the indices \n",
    "rev_word_idx_dev = dict(enumerate(vocab, start=2))\n",
    "rev_ner_idx_dev = dict(enumerate(ner_set_dev, start=2))\n",
    "rev_word_idx_dev[0]=0\n",
    "rev_word_idx_dev[1]='-unknown-'\n",
    "word_idx_dev = {v: k for k, v in rev_word_idx_dev.items()}\n",
    "ner_idx_dev = {v: k for k, v in rev_ner_idx_dev.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Embedding Matrix\n",
    "1. Create a matrix of dimensions (M, N), where M, will the size of the vocabulary: The unique words in the training set and the words in GloVe, and N, the dimension of the embeddings.\n",
    "The padding symbol and the unknown word symbol will be part of the vocabulary.\n",
    "The shape of your matrix should be: (402597, 100). Initialize it with random values.\n",
    "2. Fill the matrix with the GloVe embeddings. You will use the indices from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words=len(rev_word_idx.keys())\n",
    "embedding_dim=100\n",
    "embedding_matrix = np.random.rand(max_words, embedding_dim)*3.575#max value\n",
    "for word, i in word_idx.items():\n",
    "    embedding_vector = embeddings_index.get(word) \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the X and Y Sequences\n",
    "You will now create the input and output sequences with numerical indices\n",
    "1. Convert the X and Y list of symbols in a list of numbers using the indices you created.\n",
    "2. Pad the sentences using the pad_sequences function.\n",
    "3. Do the same for the development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have symols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "First sentence, NER ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, words', X_words[1])\n",
    "print('First sentence, NER', Y_ner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create sequences of numbers, let us convert them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_idx = [list(map(lambda x: word_idx.get(x, 1), x)) for x in X_words]\n",
    "Y_ner_idx = [list(map(lambda x: ner_idx.get(x, 1), x)) for x in Y_ner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words [142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936]\n",
      "First sentence, NER [4, 10, 3, 10, 10, 10, 3, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, words', X_words_idx[1])\n",
    "print('First sentence, NER', Y_ner_idx[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, good! Now we just need to pad the sentences so that all sentences have the same length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 150\n",
    "X_words_idx = pad_sequences(X_words_idx,maxlen=maxlen)\n",
    "Y_ner_idx = pad_sequences(Y_ner_idx,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words [     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0 142143 307143 161836  91321 363368  83766  85852 218260    936]\n",
      "First sentence, NER [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4 10  3\n",
      " 10 10 10  3 10 10]\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, words', X_words_idx[1])\n",
    "print('First sentence, NER', Y_ner_idx[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_idx_dev = [list(map(lambda x: word_idx_dev.get(x, 1), x)) for x in X_words_dev]\n",
    "Y_ner_idx_dev = [list(map(lambda x: ner_idx_dev.get(x, 1), x)) for x in Y_ner_dev]\n",
    "X_words_idx_dev = pad_sequences(X_words_idx_dev,maxlen=maxlen)\n",
    "Y_ner_idx_dev = pad_sequences(Y_ner_idx_dev,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also convert Y to categorical values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "Y_ner_idx_cat = to_categorical(Y_ner_idx)\n",
    "Y_ner_idx_dev_cat = to_categorical(Y_ner_idx_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Recurrent Neural Network\n",
    "1. Create a simple recurrent network and train a model with the train set. As layers, you will use Embedding, SimpleRNN, and Dense.\n",
    "2. Compile and fit your network. You will report the training and validation losses and accuracies and comment on the possible overfit.\n",
    "3. Apply your network to the test set and report the accuracy you obtained. You will use the evaluate method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output tolkar jag det som att det är NER tag, det finns olika tags, 0=inte NER, och I-XXX = NER, där XXX kan vara organisation, person eller plats eller andra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_vocab_size=len(ner_idx.keys())+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_vocabulary_size\t 402597\n",
      "embedding_dim\t\t 100\n",
      "maxlen\t\t\t 150\n",
      "ner_vocab_size\t\t 11\n",
      "X\t\t\t (14987, 150)\n",
      "Y\t\t\t (14987, 150)\n",
      "X_val\t\t\t (3466, 150)\n",
      "Y_val\t\t\t (3466, 150)\n"
     ]
    }
   ],
   "source": [
    "text_vocabulary_size = len(vocab) + 2\n",
    "print('text_vocabulary_size\\t',text_vocabulary_size)\n",
    "print('embedding_dim\\t\\t',embedding_dim)\n",
    "print('maxlen\\t\\t\\t',maxlen)\n",
    "print('ner_vocab_size\\t\\t',ner_vocab_size)\n",
    "print('X\\t\\t\\t',X_words_idx.shape)\n",
    "print('Y\\t\\t\\t',Y_ner_idx.shape)\n",
    "print('X_val\\t\\t\\t',X_words_idx_dev.shape)\n",
    "print('Y_val\\t\\t\\t',Y_ner_idx_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 64)           8512      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150, 11)           715       \n",
      "=================================================================\n",
      "Total params: 40,268,927\n",
      "Trainable params: 9,227\n",
      "Non-trainable params: 40,259,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, SimpleRNN,Bidirectional\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "# input här kommer vara emb_mat som vi lägger till som vikter i emb_lay och fryser så att de inte kan förändras\n",
    "model.add(Embedding(text_vocabulary_size,embedding_dim,input_length=maxlen,mask_zero=False))\n",
    "model.layers[0].set_weights([embedding_matrix]) \n",
    "model.layers[0].trainable = False\n",
    "# output blir 150 x 100\n",
    "\n",
    "model.add(Bidirectional(SimpleRNN(32,return_sequences=True)))\n",
    "model.add(Dense(ner_vocab_size, activation='softmax')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/3\n",
      "14987/14987 [==============================] - 40s 3ms/step - loss: 0.1278 - acc: 0.9650 - val_loss: 0.0439 - val_acc: 0.9881\n",
      "Epoch 2/3\n",
      "14987/14987 [==============================] - 37s 2ms/step - loss: 0.0298 - acc: 0.9919 - val_loss: 0.0267 - val_acc: 0.9924\n",
      "Epoch 3/3\n",
      "14987/14987 [==============================] - 37s 2ms/step - loss: 0.0217 - acc: 0.9938 - val_loss: 0.0221 - val_acc: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13c28b9b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_words_idx, Y_ner_idx_cat,\n",
    "          epochs=3, \n",
    "          batch_size=128,\n",
    "          validation_data=(X_words_idx_dev, Y_ner_idx_dev_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your System\n",
    "You will use the official script to evaluate the performance of your system\n",
    "1. Use the predict method to predict the tags of the whole test set\n",
    "2. Write your results in a file, where the two last columns will be the hand-annotated tag and the predicted tag. The fields must be separated by a space.\n",
    "3. Apply conlleval to your output. Report the F1 result.\n",
    "4. Try to improve your model by modifying some parameters, adding layers, adding Bidirectional and Dropout.\n",
    "5. Evaluate your network again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprossesing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = conll_dict.transform(test_sentences)\n",
    "X_words, Y_ner = build_sequences(test_dict, key_x='form', key_y='ner')\n",
    "X_words_test, Y_ner_test = build_sequences(test_dict, key_x='form', key_y='ner')\n",
    "\n",
    "# Extract the list of unique words and NER and vocab including glove \n",
    "word_set_test = sorted(list(set([item for sublist in X_words_test for item in sublist])))\n",
    "ner_set_test = sorted(list(set([item for sublist in Y_ner_test for item in sublist])))\n",
    "\n",
    "# Building the indices \n",
    "rev_word_idx_test = dict(enumerate(vocab, start=2))\n",
    "rev_ner_idx_test = dict(enumerate(ner_set_test, start=2))\n",
    "rev_word_idx_test[0]=0\n",
    "rev_word_idx_test[1]='-unknown-'\n",
    "word_idx_test = {v: k for k, v in rev_word_idx_test.items()}\n",
    "ner_idx_test = {v: k for k, v in rev_ner_idx_test.items()}\n",
    "\n",
    "# Converting sequences to indicies\n",
    "X_words_idx_test = [list(map(lambda x: word_idx_test.get(x, 1), x)) for x in X_words_test]\n",
    "X_words_idx_test = pad_sequences(X_words_idx_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_words_idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert predicted (3D matrix) to NER tags for each sequence in 3 steps:\n",
    "1. Convert probabilities to NER index\n",
    "2. Remove padding\n",
    "3. Convert index to NER tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_output(predicted, ner_idx,X_words_test,Y_ner_test,filename):\n",
    "    Y_out_pad= np.argmax(predicted,axis=2)\n",
    "    inv_ner_idx = {v: k for k, v in ner_idx.items()}\n",
    "    Y_out = []\n",
    "    inv_ner_idx[0]='O'\n",
    "    inv_ner_idx[1]='wtf'\n",
    "    for i in range(len(Y_out_pad)):\n",
    "        temp_old = Y_out_pad[i][-(len(X_words_test[i])):]\n",
    "        temp_new = []\n",
    "        for j in temp_old:\n",
    "            temp_new.append(inv_ner_idx[j])\n",
    "        Y_out.append(temp_new)\n",
    "\n",
    "    f_out = open(filename, 'w')\n",
    "    for i in range(len(X_words_test)): # For each sentence\n",
    "        for j in range(len(X_words_test[i])): # Fore each word\n",
    "            word = X_words_test[i][j]\n",
    "            NER = Y_ner_test[i][j]\n",
    "            PNER = Y_out[i][j]\n",
    "            f_out.write(word + ' ' + NER + ' ' + PNER + '\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()\n",
    "    return Y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5648 phrases; found: 5565 phrases; correct: 3792.\n",
      "accuracy:  93.33%; precision:  68.14%; recall:  67.14%; FB1:  67.64\n",
      "              LOC: precision:  70.46%; recall:  77.64%; FB1:  73.87  1838\n",
      "             MISC: precision:  60.74%; recall:  51.14%; FB1:  55.53  591\n",
      "              ORG: precision:  59.10%; recall:  56.11%; FB1:  57.57  1577\n",
      "              PER: precision:  77.36%; recall:  74.58%; FB1:  75.94  1559\n"
     ]
    }
   ],
   "source": [
    "Y_new = creat_output(predicted, ner_idx,X_words_test,Y_ner_test,'new_out')\n",
    "!perl ./conlleval.pl <new_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a LSTM Network\n",
    "1. Create a simple LSTM network and train a model with the train set. As layers, you will use Embedding, LSTM, and Dense.\n",
    "2. Apply conlleval to your output. Report the F1 result.\n",
    "3. Try to improve your model by modifying some parameters, adding layers, adding Bidirectional, Dropout, possibly mixing SimpleRNN.\n",
    "4. Apply your network to the test set and report the accuracy you obtained. you need to reach a F1 of 82 to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 100)          40259700  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 150, 200)          160800    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 150, 200)          60200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150, 200)          40200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150, 11)           2211      \n",
      "=================================================================\n",
      "Total params: 40,523,111\n",
      "Trainable params: 263,411\n",
      "Non-trainable params: 40,259,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dropout\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "# input här kommer vara emb_mat som vi lägger till som vikter i emb_lay och fryser så att de inte kan förändras\n",
    "model.add(Embedding(text_vocabulary_size,embedding_dim,input_length=maxlen,mask_zero=False))\n",
    "# output blir 150 x 100\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(SimpleRNN(100,return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(200, activation='relu')) \n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(ner_vocab_size, activation='softmax')) \n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix]) \n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/15\n",
      "14987/14987 [==============================] - 330s 22ms/step - loss: 0.0826 - acc: 0.9785 - val_loss: 0.0333 - val_acc: 0.9907\n",
      "Epoch 2/15\n",
      "14987/14987 [==============================] - 318s 21ms/step - loss: 0.0221 - acc: 0.9936 - val_loss: 0.0183 - val_acc: 0.9948\n",
      "Epoch 3/15\n",
      "14987/14987 [==============================] - 305s 20ms/step - loss: 0.0143 - acc: 0.9957 - val_loss: 0.0192 - val_acc: 0.9946\n",
      "Epoch 4/15\n",
      "14987/14987 [==============================] - 348s 23ms/step - loss: 0.0111 - acc: 0.9966 - val_loss: 0.0157 - val_acc: 0.9954\n",
      "Epoch 5/15\n",
      "14987/14987 [==============================] - 343s 23ms/step - loss: 0.0094 - acc: 0.9971 - val_loss: 0.0129 - val_acc: 0.9961\n",
      "Epoch 6/15\n",
      "14987/14987 [==============================] - 342s 23ms/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0113 - val_acc: 0.9969\n",
      "Epoch 7/15\n",
      "14987/14987 [==============================] - 4978s 332ms/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0204 - val_acc: 0.9932\n",
      "Epoch 8/15\n",
      "14987/14987 [==============================] - 313s 21ms/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0123 - val_acc: 0.9963\n",
      "Epoch 9/15\n",
      "14987/14987 [==============================] - 308s 21ms/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.0112 - val_acc: 0.9973\n",
      "Epoch 10/15\n",
      "14987/14987 [==============================] - 344s 23ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0142 - val_acc: 0.9958\n",
      "Epoch 11/15\n",
      "14987/14987 [==============================] - 325s 22ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0121 - val_acc: 0.9968\n",
      "Epoch 12/15\n",
      "14987/14987 [==============================] - 325s 22ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0146 - val_acc: 0.9969\n",
      "Epoch 13/15\n",
      "14987/14987 [==============================] - 335s 22ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0142 - val_acc: 0.9972\n",
      "Epoch 14/15\n",
      "14987/14987 [==============================] - 326s 22ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0139 - val_acc: 0.9973\n",
      "Epoch 15/15\n",
      "14987/14987 [==============================] - 314s 21ms/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0152 - val_acc: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e0df198>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc']) \n",
    "model.fit(X_words_idx, Y_ner_idx_cat,\n",
    "          epochs=15, \n",
    "          batch_size=128,\n",
    "          validation_data=(X_words_idx_dev, Y_ner_idx_dev_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_words_idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_new = creat_output(predicted, ner_idx,X_words_test,Y_ner_test,'BILSTM100_BISRNN100_200_DO_8ep_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5648 phrases; found: 5673 phrases; correct: 4657.\n",
      "accuracy:  96.30%; precision:  82.09%; recall:  82.45%; FB1:  82.27\n",
      "              LOC: precision:  87.58%; recall:  86.27%; FB1:  86.92  1643\n",
      "             MISC: precision:  69.23%; recall:  65.38%; FB1:  67.25  663\n",
      "              ORG: precision:  73.96%; recall:  78.51%; FB1:  76.17  1763\n",
      "              PER: precision:  90.71%; recall:  89.98%; FB1:  90.34  1604\n"
     ]
    }
   ],
   "source": [
    "!perl ./conlleval.pl <BILSTM100_BISRNN100_200_DO_8ep_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up keras checkpoint to save weights at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
