{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.layers import SimpleRNN, Bidirectional, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the Embeddings\n",
    "\n",
    "1. Download the GloVe embeddings 6B from https://nlp.stanford.edu/projects/glove/ and keep the 100d vectors.\n",
    "2. Write a function that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embeddings.\n",
    "3. Using a cosine similarity, compute the 5 closest words to the words table, france, and sweden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 6.10 Parsing the GloVe word-embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '/Users/madeleinejansson/Documents/AppliedML/A4/glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4359    -0.04961    0.31388    0.6056     0.36372   -0.03974\n",
      "  0.42844    0.58571   -0.0048055 -0.24868   -0.069374   0.036042\n",
      " -0.68352    0.55531   -0.36235    0.13997    0.020455  -0.82498\n",
      " -0.68761    0.28685    0.37247    0.62814    0.45655    0.11955\n",
      "  1.087     -1.3191     0.1611    -0.37508    1.1136    -0.8442\n",
      " -0.78734   -0.61974   -0.48193   -0.76358    0.55343    0.80071\n",
      "  0.53818   -0.13143   -1.2196    -0.10605   -0.60646   -0.45015\n",
      "  0.34853   -0.49596    0.31408    0.24088    0.28625   -0.11539\n",
      "  0.40018   -0.94102    0.30618    1.3605    -0.75011    1.3509\n",
      " -1.1497    -2.6059    -0.078406   0.09977    0.93239    0.15718\n",
      "  1.0857    -0.52454    0.14787   -0.61679    0.49391    0.38589\n",
      " -1.264      0.78695    0.13566    0.22713    0.49846   -0.25495\n",
      " -0.88281   -0.17737    0.50366   -0.094379   0.65361   -0.14926\n",
      " -0.79488    0.42061    0.92563   -0.5842    -0.21701    0.56262\n",
      " -1.0399    -0.97772   -0.34684    0.58105   -0.38675   -0.42649\n",
      " -0.35594   -0.45715    0.46549    0.64176   -0.89077    1.021\n",
      " -0.76028   -0.59967    0.10388   -0.45569  ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index['france'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cosine similiarity between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    \n",
    "    den = np.dot(v1, v2)\n",
    "    num = np.linalg.norm(v1)*np.linalg.norm(v2)\n",
    "    return den/num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(embeddings_index['france'], embeddings_index['france'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts the five most similar words to a given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_words(word, dictionary):\n",
    "    compared_dic = {}\n",
    "    for i in dictionary.keys():\n",
    "        if i == word:\n",
    "            continue\n",
    "        angle = cos_sim(dictionary[word], dictionary[i])\n",
    "        compared_dic[i] = angle\n",
    "    \n",
    "    sorted_dic = sorted(compared_dic.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_dic[0:5] \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('belgium', 0.8076423),\n",
       " ('french', 0.8004377),\n",
       " ('britain', 0.79505277),\n",
       " ('spain', 0.7557464),\n",
       " ('paris', 0.74815863)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_words('france', embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tables', 0.80211616),\n",
       " ('place', 0.6582379),\n",
       " ('bottom', 0.65597206),\n",
       " ('room', 0.65436906),\n",
       " ('side', 0.6433667)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_words('table', embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('denmark', 0.8624401),\n",
       " ('norway', 0.80732495),\n",
       " ('finland', 0.7906495),\n",
       " ('netherlands', 0.74684644),\n",
       " ('austria', 0.74668366)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_words('sweden', embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Corpus and Building Indices\n",
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries.\n",
    "\n",
    "1. Write a function that for each sentence returns the x and y lists of symbols consisting of words and NER tags.\n",
    "2. Apply this function to your datasets so that you create X and Y lists of lists consisting of words and NER tags\n",
    "3. Create a vocabulary of all the words observed in the training set and the words in GloVe.\n",
    "4. Create indices and inverted indices for the words and the NER: i.e. you will associate each word with a number. You will use index 0 for the padding symbol and 1 for unknown words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/madeleinejansson/Documents/AppliedML/A4/conll003-englishversion/'\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + 'train.txt'\n",
    "    dev_file = BASE_DIR + 'valid.txt'\n",
    "    test_file = BASE_DIR + 'test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictorizer that transforms the CoNLL files into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the CoNLL2003 data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[0])\n",
    "print(train_dict[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='pos', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = [word[key_x] for word in sentence]\n",
    "        y = [word[key_y] for word in sentence]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the words and NER sequence tags (collner 2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words, Y_ner = build_sequences(train_dict, key_x='form', key_y='ner')\n",
    "\n",
    "X_words_test, Y_ner_test = build_sequences(test_dict, key_x='form', key_y='ner')\n",
    "\n",
    "X_dev_words, Y_dev_ner = build_sequences(dev_dict, key_x='form', key_y='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "First sentence, NER ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, words', X_words[1])\n",
    "print('First sentence, NER', Y_ner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the list of unique words and NER (collner 2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = sorted(list(set([item for sublist in X_words for item in sublist])))\n",
    "ner_set = sorted(list(set([item for sublist in Y_ner for item in sublist])))\n",
    "\n",
    "word_set_test = sorted(list(set([item for sublist in X_words_test for item in sublist])))\n",
    "ner_set_test = sorted(list(set([item for sublist in Y_ner_test for item in sublist])))\n",
    "\n",
    "word_set_dev = sorted(list(set([item for sublist in X_dev_words for item in sublist])))\n",
    "ner_set_dev = sorted(list(set([item for sublist in Y_dev_ner for item in sublist])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "21010\n"
     ]
    }
   ],
   "source": [
    "print(len(ner_set))\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the keys that are words (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402595"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_set = sorted([key for key in embeddings_index.keys()]) \n",
    "\n",
    "glove_set.extend(word_set)\n",
    "voc = sorted(set(glove_set))\n",
    "\n",
    "rev_voc_idx = dict(enumerate(voc, start=2))\n",
    "\n",
    "voc_idx = {v: k for k, v in rev_voc_idx.items()}\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the indices, pairing padding to 0 and unknown to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "rev_word_idx = dict(enumerate(voc, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner_set, start=2))\n",
    "\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "\n",
    "# Test\n",
    "rev_word_idx_test = dict(enumerate(voc, start=2))\n",
    "rev_ner_idx_test = dict(enumerate(ner_set_test, start=2))\n",
    "\n",
    "word_idx_test = {v: k for k, v in rev_word_idx_test.items()}\n",
    "ner_idx_test = {v: k for k, v in rev_ner_idx_test.items()}\n",
    "\n",
    "# Development\n",
    "rev_word_idx_dev = dict(enumerate(voc, start=2))\n",
    "rev_ner_idx_dev = dict(enumerate(ner_set_dev, start=2))\n",
    "\n",
    "word_idx_dev = {v: k for k, v in rev_word_idx_dev.items()}\n",
    "ner_idx_dev = {v: k for k, v in rev_ner_idx_dev.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 2,\n",
       " '!!': 3,\n",
       " '!!!': 4,\n",
       " '!!!!': 5,\n",
       " '!!!!!': 6,\n",
       " '!?': 7,\n",
       " '!?!': 8,\n",
       " '\"': 9,\n",
       " '#': 10,\n",
       " '##': 11,\n",
       " '###': 12,\n",
       " '#a': 13,\n",
       " '#aabccc': 14,\n",
       " '#b': 15,\n",
       " '#c': 16,\n",
       " '#cc': 17,\n",
       " '#ccc': 18,\n",
       " '#cccccc': 19,\n",
       " '#ccccff': 20,\n",
       " '#d': 21,\n",
       " '#daa': 22,\n",
       " '#dcdcdc': 23,\n",
       " '#e': 24,\n",
       " '#f': 25,\n",
       " '#faf': 26,\n",
       " '#ff': 27,\n",
       " '#ffffff': 28,\n",
       " '#m': 29,\n",
       " '#p': 30,\n",
       " '#s': 31,\n",
       " '#the': 32,\n",
       " '#ukqa': 33,\n",
       " '#ukqaqfqs': 34,\n",
       " '#ukqaqs': 35,\n",
       " '#ukqaqsqf': 36,\n",
       " '#ukqaqsqm': 37,\n",
       " '#ukqaqtqszbzszr': 38,\n",
       " '#ukqec': 39,\n",
       " '#ukqeqtqszb': 40,\n",
       " '$': 41,\n",
       " '%': 42,\n",
       " '&': 43,\n",
       " '&#8211;': 44,\n",
       " '&#8217;': 45,\n",
       " '&#8220;': 46,\n",
       " '&#8221;': 47,\n",
       " '&amp;': 48,\n",
       " \"'\": 49,\n",
       " \"''\": 50,\n",
       " \"'00\": 51,\n",
       " \"'01\": 52,\n",
       " \"'02\": 53,\n",
       " \"'03\": 54,\n",
       " \"'04\": 55,\n",
       " \"'05\": 56,\n",
       " \"'06\": 57,\n",
       " \"'07\": 58,\n",
       " \"'08\": 59,\n",
       " \"'09\": 60,\n",
       " \"'10\": 61,\n",
       " \"'11\": 62,\n",
       " \"'12\": 63,\n",
       " \"'13\": 64,\n",
       " \"'14\": 65,\n",
       " \"'15\": 66,\n",
       " \"'20\": 67,\n",
       " \"'20s\": 68,\n",
       " \"'25\": 69,\n",
       " \"'27\": 70,\n",
       " \"'28\": 71,\n",
       " \"'29\": 72,\n",
       " \"'30\": 73,\n",
       " \"'30s\": 74,\n",
       " \"'32\": 75,\n",
       " \"'34\": 76,\n",
       " \"'36\": 77,\n",
       " \"'37\": 78,\n",
       " \"'38\": 79,\n",
       " \"'39\": 80,\n",
       " \"'40\": 81,\n",
       " \"'40s\": 82,\n",
       " \"'41\": 83,\n",
       " \"'42\": 84,\n",
       " \"'44\": 85,\n",
       " \"'45\": 86,\n",
       " \"'46\": 87,\n",
       " \"'47\": 88,\n",
       " \"'48\": 89,\n",
       " \"'49\": 90,\n",
       " \"'50\": 91,\n",
       " \"'50s\": 92,\n",
       " \"'51\": 93,\n",
       " \"'52\": 94,\n",
       " \"'53\": 95,\n",
       " \"'54\": 96,\n",
       " \"'55\": 97,\n",
       " \"'56\": 98,\n",
       " \"'57\": 99,\n",
       " \"'58\": 100,\n",
       " \"'59\": 101,\n",
       " \"'60\": 102,\n",
       " \"'60s\": 103,\n",
       " \"'61\": 104,\n",
       " \"'62\": 105,\n",
       " \"'63\": 106,\n",
       " \"'64\": 107,\n",
       " \"'65\": 108,\n",
       " \"'66\": 109,\n",
       " \"'67\": 110,\n",
       " \"'68\": 111,\n",
       " \"'69\": 112,\n",
       " \"'70\": 113,\n",
       " \"'70s\": 114,\n",
       " \"'71\": 115,\n",
       " \"'72\": 116,\n",
       " \"'73\": 117,\n",
       " \"'74\": 118,\n",
       " \"'75\": 119,\n",
       " \"'76\": 120,\n",
       " \"'77\": 121,\n",
       " \"'78\": 122,\n",
       " \"'79\": 123,\n",
       " \"'80\": 124,\n",
       " \"'80s\": 125,\n",
       " \"'81\": 126,\n",
       " \"'82\": 127,\n",
       " \"'83\": 128,\n",
       " \"'84\": 129,\n",
       " \"'85\": 130,\n",
       " \"'86\": 131,\n",
       " \"'87\": 132,\n",
       " \"'88\": 133,\n",
       " \"'89\": 134,\n",
       " \"'90\": 135,\n",
       " \"'90s\": 136,\n",
       " \"'91\": 137,\n",
       " \"'92\": 138,\n",
       " \"'93\": 139,\n",
       " \"'94\": 140,\n",
       " \"'95\": 141,\n",
       " \"'96\": 142,\n",
       " \"'97\": 143,\n",
       " \"'98\": 144,\n",
       " \"'99\": 145,\n",
       " \"'cause\": 146,\n",
       " \"'d\": 147,\n",
       " \"'em\": 148,\n",
       " \"'ll\": 149,\n",
       " \"'m\": 150,\n",
       " \"'n\": 151,\n",
       " \"'n'\": 152,\n",
       " \"'re\": 153,\n",
       " \"'s\": 154,\n",
       " \"'til\": 155,\n",
       " \"'till\": 156,\n",
       " \"'twas\": 157,\n",
       " \"'ve\": 158,\n",
       " '(': 159,\n",
       " '(02)': 160,\n",
       " '(020)': 161,\n",
       " '(026)': 162,\n",
       " '(029)': 163,\n",
       " '(171)': 164,\n",
       " '(201)': 165,\n",
       " '(202)': 166,\n",
       " '(203)': 167,\n",
       " '(205)': 168,\n",
       " '(206)': 169,\n",
       " '(207)': 170,\n",
       " '(208)': 171,\n",
       " '(209)': 172,\n",
       " '(210)': 173,\n",
       " '(212)': 174,\n",
       " '(212)556-3622': 175,\n",
       " '(212)556-7652': 176,\n",
       " '(213)': 177,\n",
       " '(214)': 178,\n",
       " '(215)': 179,\n",
       " '(216)': 180,\n",
       " '(239)': 181,\n",
       " '(248)': 182,\n",
       " '(250)': 183,\n",
       " '(301)': 184,\n",
       " '(303)': 185,\n",
       " '(305)': 186,\n",
       " '(307)': 187,\n",
       " '(310)': 188,\n",
       " '(312)': 189,\n",
       " '(313)': 190,\n",
       " '(314)': 191,\n",
       " '(315)': 192,\n",
       " '(316)': 193,\n",
       " '(318)': 194,\n",
       " '(323)': 195,\n",
       " '(352)': 196,\n",
       " '(360)': 197,\n",
       " '(401)': 198,\n",
       " '(403)': 199,\n",
       " '(404)': 200,\n",
       " '(405)': 201,\n",
       " '(406)': 202,\n",
       " '(407)': 203,\n",
       " '(408)': 204,\n",
       " '(410)': 205,\n",
       " '(412)': 206,\n",
       " '(413)': 207,\n",
       " '(414)': 208,\n",
       " '(415)': 209,\n",
       " '(416)': 210,\n",
       " '(418)': 211,\n",
       " '(419)': 212,\n",
       " '(435)': 213,\n",
       " '(45)': 214,\n",
       " '(480)': 215,\n",
       " '(502)': 216,\n",
       " '(503)': 217,\n",
       " '(504)': 218,\n",
       " '(505)': 219,\n",
       " '(508)': 220,\n",
       " '(510)': 221,\n",
       " '(512)': 222,\n",
       " '(513)': 223,\n",
       " '(514)': 224,\n",
       " '(516)': 225,\n",
       " '(518)': 226,\n",
       " '(520)': 227,\n",
       " '(525)': 228,\n",
       " '(530)': 229,\n",
       " '(540)': 230,\n",
       " '(541)': 231,\n",
       " '(559)': 232,\n",
       " '(561)': 233,\n",
       " '(562)': 234,\n",
       " '(573)': 235,\n",
       " '(602)': 236,\n",
       " '(603)': 237,\n",
       " '(604)': 238,\n",
       " '(607)': 239,\n",
       " '(609)': 240,\n",
       " '(610)': 241,\n",
       " '(612)': 242,\n",
       " '(613)': 243,\n",
       " '(615)': 244,\n",
       " '(617)': 245,\n",
       " '(619)': 246,\n",
       " '(626)': 247,\n",
       " '(631)': 248,\n",
       " '(65)': 249,\n",
       " '(650)': 250,\n",
       " '(651)': 251,\n",
       " '(661)': 252,\n",
       " '(702)': 253,\n",
       " '(703)': 254,\n",
       " '(704)': 255,\n",
       " '(707)': 256,\n",
       " '(708)': 257,\n",
       " '(713)': 258,\n",
       " '(714)': 259,\n",
       " '(716)': 260,\n",
       " '(717)': 261,\n",
       " '(718)': 262,\n",
       " '(719)': 263,\n",
       " '(727)': 264,\n",
       " '(727)893-8241': 265,\n",
       " '(732)': 266,\n",
       " '(734)': 267,\n",
       " '(757)': 268,\n",
       " '(760)': 269,\n",
       " '(770)': 270,\n",
       " '(773)': 271,\n",
       " '(775)': 272,\n",
       " '(787)': 273,\n",
       " '(800)': 274,\n",
       " '(801)': 275,\n",
       " '(802)': 276,\n",
       " '(803)': 277,\n",
       " '(804)': 278,\n",
       " '(805)': 279,\n",
       " '(806)': 280,\n",
       " '(808)': 281,\n",
       " '(809)': 282,\n",
       " '(810)': 283,\n",
       " '(813)': 284,\n",
       " '(816)': 285,\n",
       " '(817)': 286,\n",
       " '(818)': 287,\n",
       " '(828)': 288,\n",
       " '(830)': 289,\n",
       " '(831)': 290,\n",
       " '(843)': 291,\n",
       " '(845)': 292,\n",
       " '(847)': 293,\n",
       " '(850)': 294,\n",
       " '(852)': 295,\n",
       " '(858)': 296,\n",
       " '(860)': 297,\n",
       " '(863)': 298,\n",
       " '(864)': 299,\n",
       " '(866)': 300,\n",
       " '(877)': 301,\n",
       " '(888)': 302,\n",
       " '(900)': 303,\n",
       " '(901)': 304,\n",
       " '(903)': 305,\n",
       " '(904)': 306,\n",
       " '(907)': 307,\n",
       " '(908)': 308,\n",
       " '(909)': 309,\n",
       " '(912)': 310,\n",
       " '(913)': 311,\n",
       " '(914)': 312,\n",
       " '(915)': 313,\n",
       " '(916)': 314,\n",
       " '(919)': 315,\n",
       " '(925)': 316,\n",
       " '(928)': 317,\n",
       " '(937)': 318,\n",
       " '(941)': 319,\n",
       " '(949)': 320,\n",
       " '(954)': 321,\n",
       " '(970)': 322,\n",
       " '(972)': 323,\n",
       " '(978)': 324,\n",
       " ')': 325,\n",
       " '*': 326,\n",
       " '**': 327,\n",
       " '***': 328,\n",
       " '**general': 329,\n",
       " '*indices': 330,\n",
       " '*name': 331,\n",
       " '*note': 332,\n",
       " '+': 333,\n",
       " '++359-2': 334,\n",
       " '+.01': 335,\n",
       " '+.02': 336,\n",
       " '+.03': 337,\n",
       " '+.04': 338,\n",
       " '+.05': 339,\n",
       " '+.06': 340,\n",
       " '+.07': 341,\n",
       " '+.08': 342,\n",
       " '+.09': 343,\n",
       " '+.10': 344,\n",
       " '+.11': 345,\n",
       " '+.12': 346,\n",
       " '+.13': 347,\n",
       " '+.14': 348,\n",
       " '+.15': 349,\n",
       " '+.19': 350,\n",
       " '+.20': 351,\n",
       " '+.25': 352,\n",
       " '+.31': 353,\n",
       " '+.38': 354,\n",
       " '+.44': 355,\n",
       " '+.50': 356,\n",
       " '+.56': 357,\n",
       " '+.75': 358,\n",
       " '+0': 359,\n",
       " '+0,2': 360,\n",
       " '+0.0': 361,\n",
       " '+0.05': 362,\n",
       " '+0.1': 363,\n",
       " '+0.2': 364,\n",
       " '+0.25': 365,\n",
       " '+0.3': 366,\n",
       " '+0.4': 367,\n",
       " '+0.4m': 368,\n",
       " '+0.5': 369,\n",
       " '+0.50': 370,\n",
       " '+0.6': 371,\n",
       " '+0.7': 372,\n",
       " '+0.75': 373,\n",
       " '+0.8': 374,\n",
       " '+0.9': 375,\n",
       " '+0.9;+23.6': 376,\n",
       " '+00': 377,\n",
       " '+01': 378,\n",
       " '+1': 379,\n",
       " '+1,161': 380,\n",
       " '+1.0': 381,\n",
       " '+1.00': 382,\n",
       " '+1.1': 383,\n",
       " '+1.2': 384,\n",
       " '+1.25': 385,\n",
       " '+1.3': 386,\n",
       " '+1.4': 387,\n",
       " '+1.5': 388,\n",
       " '+1.50': 389,\n",
       " '+1.6': 390,\n",
       " '+1.7': 391,\n",
       " '+1.75': 392,\n",
       " '+1.7;+22.0': 393,\n",
       " '+1.8': 394,\n",
       " '+1.9': 395,\n",
       " '+10': 396,\n",
       " '+10.00': 397,\n",
       " '+10.8': 398,\n",
       " '+100': 399,\n",
       " '+105': 400,\n",
       " '+11': 401,\n",
       " '+11.00': 402,\n",
       " '+12': 403,\n",
       " '+12,696': 404,\n",
       " '+13': 405,\n",
       " '+14': 406,\n",
       " '+15': 407,\n",
       " '+15,272': 408,\n",
       " '+16': 409,\n",
       " '+16.4': 410,\n",
       " '+167,330': 411,\n",
       " '+168,130': 412,\n",
       " '+17': 413,\n",
       " '+18': 414,\n",
       " '+18.3': 415,\n",
       " '+19': 416,\n",
       " '+2': 417,\n",
       " '+2.0': 418,\n",
       " '+2.00': 419,\n",
       " '+2.1': 420,\n",
       " '+2.25': 421,\n",
       " '+2.3r': 422,\n",
       " '+2.4': 423,\n",
       " '+2.5': 424,\n",
       " '+2.50': 425,\n",
       " '+2.6': 426,\n",
       " '+2.75': 427,\n",
       " '+20': 428,\n",
       " '+21': 429,\n",
       " '+22': 430,\n",
       " '+225': 431,\n",
       " '+23': 432,\n",
       " '+230.4': 433,\n",
       " '+24': 434,\n",
       " '+25': 435,\n",
       " '+27': 436,\n",
       " '+282.1': 437,\n",
       " '+3': 438,\n",
       " '+3,428': 439,\n",
       " '+3,831': 440,\n",
       " '+3.00': 441,\n",
       " '+3.06': 442,\n",
       " '+3.4': 443,\n",
       " '+3.5': 444,\n",
       " '+3.50': 445,\n",
       " '+3.6': 446,\n",
       " '+3.7': 447,\n",
       " '+30': 448,\n",
       " '+301': 449,\n",
       " '+31': 450,\n",
       " '+31,230': 451,\n",
       " '+310.4': 452,\n",
       " '+32': 453,\n",
       " '+33': 454,\n",
       " '+331': 455,\n",
       " '+34': 456,\n",
       " '+35': 457,\n",
       " '+353': 458,\n",
       " '+358': 459,\n",
       " '+361': 460,\n",
       " '+381': 461,\n",
       " '+387-71-663-864': 462,\n",
       " '+39': 463,\n",
       " '+4': 464,\n",
       " '+4.00': 465,\n",
       " '+4.2r': 466,\n",
       " '+4.50': 467,\n",
       " '+4.7': 468,\n",
       " '+40': 469,\n",
       " '+400.9': 470,\n",
       " '+41': 471,\n",
       " '+431': 472,\n",
       " '+44': 473,\n",
       " '+45': 474,\n",
       " '+46-8-700': 475,\n",
       " '+47': 476,\n",
       " '+48': 477,\n",
       " '+49': 478,\n",
       " '+5': 479,\n",
       " '+5.00': 480,\n",
       " '+5.2': 481,\n",
       " '+5.50': 482,\n",
       " '+50': 483,\n",
       " '+52': 484,\n",
       " '+525': 485,\n",
       " '+5255-3300-7600': 486,\n",
       " '+541': 487,\n",
       " '+6': 488,\n",
       " '+6.00': 489,\n",
       " '+60': 490,\n",
       " '+6221': 491,\n",
       " '+65': 492,\n",
       " '+65-8703086': 493,\n",
       " '+66': 494,\n",
       " '+6613377': 495,\n",
       " '+67': 496,\n",
       " '+7': 497,\n",
       " '+7.00': 498,\n",
       " '+7.1': 499,\n",
       " '+7.3;-3.6': 500,\n",
       " '+7.50': 501,\n",
       " '+70': 502,\n",
       " '+7095': 503,\n",
       " '+75': 504,\n",
       " '+78': 505,\n",
       " '+8': 506,\n",
       " '+8.00': 507,\n",
       " '+80': 508,\n",
       " '+9': 509,\n",
       " '+9.00': 510,\n",
       " '+9.8': 511,\n",
       " '+90': 512,\n",
       " '+90-212-275': 513,\n",
       " '+91': 514,\n",
       " '+91-11-3012024': 515,\n",
       " '+91-22-265': 516,\n",
       " ',': 517,\n",
       " ',0': 518,\n",
       " ',00': 519,\n",
       " ',000': 520,\n",
       " ',000,000': 521,\n",
       " ',0000': 522,\n",
       " ',001': 523,\n",
       " ',007': 524,\n",
       " ',040': 525,\n",
       " ',050': 526,\n",
       " ',060': 527,\n",
       " ',070': 528,\n",
       " ',080': 529,\n",
       " ',082,500': 530,\n",
       " ',090': 531,\n",
       " ',1': 532,\n",
       " ',10': 533,\n",
       " ',100': 534,\n",
       " ',102': 535,\n",
       " ',103': 536,\n",
       " ',104': 537,\n",
       " ',105': 538,\n",
       " ',106': 539,\n",
       " ',107': 540,\n",
       " ',108': 541,\n",
       " ',109': 542,\n",
       " ',11': 543,\n",
       " ',110': 544,\n",
       " ',114': 545,\n",
       " ',115': 546,\n",
       " ',12': 547,\n",
       " ',120': 548,\n",
       " ',125': 549,\n",
       " ',13': 550,\n",
       " ',130': 551,\n",
       " ',14': 552,\n",
       " ',140': 553,\n",
       " ',148': 554,\n",
       " ',15': 555,\n",
       " ',150': 556,\n",
       " ',16': 557,\n",
       " ',160': 558,\n",
       " ',17': 559,\n",
       " ',170': 560,\n",
       " ',18': 561,\n",
       " ',180': 562,\n",
       " ',185': 563,\n",
       " ',19': 564,\n",
       " ',190': 565,\n",
       " ',195': 566,\n",
       " ',1996': 567,\n",
       " ',1998': 568,\n",
       " ',1999': 569,\n",
       " ',2': 570,\n",
       " ',2,3': 571,\n",
       " ',20': 572,\n",
       " ',200': 573,\n",
       " ',2009': 574,\n",
       " ',2012': 575,\n",
       " ',2013': 576,\n",
       " ',21': 577,\n",
       " ',210': 578,\n",
       " ',22': 579,\n",
       " ',220': 580,\n",
       " ',230': 581,\n",
       " ',240': 582,\n",
       " ',25': 583,\n",
       " ',250': 584,\n",
       " ',255': 585,\n",
       " ',260': 586,\n",
       " ',270': 587,\n",
       " ',275': 588,\n",
       " ',280': 589,\n",
       " ',290': 590,\n",
       " ',295': 591,\n",
       " ',3': 592,\n",
       " ',300': 593,\n",
       " ',310': 594,\n",
       " ',320': 595,\n",
       " ',330': 596,\n",
       " ',340': 597,\n",
       " ',345': 598,\n",
       " ',350': 599,\n",
       " ',360': 600,\n",
       " ',370': 601,\n",
       " ',380': 602,\n",
       " ',390': 603,\n",
       " ',4': 604,\n",
       " ',400': 605,\n",
       " ',410': 606,\n",
       " ',415': 607,\n",
       " ',420': 608,\n",
       " ',430': 609,\n",
       " ',440': 610,\n",
       " ',450': 611,\n",
       " ',46': 612,\n",
       " ',460': 613,\n",
       " ',470': 614,\n",
       " ',480': 615,\n",
       " ',490': 616,\n",
       " ',496': 617,\n",
       " ',5': 618,\n",
       " ',500': 619,\n",
       " ',504': 620,\n",
       " ',510': 621,\n",
       " ',520': 622,\n",
       " ',530': 623,\n",
       " ',540': 624,\n",
       " ',550': 625,\n",
       " ',554.89': 626,\n",
       " ',560': 627,\n",
       " ',565': 628,\n",
       " ',570': 629,\n",
       " ',580': 630,\n",
       " ',590': 631,\n",
       " ',6': 632,\n",
       " ',600': 633,\n",
       " ',620': 634,\n",
       " ',630': 635,\n",
       " ',640': 636,\n",
       " ',650': 637,\n",
       " ',660': 638,\n",
       " ',670': 639,\n",
       " ',675': 640,\n",
       " ',680': 641,\n",
       " ',690': 642,\n",
       " ',7': 643,\n",
       " ',700': 644,\n",
       " ',720': 645,\n",
       " ',730': 646,\n",
       " ',740': 647,\n",
       " ',750': 648,\n",
       " ',760': 649,\n",
       " ',763': 650,\n",
       " ',770': 651,\n",
       " ',775': 652,\n",
       " ',78': 653,\n",
       " ',780': 654,\n",
       " ',790': 655,\n",
       " ',8': 656,\n",
       " ',800': 657,\n",
       " ',815': 658,\n",
       " ',820': 659,\n",
       " ',830': 660,\n",
       " ',840': 661,\n",
       " ',850': 662,\n",
       " ',860': 663,\n",
       " ',870': 664,\n",
       " ',880': 665,\n",
       " ',890': 666,\n",
       " ',9': 667,\n",
       " ',900': 668,\n",
       " ',910': 669,\n",
       " ',920': 670,\n",
       " ',930': 671,\n",
       " ',940': 672,\n",
       " ',950': 673,\n",
       " ',960': 674,\n",
       " ',970': 675,\n",
       " ',980': 676,\n",
       " ',990': 677,\n",
       " ',995': 678,\n",
       " '-': 679,\n",
       " '--': 680,\n",
       " '---': 681,\n",
       " '----': 682,\n",
       " '-----': 683,\n",
       " '------': 684,\n",
       " '-------': 685,\n",
       " '--------': 686,\n",
       " '---------': 687,\n",
       " '----------': 688,\n",
       " '-----------': 689,\n",
       " '------------': 690,\n",
       " '-------------': 691,\n",
       " '--------------': 692,\n",
       " '---------------': 693,\n",
       " '----------------': 694,\n",
       " '-----------------': 695,\n",
       " '------------------': 696,\n",
       " '-------------------': 697,\n",
       " '--------------------': 698,\n",
       " '---------------------': 699,\n",
       " '----------------------': 700,\n",
       " '-----------------------': 701,\n",
       " '------------------------': 702,\n",
       " '-------------------------': 703,\n",
       " '--------------------------': 704,\n",
       " '----------------------------': 705,\n",
       " '-----------------------------': 706,\n",
       " '--------------------------------': 707,\n",
       " '---------------------------------': 708,\n",
       " '----------------------------------': 709,\n",
       " '---------------------------------------': 710,\n",
       " '----------------------------------------------': 711,\n",
       " '-----------------------------------------------': 712,\n",
       " '------------------------------------------------': 713,\n",
       " '-------------------------------------------------': 714,\n",
       " '--------------------------------------------------': 715,\n",
       " '---------------------------------------------------': 716,\n",
       " '----------------------------------------------------': 717,\n",
       " '------------------------------------------------------': 718,\n",
       " '-------------------------------------------------------': 719,\n",
       " '---------------------------------------------------------': 720,\n",
       " '------------------------------------------------------------': 721,\n",
       " '-------------------------------------------------------------': 722,\n",
       " '-----------------------------------------------------------------': 723,\n",
       " '------------------------------------------------------------------': 724,\n",
       " '--------------------------------------------------------------------': 725,\n",
       " '-.02': 726,\n",
       " '-.500': 727,\n",
       " '-0': 728,\n",
       " '-0.07': 729,\n",
       " '-0.09': 730,\n",
       " '-0.1': 731,\n",
       " '-0.2': 732,\n",
       " '-0.25': 733,\n",
       " '-0.3': 734,\n",
       " '-0.4': 735,\n",
       " '-0.5': 736,\n",
       " '-0.50': 737,\n",
       " '-0.6': 738,\n",
       " '-0.7': 739,\n",
       " '-0.75': 740,\n",
       " '-0.8': 741,\n",
       " '-0.9': 742,\n",
       " '-01': 743,\n",
       " '-02': 744,\n",
       " '-03': 745,\n",
       " '-04': 746,\n",
       " '-0400': 747,\n",
       " '-05': 748,\n",
       " '-0500': 749,\n",
       " '-06': 750,\n",
       " '-07': 751,\n",
       " '-08': 752,\n",
       " '-09': 753,\n",
       " '-1': 754,\n",
       " '-1.0': 755,\n",
       " '-1.00': 756,\n",
       " '-1.1': 757,\n",
       " '-1.2': 758,\n",
       " '-1.25': 759,\n",
       " '-1.3': 760,\n",
       " '-1.4': 761,\n",
       " '-1.5': 762,\n",
       " '-1.50': 763,\n",
       " '-1.6': 764,\n",
       " '-1.7': 765,\n",
       " '-1.75': 766,\n",
       " '-1.8': 767,\n",
       " '-1.9': 768,\n",
       " '-10': 769,\n",
       " '-10.00': 770,\n",
       " '-100': 771,\n",
       " '-105': 772,\n",
       " '-10:00': 773,\n",
       " '-10:30': 774,\n",
       " '-11': 775,\n",
       " '-110': 776,\n",
       " '-117': 777,\n",
       " '-11:00': 778,\n",
       " '-12': 779,\n",
       " '-12:00': 780,\n",
       " '-13': 781,\n",
       " '-14': 782,\n",
       " '-15': 783,\n",
       " '-15:00': 784,\n",
       " '-16': 785,\n",
       " '-17': 786,\n",
       " '-18': 787,\n",
       " '-18:00': 788,\n",
       " '-19': 789,\n",
       " '-1927': 790,\n",
       " '-1990': 791,\n",
       " '-1999': 792,\n",
       " '-2': 793,\n",
       " '-2,3': 794,\n",
       " '-2.0': 795,\n",
       " '-2.00': 796,\n",
       " '-2.1': 797,\n",
       " '-2.2': 798,\n",
       " '-2.25': 799,\n",
       " '-2.3': 800,\n",
       " '-2.4': 801,\n",
       " '-2.5': 802,\n",
       " '-2.50': 803,\n",
       " '-2.6': 804,\n",
       " '-2.7': 805,\n",
       " '-2.8': 806,\n",
       " '-2.9': 807,\n",
       " '-20': 808,\n",
       " '-200': 809,\n",
       " '-2000': 810,\n",
       " '-2006': 811,\n",
       " '-2009': 812,\n",
       " '-2010': 813,\n",
       " '-20:00': 814,\n",
       " '-21': 815,\n",
       " '-22': 816,\n",
       " '-23': 817,\n",
       " '-24': 818,\n",
       " '-25': 819,\n",
       " '-26': 820,\n",
       " '-27': 821,\n",
       " '-28': 822,\n",
       " '-29': 823,\n",
       " '-3': 824,\n",
       " '-3.0': 825,\n",
       " '-3.00': 826,\n",
       " '-3.1': 827,\n",
       " '-3.2': 828,\n",
       " '-3.3': 829,\n",
       " '-3.4': 830,\n",
       " '-3.5': 831,\n",
       " '-3.50': 832,\n",
       " '-3.7': 833,\n",
       " '-3.8': 834,\n",
       " '-3.9': 835,\n",
       " '-30': 836,\n",
       " '-300': 837,\n",
       " '-31': 838,\n",
       " '-32': 839,\n",
       " '-33': 840,\n",
       " '-34': 841,\n",
       " '-35': 842,\n",
       " '-36': 843,\n",
       " '-37': 844,\n",
       " '-38': 845,\n",
       " '-39': 846,\n",
       " '-4': 847,\n",
       " '-4.0': 848,\n",
       " '-4.00': 849,\n",
       " '-4.1': 850,\n",
       " '-4.2': 851,\n",
       " '-4.5': 852,\n",
       " '-4.50': 853,\n",
       " '-4.7': 854,\n",
       " '-40': 855,\n",
       " '-400': 856,\n",
       " '-41': 857,\n",
       " '-42': 858,\n",
       " '-43': 859,\n",
       " '-44': 860,\n",
       " '-45': 861,\n",
       " '-46': 862,\n",
       " '-47': 863,\n",
       " '-48': 864,\n",
       " '-49': 865,\n",
       " '-5': 866,\n",
       " '-5.00': 867,\n",
       " '-5.1': 868,\n",
       " '-5.6': 869,\n",
       " '-50': 870,\n",
       " '-500': 871,\n",
       " '-5000': 872,\n",
       " '-51': 873,\n",
       " '-52': 874,\n",
       " '-53': 875,\n",
       " '-54': 876,\n",
       " '-55': 877,\n",
       " '-56': 878,\n",
       " '-57': 879,\n",
       " '-58': 880,\n",
       " '-59': 881,\n",
       " '-6': 882,\n",
       " '-6.0': 883,\n",
       " '-6.00': 884,\n",
       " '-6.5': 885,\n",
       " '-6.50': 886,\n",
       " '-6.7': 887,\n",
       " '-60': 888,\n",
       " '-600': 889,\n",
       " '-61': 890,\n",
       " '-62': 891,\n",
       " '-63': 892,\n",
       " '-64': 893,\n",
       " '-65': 894,\n",
       " '-66': 895,\n",
       " '-67': 896,\n",
       " '-68': 897,\n",
       " '-69': 898,\n",
       " '-7': 899,\n",
       " '-7.00': 900,\n",
       " '-70': 901,\n",
       " '-71': 902,\n",
       " '-72': 903,\n",
       " '-73': 904,\n",
       " '-74': 905,\n",
       " '-75': 906,\n",
       " '-77': 907,\n",
       " '-78': 908,\n",
       " '-79': 909,\n",
       " '-8': 910,\n",
       " '-8.00': 911,\n",
       " '-80': 912,\n",
       " '-800': 913,\n",
       " '-81': 914,\n",
       " '-84': 915,\n",
       " '-85': 916,\n",
       " '-86': 917,\n",
       " '-88': 918,\n",
       " '-89': 919,\n",
       " '-8:11': 920,\n",
       " '-8:30': 921,\n",
       " '-9': 922,\n",
       " '-9.00': 923,\n",
       " '-90': 924,\n",
       " '-900': 925,\n",
       " '-91': 926,\n",
       " '-92': 927,\n",
       " '-93': 928,\n",
       " '-94': 929,\n",
       " '-95': 930,\n",
       " '-96': 931,\n",
       " '-99': 932,\n",
       " '-9:00': 933,\n",
       " '-9:30': 934,\n",
       " '-docstart-': 935,\n",
       " '.': 936,\n",
       " '..': 937,\n",
       " '...': 938,\n",
       " '....': 939,\n",
       " '.....': 940,\n",
       " '...and': 941,\n",
       " '...for': 942,\n",
       " '.0': 943,\n",
       " '.00': 944,\n",
       " '.000': 945,\n",
       " '.000001': 946,\n",
       " '.000088': 947,\n",
       " '.000104': 948,\n",
       " '.000105': 949,\n",
       " '.000106': 950,\n",
       " '.000434': 951,\n",
       " '.000435': 952,\n",
       " '.000446': 953,\n",
       " '.000660': 954,\n",
       " '.000661': 955,\n",
       " '.000663': 956,\n",
       " '.001': 957,\n",
       " '.0014': 958,\n",
       " '.002': 959,\n",
       " '.0032': 960,\n",
       " '.0033': 961,\n",
       " '.0034': 962,\n",
       " '.0035': 963,\n",
       " '.005': 964,\n",
       " '.01': 965,\n",
       " '.0156': 966,\n",
       " '.0163': 967,\n",
       " '.0170': 968,\n",
       " '.0198': 969,\n",
       " '.0199': 970,\n",
       " '.02': 971,\n",
       " '.0200': 972,\n",
       " '.0201': 973,\n",
       " '.0202': 974,\n",
       " '.0203': 975,\n",
       " '.0204': 976,\n",
       " '.0205': 977,\n",
       " '.0206': 978,\n",
       " '.0207': 979,\n",
       " '.0208': 980,\n",
       " '.0210': 981,\n",
       " '.0211': 982,\n",
       " '.0212': 983,\n",
       " '.0213': 984,\n",
       " '.0214': 985,\n",
       " '.0215': 986,\n",
       " '.0216': 987,\n",
       " '.0217': 988,\n",
       " '.03': 989,\n",
       " '.0304': 990,\n",
       " '.0309': 991,\n",
       " '.0342': 992,\n",
       " '.0349': 993,\n",
       " '.0358': 994,\n",
       " '.0359': 995,\n",
       " '.04': 996,\n",
       " '.05': 997,\n",
       " '.059': 998,\n",
       " '.06': 999,\n",
       " '.0613': 1000,\n",
       " '.0617': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Embedding Matrix\n",
    "\n",
    "1. Create a matrix of dimensions (M, N), where M, will the size of the vocabulary: The unique words in the training set and the words in GloVe, and N, the dimension of the embeddings. Initialize it with random values.\n",
    "2. Fill the matrix with the GloVe embeddings. You will use the indices from the previous section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing 6.11 Preparing the GloVe word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word = len(voc) + 2 # M\n",
    "embedding_dim = len(embeddings_index['france']) # N\n",
    "embedding_matrix = np.random.rand(max_word,embedding_dim)\n",
    "\n",
    "for word, i in voc_idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the X and Y Sequences\n",
    "\n",
    "You will now create the input and output sequences with numerical indices\n",
    "\n",
    "1. Convert the X and Y lists of symbols in lists of numbers using the indices you created.\n",
    "2. Pad the sentences using the pad_sequences function.\n",
    "3. Do the same for the development set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the matrices into numbers. Before: We have the symbols. After: We have the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_idx = [list(map(lambda x: word_idx.get(x, 1), x)) for x in X_words]\n",
    "Y_ner_idx = [list(map(lambda x: ner_idx.get(x, 1), x)) for x in Y_ner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936]\n",
      "[284434, 79019]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 10, 3, 10, 10, 10, 3, 10, 10]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_words_idx[1])\n",
    "print(X_words_idx[2])\n",
    "Y_ner_idx[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_idx = pad_sequences(X_words_idx, maxlen=150)\n",
    "Y_ner_idx = pad_sequences(Y_ner_idx, maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_idx_test = [list(map(lambda x: word_idx.get(x, 1), x)) for x in X_words_test]\n",
    "Y_ner_idx_test = [list(map(lambda x: ner_idx.get(x, 1), x)) for x in Y_ner_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_words_idx = pad_sequences(X_words_idx_test, maxlen=150)\n",
    "Y_test_ner_idx = pad_sequences(Y_ner_idx_test, maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_words_idx = [list(map(lambda x: word_idx.get(x, 1), x)) for x in X_dev_words]\n",
    "Y_dev_ner_idx = [list(map(lambda x: ner_idx.get(x, 1), x)) for x in Y_dev_ner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_words_idx = pad_sequences(X_dev_words_idx, maxlen=150)\n",
    "Y_dev_ner_idx = pad_sequences(Y_dev_ner_idx, maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple Recurrent Neural Network\n",
    "\n",
    "1. Create a simple recurrent network and train a model with the train set. As layers, you will use Embedding, SimpleRNN, and Dense.\n",
    "2. Compile and fit your network. You will report the training and validation losses and accuracies and comment on the possible overfit.\n",
    "3. Apply your network to the test set and report the accuracy you obtained. You will use the evaluate method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabulary_size = len(voc) + 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(text_vocabulary_size,\n",
    "                    embedding_dim,\n",
    "                    input_length=150,\n",
    "                    mask_zero=True))\n",
    "\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "#model.add(Bidirectional(SimpleRNN(32, return_sequences=True)))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(len(ner_set) + 2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained word embeddings into the Embedding layer (frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting arrays of labeled data to one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_ner_idx, num_classes = len(ner_set)+2)\n",
    "X_train = X_words_idx\n",
    "\n",
    "Y_test = to_categorical(Y_test_ner_idx, num_classes = len(ner_set)+2)\n",
    "X_test = X_test_words_idx\n",
    "\n",
    "Y_val = to_categorical(Y_dev_ner_idx, num_classes = len(ner_set)+2)\n",
    "X_val = X_dev_words_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y_train[1])\n",
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/10\n",
      "14987/14987 [==============================] - 18s 1ms/step - loss: 0.0628 - acc: 0.8299 - val_loss: 0.0406 - val_acc: 0.8962\n",
      "Epoch 2/10\n",
      "14987/14987 [==============================] - 15s 1ms/step - loss: 0.0308 - acc: 0.9087 - val_loss: 0.0297 - val_acc: 0.9195\n",
      "Epoch 3/10\n",
      "14987/14987 [==============================] - 15s 990us/step - loss: 0.0248 - acc: 0.9232 - val_loss: 0.0256 - val_acc: 0.9299\n",
      "Epoch 4/10\n",
      "14987/14987 [==============================] - 15s 993us/step - loss: 0.0221 - acc: 0.9314 - val_loss: 0.0235 - val_acc: 0.9358\n",
      "Epoch 5/10\n",
      "14987/14987 [==============================] - 15s 990us/step - loss: 0.0204 - acc: 0.9371 - val_loss: 0.0221 - val_acc: 0.9394\n",
      "Epoch 6/10\n",
      "14987/14987 [==============================] - 15s 982us/step - loss: 0.0192 - acc: 0.9407 - val_loss: 0.0214 - val_acc: 0.9417\n",
      "Epoch 7/10\n",
      "14987/14987 [==============================] - 15s 970us/step - loss: 0.0183 - acc: 0.9432 - val_loss: 0.0210 - val_acc: 0.9429\n",
      "Epoch 8/10\n",
      "14987/14987 [==============================] - 14s 943us/step - loss: 0.0176 - acc: 0.9453 - val_loss: 0.0202 - val_acc: 0.9445\n",
      "Epoch 9/10\n",
      "14987/14987 [==============================] - 14s 939us/step - loss: 0.0169 - acc: 0.9468 - val_loss: 0.0196 - val_acc: 0.9460\n",
      "Epoch 10/10\n",
      "14987/14987 [==============================] - 14s 933us/step - loss: 0.0164 - acc: 0.9485 - val_loss: 0.0189 - val_acc: 0.9479\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    epochs=10, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val, Y_val)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhV5bn+8e8DgmGeUQQZnIEwGCNiQQVRQBSoSCsUWqFVWsej1Z9F8RSqh3o8Wmt76ukptVq1VMrRWlBRai2K1iqDAgJWQEAMk0wiMgiB5/fHu5LsbDLsQJKdLO7PdeVi77XevfazVxZ33v2uydwdERGJrxrpLkBERCqWgl5EJOYU9CIiMaegFxGJOQW9iEjMKehFRGJOQX8MMrOaZvalmbUtz7bpZGanmVm5HytsZpeY2dqE5x+Z2QWptD2C93rMzO4+0teLFOe4dBcgpTOzLxOe1gW+Ag5Gz7/v7lPLsjx3PwjUL++2xwJ3P7M8lmNm1wKj3b1PwrKvLY9liyRT0FcD7p4ftFGP8Vp3/1tx7c3sOHfPrYzaREqj7TH9NHQTA2b2H2b2JzN7xsx2AaPN7Hwze8fMPjezjWb2SzOrFbU/zszczNpHz/8QzX/ZzHaZ2T/NrENZ20bzLzOzFWa208z+28z+YWZjiqk7lRq/b2arzGyHmf0y4bU1zeznZrbNzD4GBpawfu4xs2lJ0x41s4ejx9ea2YfR5/k46m0Xt6wcM+sTPa5rZk9HtS0DzinifVdHy11mZkOi6V2AXwEXRMNiWxPW7aSE1/8g+uzbzOwvZtYqlXVTlvWcV4+Z/c3MtpvZJjO7M+F9/j1aJ1+Y2QIzO6moYTIzeyvv9xytz7nR+2wH7jGz081sTvRZtkbrrVHC69tFn3FLNP8XZpYR1dwxoV0rM9tjZs2K+7xSBHfXTzX6AdYClyRN+w9gPzCY8Me7DnAucB7hW9spwArgpqj9cYAD7aPnfwC2AtlALeBPwB+OoG1LYBcwNJr3Q+AAMKaYz5JKjTOARkB7YHveZwduApYBbYBmwNywORf5PqcAXwL1Epb9GZAdPR8ctTHgYmAv0DWadwmwNmFZOUCf6PFDwOtAE6AdsDyp7TeBVtHv5FtRDSdE864FXk+q8w/ApOhx/6jG7kAG8D/A31NZN2Vcz42AzcC/AccDDYEe0by7gMXA6dFn6A40BU5LXtfAW3m/5+iz5QLXAzUJ2+MZQD+gdrSd/AN4KOHzLI3WZ72ofa9o3hRgcsL73A48n+7/h9XtJ+0F6KeMv7Dig/7vpbzuDuD/osdFhff/JrQdAiw9grbfBd5MmGfARooJ+hRr7Jkw/8/AHdHjuYQhrLx5g5LDJ2nZ7wDfih5fBqwooe2LwI3R45KCfl3i7wK4IbFtEctdClwePS4t6J8EfpowryFhv0yb0tZNGdfzt4EFxbT7OK/epOmpBP3qUmoYDsyPHl8AbAJqFtGuF7AGsOj5ImBYef+/ivuPhm7i49PEJ2Z2lpm9FH0V/wK4F2hewus3JTzeQ8k7YItre1JiHR7+Z+YUt5AUa0zpvYBPSqgX4I/AyOjxt4D8HdhmdoWZvRsNXXxO6E2XtK7ytCqpBjMbY2aLo+GHz4GzUlwuhM+Xvzx3/wLYAbROaJPS76yU9XwysKqYGk4mhP2RSN4eTzSz6Wa2Pqrh90k1rPWw478Qd/8H4dtBbzPLBNoCLx1hTccsBX18JB9a+BtCD/I0d28I/JjQw65IGwk9TgDMzCgcTMmOpsaNhIDIU9rhn38CLjGzNoShpT9GNdYBngXuJwyrNAb+mmIdm4qrwcxOAX5NGL5oFi33XwnLLe1Q0A2E4aC85TUgDBGtT6GuZCWt50+BU4t5XXHzdkc11U2YdmJSm+TP9wDhaLEuUQ1jkmpoZ2Y1i6njKWA04dvHdHf/qph2UgwFfXw1AHYCu6OdWd+vhPd8Ecgys8Fmdhxh3LdFBdU4HbjVzFpHO+Z+VFJjd99MGF54AvjI3VdGs44njBtvAQ6a2RWEseRUa7jbzBpbOM/gpoR59Qlht4XwN+9aQo8+z2agTeJO0STPAN8zs65mdjzhD9Gb7l7sN6QSlLSeZwJtzewmM6ttZg3NrEc07zHgP8zsVAu6m1lTwh+4TYSd/jXNbBwJf5RKqGE3sNPMTiYMH+X5J7AN+KmFHdx1zKxXwvynCUM93yKEvpSRgj6+bgeuIewc/Q2hR1uhojC9GniY8B/3VOB9Qk+uvGv8NfAa8AEwn9ArL80fCWPuf0yo+XPgNuB5wg7N4YQ/WKmYSPhmsRZ4mYQQcvclwC+BeVGbs4B3E177KrAS2GxmiUMwea9/hTDE8nz0+rbAqBTrSlbsenb3ncClwFWEnb8rgIui2Q8CfyGs5y8IO0YzoiG564C7CTvmT0v6bEWZCPQg/MGZCTyXUEMucAXQkdC7X0f4PeTNX0v4Pe9397fL+NmFgh0cIuUu+iq+ARju7m+mux6pvszsKcIO3knprqU60glTUq7MbCDhq/g+wuF5uYRercgRifZ3DAW6pLuW6kpDN1LeegOrCV/pBwJf184zOVJmdj/hWP6fuvu6dNdTXWnoRkQk5tSjFxGJuSo3Rt+8eXNv3759ussQEalWFi5cuNXdizycucoFffv27VmwYEG6yxARqVbMrNizwzV0IyIScwp6EZGYU9CLiMRclRujL8qBAwfIyclh37596S5FSpCRkUGbNm2oVau4y7eISDpUi6DPycmhQYMGtG/fnnBBRKlq3J1t27aRk5NDhw4dSn+BiFSaajF0s2/fPpo1a6aQr8LMjGbNmulbl0gVVC2CHlDIVwP6HYlUTdVi6EZEJK62boUPPgg/GRkwblz5v4eCPgXbtm2jX79wL4pNmzZRs2ZNWrQIJ6DNmzeP2rVrl7qMsWPHMn78eM4888xi2zz66KM0btyYUaOO9LLjIlJV7d0Ly5cXhHrez6aEuxGcf34agz669OwvCHd0f8zd/zNpfjvgccLdhLYDoxPvhGNmDYEPCXdvT7wLT4WYOhUmTIB166BtW5g8GY4mO5s1a8aiRYsAmDRpEvXr1+eOO+4o1Cb/Jrw1ih4Ne+KJJ0p9nxtvvPHIixSRKuHgQfj44xDiS5cWBPqqVXDoUGiTkQGdOsGAAdClS8HPick3ZCwnpY7RRzePeBS4DOgEjDSzTknNHgKecveuhLvi3J80/z7gjaMvt3RTp4a/iJ98Au7h33HjwvTytmrVKjIzM/nBD35AVlYWGzduZNy4cWRnZ9O5c2fuvffe/La9e/dm0aJF5Obm0rhxY8aPH0+3bt04//zz+eyzzwC45557eOSRR/Lbjx8/nh49enDmmWfy9tvhxjq7d+/mqquuolu3bowcOZLs7Oz8P0KJJk6cyLnnnptfX95VSlesWMHFF19Mt27dyMrKYu3atQD89Kc/pUuXLnTr1o0JEyaU/8oSiRn30Bt/9VV4+GEYOxays6FBAzjzTBg+HH7yE1iyBDIz4d//HZ59Fj76CL78EhYuhN//Hm6/Hfr3h1atoMJ2c+X1RIv7Ac4HZic8vwu4K6nNMqBN9NiALxLmnQNMI9wM+Felvd8555zjyZYvX37YtOK0a+cefgWFf9q1S3kRJZo4caI/+OCD7u6+cuVKNzOfN29e/vxt27a5u/uBAwe8d+/evmzZMnd379Wrl7///vt+4MABB3zWrFnu7n7bbbf5/fff7+7uEyZM8J///Of57e+88053d58xY4YPGDDA3d3vv/9+v+GGG9zdfdGiRV6jRg1///33D6szr45Dhw75iBEj8t8vKyvLZ86c6e7ue/fu9d27d/vMmTO9d+/evmfPnkKvPRJl+V2JVBe7drm/8477b3/rfsst7n37ujdvXjhjTjjB/ZJL3G+7zf3xx93nz3ffvbvyagQWeDG5msrQTWvCfRzz5ADnJbVZTLjn5C+AK4EG0Q2bdwA/I9y9vdgbLkc3Fx4H0LZt2xRKKt66Ym5NUNz0o3Xqqady7rnn5j9/5pln+N3vfkdubi4bNmxg+fLldOpU+AtQnTp1uOyyywA455xzePPNou+yN2zYsPw2eT3vt956ix/9KNwHu1u3bnTu3LnI17722ms8+OCD7Nu3j61bt3LOOefQs2dPtm7dyuDBg4FwghPA3/72N7773e9Sp04dAJo2bXokq0Kk2svNhRUrCoZb8oZeVq8uaFOvXuihDx1aeNilRZHXjawaUgn6or5MJN+t5A7gV2Y2BpgLrCfcQu4GYJa7f1rSoXfuPoVw42Gys7OP6k4obduG4ZqipleEevXq5T9euXIlv/jFL5g3bx6NGzdm9OjRRR5XnrjztmbNmuTm5ha57OOPP/6wNp7CjWL27NnDTTfdxHvvvUfr1q2555578uso6vfg7jo0Uo4p7rB+/eE7Rj/8EPbvD21q1oQzzgjDMWPHFgR6+/ZQzK64KiuVoM8BTk543oZww+d87r4BGAZgZvWBq9x9p5mdD1xgZjcA9YHaZvalu48vl+qLMHlyGJPfs6dgWt26YXpF++KLL2jQoAENGzZk48aNzJ49m4EDB5bre/Tu3Zvp06dzwQUX8MEHH7B8+fLD2uzdu5caNWrQvHlzdu3axXPPPceoUaNo0qQJzZs354UXXmDw4MHs27ePQ4cO0b9/fx544AGuvvpq6tSpw/bt29Wrl9jYubPwTtG8n88/L2jTunUI8f79CwL9rLPCTtM4SCXo5wOnm1kHQk99BPCtxAZm1hzY7u6HCGP4jwO4+6iENmOA7IoMeSg4uqY8j7pJVVZWFp06dSIzM5NTTjmFXr16lft73HzzzXznO9+ha9euZGVlkZmZSaNGjQq1adasGddccw2ZmZm0a9eO884rGGmbOnUq3//+95kwYQK1a9fmueee44orrmDx4sVkZ2dTq1YtBg8ezH333VfutYtUpP374V//OjzQP00YeG7YMIT41VcXHnZp0iR9dVeGlO4Za2aDgEcIh1c+7u6TzexewuD/TDMbTjjSxglDNzd60g2hE4K+xMMrs7OzPfnGIx9++CEdO3ZM/VPFWG5uLrm5uWRkZLBy5Ur69+/PypUrOe64qnFKhH5XUtHyjqZLDvSPPgpj7AC1aoUeeWKYd+kCJ59cgUe2HIXyOCTczBa6e3ZR81JKB3efBcxKmvbjhMfPAs+WsozfA79P5f2keF9++SX9+vUjNzcXd+c3v/lNlQl5kfK2bdvhwy5Ll8KuXQVt2rULIT5kSEGgn3EGpHAeY5WQd0h43nBz3iHhUH4jEUqIaqZx48YsXLgw3WWIlKt9+4o+a3TjxoI2TZuGEL/mmnDUS5cu4d+GDdNXd3mYMKHwPkUIzydMUNCLSDV06FA4VDE50FeuLDhr9Pjjw1mjl15aeNilQk8oSqPKOCRcQS8iFWLz5sOHXZYtK+i9msEpp4QQ/+Y3CwL9tNPgWBqNrIxDwo+h1SkiFWH37hDgyb30LVsK2rRsGUJ83LiCIZfOncPJR8e6yjgkXEEvIinJzQ0X5koO9NWrw5EwEAKqc2cYPLjwsEvLlumtvSqrjEPCq9n5XenRp08fZs+eXWjaI488wg033FDi6+rXrw/Ahg0bGD58eLHLTj6cNNkjjzzCnoQ/94MGDeLzxLM9RMpR3lmjs2fDQw+FnZ9ZWVC/PnTsGIZZJk8OZ5GefTZMmgR//nMYZ9+1C+bNg9/9Dm69Ffr1U8inYtQoWLs27KdYu7b8z/tRjz4FI0eOZNq0aQwYMCB/2rRp03jwwQdTev1JJ53Es8+WePRpiR555BFGjx5N3bp1AZg1a1YprxBJzRdfFH344vbtBW1OOin0yvv1K3zWaHRpJKkG1KNPwfDhw3nxxRf56qtwDtjatWvZsGEDvXv3zj+uPSsriy5dujBjxozDXr927VoyMzOBcHmCESNG0LVrV66++mr27t2b3+7666/Pv8TxxIkTAfjlL3/Jhg0b6Nu3L3379gWgffv2bN26FYCHH36YzMxMMjMz8y9xvHbtWjp27Mh1111H586d6d+/f6H3yfPCCy9w3nnncfbZZ3PJJZewefNmIByrP3bsWLp06ULXrl157rnnAHjllVfIysqiW7du+TdikerhwIEQ4M88A3ffHYZW2reHRo2gVy/4wQ/gqadCu+HD4b//G15/Pdz9aP16eOUVePBB+M53Qi9eIV+9VLse/a23QhGXXz8q3btDlJFFatasGT169OCVV15h6NChTJs2jauvvhozIyMjg+eff56GDRuydetWevbsyZAhQ4q9SNivf/1r6taty5IlS1iyZAlZWVn58yZPnkzTpk05ePAg/fr1Y8mSJdxyyy08/PDDzJkzh+bNmxda1sKFC3niiSd49913cXfOO+88LrroIpo0acLKlSt55pln+O1vf8s3v/lNnnvuOUaPHl3o9b179+add97BzHjsscf4r//6L372s59x33330ahRIz744AMAduzYwZYtW7juuuuYO3cuHTp0YHtil0+qDPcwzpt89cV//SuEOIQjWs48s+BuRnm99Hbt4nn4olTDoE+XvOGbvKB//PHHgXDlx7vvvpu5c+dSo0YN1q9fz+bNmzmxmFvFzJ07l1tuuQWArl270rVr1/x506dPZ8qUKeTm5rJx40aWL19eaH6yt956iyuvvDL/CprDhg3jzTffZMiQIXTo0IHu3bsDhS9znCgnJ4err76ajRs3sn//fjp06ACEyxZPmzYtv12TJk144YUXuPDCC/Pb6KJn6bdjx+E7RpcuDcMxedq2DSF++eUFR7ucdVb1OWtUyke1C/qSet4V6etf/zo//OEPee+999i7d29+T3zq1Kls2bKFhQsXUqtWLdq3b1/kpYkTFdXbX7NmDQ899BDz58+nSZMmjBkzptTllHSdorxLHEO4zHFRQzc333wzP/zhDxkyZAivv/46kyZNyl9uco26lHH6fPVV2PGZHOrr1xe0adw4BPno0QU99MzMMDQjUu2CPl3q169Pnz59+O53v8vIkSPzp+/cuZOWLVtSq1Yt5syZwydFnfmQ4MILL2Tq1Kn07duXpUuXsmTJEiBc4rhevXo0atSIzZs38/LLL9OnTx8AGjRowK5duw4burnwwgsZM2YM48ePx915/vnnefrpp1P+TDt37qR169YAPPnkk/nT+/fvz69+9av8Mf8dO3Zw/vnnc+ONN7JmzZr8oRv16svXoUOwZs3hPfQVK8J9SCH0xDt2hL59Cx++2Lq1hl2keAr6Mhg5ciTDhg0rNKwxatQoBg8eTHZ2Nt27d+ess84qcRnXX389Y8eOpWvXrnTv3p0ePXoA4W5RZ599Np07dz7sEsfjxo3jsssuo1WrVsyZMyd/elZWFmPGjMlfxrXXXsvZZ59d5DBNUSZNmsQ3vvENWrduTc+ePVmzZg0Q7l174403kpmZSc2aNZk4cSLDhg1jypQpDBs2jEOHDtGyZUteffXVlN5HDrdly+E99GXLwslHefLOGr3qqoIe+umnhyszipRFSpcprky6THH1pt9VYXv2FH2xrugAJwCaNz/8crqdO4fj1kVSddSXKRaRkh08WPRZox9/XHDWaEZGCPDLLisc6iecoGEXqVgKepEycIdNmw4P9OXLw6V2IdxP9LTToFu3gp2jmZlw6qnhPqQila3aBL2O+qj6qtowYHnJyYGZM+GFF2D+/HAzjDwnnhiC/IYbCnronTrphCKpWqpF0GdkZLBt2zaaNWumsK+i3J1t27aREYO7KbvD4sUh3GfMgPfeC9NPPx2uvLLwsEvSgVAiVVK1CPo2bdqQk5PDlsTrnkqVk5GRQZs2bdJdxhE5cADeeCME+8yZ4exSs3D26AMPhNvUlXJAlUiVVS2CvlatWvlnZIqUl88/D9dwmTEDXn4Zdu4MQy6XXgoTJ8IVV+jKixIP1SLoRcrLJ5+EHvvMmeGiXbm5IcyHDw+99ksuCddUF4kTBb3Emju8/37BkEzeBfHOOgtuvx2GDoUePXQ0jMSbgl5iZ//+0FvPC/ecnHDIY69e4VK7Q4bAGWeku0qRyqOgl1jYsQNmzQrB/vLL4U5HdevCgAFw333h6o0tWqS7SpH0UNBLtbVmTcEhkHPnhrNTTzgBRowIvfZ+/XQ8uwgo6KUaOXQIFi4sCPfovih06gR33hnG2889NwzTiEgBBb1UaV99BX//ewj2F16ADRtCkF9wAfzsZ6Hnftpp6a5SpGpT0EuVs21bGG+fMQNmz4Yvv4R69WDgwNBrHzQImjVLd5Ui1YeCXqqEjz8uGJJ5660w3t6qFYwaFcK9b99w9UcRKTsFvaTFoUPhAmF54b5sWZjepQvcdVcYkjnnHI23i5QHBb1Umn374LXXCsbbN20KJypdeCFcdx0MHhzuqlQVTZ0KEyaEa+C0bQuTJ4dvGyLVgYJeKtTWrfDSSwXj7Xv2QIMGhcfbmzRJd5UlmzoVxo0LtUO4jMK4ceGxwl6qg2pxK0GpXlauLDgr9R//CMM0rVuH4ZihQ6FPHzj++HRXmbr27UO4J2vXDlK8Pa9IhdOtBKVCHToE775bEO4ffhimd+sG99wTAj4rq/reLm/durJNF6lqFPRyRPbsgb/9reDOS599BscdBxddBNdfH8K9Xbt0V1k+2rYtukfftm3l1yJyJBT0krLPPoMXXwzh/te/wt690LBhGGcfMiTc9Lpx43RXWf4mTy48Rg/hOjqTJ6evJpGySCnozWwg8AugJvCYu/9n0vx2wONAC2A7MNrdc8ysO/BroCFwEJjs7n8qx/qlgn30URiSmTED/vnPcNnfk0+G730vhPtFF0Ht2umusmLl7XDVUTdSXZW6M9bMagIrgEuBHGA+MNLdlye0+T/gRXd/0swuBsa6+7fN7AzA3X2lmZ0ELAQ6uvvnxb2fdsam18GDIdDzjm9fsSJMz8oq2JnarVv1HW8Xiauj3RnbA1jl7qujhU0DhgLLE9p0Am6LHs8B/gLg7ivyGrj7BjP7jNDrLzbopfLt3g2vvhqC/cUXwyGRtWqFs1FvuSUE/Mknp7tKETlSqQR9a+DThOc5wHlJbRYDVxGGd64EGphZM3ffltfAzHoAtYGPk9/AzMYB4wDaag9Xpdi0KYT6jBlhp+q+fWF8fdCg0GsfMAAaNUp3lSJSHlIJ+qK+pCeP99wB/MrMxgBzgfVAbv4CzFoBTwPXuPuhwxbmPgWYAmHoJqXKpUzcw2GPeUMy774bprVrF3Y0Dh0arghZq1a6KxWR8pZK0OcAiV/c2wAbEhu4+wZgGICZ1Qeucved0fOGwEvAPe7+TnkULanJzYW33y4I91WrwvTsbPjJT0K4d+mi8XaRuEsl6OcDp5tZB0JPfQTwrcQGZtYc2B711u8iHIGDmdUGngeecvf/K8/CpWhffhkOfZwxI1x6YNu2cFTMxReHm2FfcQW0aZPuKkWkMpUa9O6ea2Y3AbMJh1c+7u7LzOxeYIG7zwT6APebmROGbm6MXv5N4EKgWTSsAzDG3ReV78c4tm3cGE5amjEjXDTsq6/C9WMuv7xgvL1Bg3RXKSLpomvdVEPu4bK+eZccmDcvTO/QIQT70KHQu3c4U1VEjg261k0M5OaGG3Lkhfvq1WF6jx7h5J0hQ6BzZ423i8jhFPRV2K5d8MorIdhfegl27AhXfezXD370o3D99lat0l2liFR1CvoqZv36EOwzZ4abYu/fH+6POmRI+OnfH+rXT3eVIlKdKOjTzB2WLCk4BHLhwjD9tNPg5pvDePv552u8XUSOnOIjDQ4cgLlzC3rua9eGsfWePeH++0O4n3WWxttFpHwo6CvJzp1hvH3GDJg1KzzPyIBLLw1XRbziCjjxxHRXKSJxpKCvQOvWFRzf/vrroSffvDkMGxZ67ZdcAvXqpbtKEYk7BX05codFiwrG299/P0w/4wy49dYQ7j17Qs2a6a1TRI4tCvqjtH8/vPFGwfHtn34axta/9jV44IEQ7meeme4qReRYpqA/Ap9/Di+/HML95Zfhiy+gTp1w6ONPfhIuPdCyZbqrFBEJFPQp+uSTgiGZN94IZ6q2bAnf+EbotffrF+4jKiJS1Sjoi+EO771XMCSzeHGY3rEj3HFHOHnpvPOgRo301ikiUhoFfYKvvgpHx+SF+/r1Ich79YKHHgrhfvrp6a5SRKRsjvmg3749HNc+c2Y4zn3XrjAEM2BAGJK5/PJwSKSISHV1TAb9mjUFvfa5c+HgwXCy0siRodfer184mUlEJA6OiaA/dAgWLCjYmbp0aZjeuXO4CuSQIXDuuRpvF5F4im3Q79sXrv6Ydz2ZjRtDkF9wATz8cAj3U09Nd5UiIhUvVkG/bVu4bvuMGTB7NuzeHS7pO3BgCPZBg8Ilf0VEjiWxCfrVq8MRMYcOwUknwbe/HcK9b1+Nt4vIsS02Qd+hQ7jE78UXQ1aWxttFRPLEJujN4M47012FiEjVo36viEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5lIKejMbaGYfmdkqMxtfxPx2ZvaamS0xs9fNrE3CvGvMbGX0c015Fi8iIqUrNejNrCbwKHAZ0AkYaWadkpo9BDzl7l2Be4H7o9c2BSYC5wE9gIlm1qT8yhcRkdKk0qPvAaxy99Xuvh+YBgxNatMJeC16PCdh/gDgVXff7u47gFeBgUdftoiIpCqVoG8NfJrwPCealmgxcFX0+EqggZk1S/G1mNk4M1tgZgu2bNmSau0iIpKCVILeipjmSc/vAC4ys/eBi4D1QG6Kr8Xdp7h7trtnt2jRIoWSREQkVancMzYHODnheRtgQ2IDd98ADAMws/rAVe6+08xygD5Jr339KOoVEZEySqVHPx843cw6mFltYAQwM7GBmTU3s7xl3QU8Hj2eDfQ3sybRTtj+0TQREakkpQa9u+cCNxEC+kNgursvM7N7zWxI1KwP8JGZrQBOACZHr90O3Ef4YzEfuDeaJiIilcTcDxsyT6vs7GxfsGBBussQEalWzGyhu2cXNU9nxoqIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMwp6EVEYk5BLyIScwp6EZGYU/8lCQsAAAqMSURBVNCLiMScgl5EJOYU9CIiMaegFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMwp6EVEYk5BLyIScwp6EZGYU9CLiMScgl5EJOYU9CIiMaegFxGJuZSC3swGmtlHZrbKzMYXMb+tmc0xs/fNbImZDYqm1zKzJ83sAzP70MzuKu8PICIiJSs16M2sJvAocBnQCRhpZp2Smt0DTHf3s4ERwP9E078BHO/uXYBzgO+bWfvyKV1ERFKRSo++B7DK3Ve7+35gGjA0qY0DDaPHjYANCdPrmdlxQB1gP/DFUVctIiIpSyXoWwOfJjzPiaYlmgSMNrMcYBZwczT9WWA3sBFYBzzk7tuT38DMxpnZAjNbsGXLlrJ9AhERKVEqQW9FTPOk5yOB37t7G2AQ8LSZ1SB8GzgInAR0AG43s1MOW5j7FHfPdvfsFi1alOkDiIhIyVIJ+hzg5ITnbSgYmsnzPWA6gLv/E8gAmgPfAl5x9wPu/hnwDyD7aIsWEZHUpRL084HTzayDmdUm7GydmdRmHdAPwMw6EoJ+SzT9YgvqAT2Bf5VX8SIiUrpSg97dc4GbgNnAh4Sja5aZ2b1mNiRqdjtwnZktBp4Bxri7E47WqQ8sJfzBeMLdl1TA5xARkWJYyOOqIzs72xcsWJDuMkREqhUzW+juRQ6N68xYEZGYU9CLiMScgl5EJOYU9CIiMaegFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMwp6EVEYk5BLyIScwp6EZGYU9CLiMScgl5EJOYU9CIiMaegFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGXUtCb2UAz+8jMVpnZ+CLmtzWzOWb2vpktMbNBCfO6mtk/zWyZmX1gZhnl+QFERKRkx5XWwMxqAo8ClwI5wHwzm+nuyxOa3QNMd/dfm1knYBbQ3syOA/4AfNvdF5tZM+BAuX8KEREpVio9+h7AKndf7e77gWnA0KQ2DjSMHjcCNkSP+wNL3H0xgLtvc/eDR1+2iIikKpWgbw18mvA8J5qWaBIw2sxyCL35m6PpZwBuZrPN7D0zu7OoNzCzcWa2wMwWbNmypUwfQERESpZK0FsR0zzp+Ujg9+7eBhgEPG1mNQhDQ72BUdG/V5pZv8MW5j7F3bPdPbtFixZl+gAiIlKyVII+Bzg54XkbCoZm8nwPmA7g7v8EMoDm0WvfcPet7r6H0NvPOtqiRUQkdakE/XzgdDPrYGa1gRHAzKQ264B+AGbWkRD0W4DZQFczqxvtmL0IWI6IiFSaUo+6cfdcM7uJENo1gcfdfZmZ3QsscPeZwO3Ab83sNsKwzhh3d2CHmT1M+GPhwCx3f6miPoyIiBzOQh5XHdnZ2b5gwYJ0lyEiUq2Y2UJ3zy5qns6MFRGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEXGyCfupUaN8eatQI/06dmu6KRESqhlJPmKoOpk6FceNgz57w/JNPwnOAUaPSV5eISFUQix79hAkFIZ9nz54wXUTkWBeLoF+3rmzTRUSOJbEI+rZtyzZdRORYEougnzwZ6tYtPK1u3TBdRORYF4ugHzUKpkyBdu3ALPw7ZYp2xIqIQEyOuoEQ6gp2EZHDxaJHLyIixVPQi4jEnIJeRCTmFPQiIjGnoBcRibkqd89YM9sCfHIUi2gObC2ncsqT6iob1VU2qqts4lhXO3dvUdSMKhf0R8vMFhR3g9x0Ul1lo7rKRnWVzbFWl4ZuRERiTkEvIhJzcQz6KekuoBiqq2xUV9morrI5puqK3Ri9iIgUFscevYiIJFDQi4jEXLUJejN73Mw+M7Olxcw3M/ulma0ysyVmlpUw7xozWxn9XFPJdY2K6lliZm+bWbeEeWvN7AMzW2RmCyq5rj5mtjN670Vm9uOEeQPN7KNoXY6v5Lr+X0JNS83soJk1jeZV5Po62czmmNmHZrbMzP6tiDaVuo2lWFO6tq9Uaqv0bSzFuip9GzOzDDObZ2aLo7p+UkSb483sT9E6edfM2ifMuyua/pGZDShzAe5eLX6AC4EsYGkx8wcBLwMG9ATejaY3BVZH/zaJHjepxLq+lvd+wGV5dUXP1wLN07S++gAvFjG9JvAxcApQG1gMdKqsupLaDgb+XknrqxWQFT1uAKxI/tyVvY2lWFO6tq9Uaqv0bSyVutKxjUXbTP3ocS3gXaBnUpsbgP+NHo8A/hQ97hSto+OBDtG6q1mW9682PXp3nwtsL6HJUOApD94BGptZK2AA8Kq7b3f3HcCrwMDKqsvd347eF+AdoE15vffR1FWCHsAqd1/t7vuBaYR1m466RgLPlNd7l8TdN7r7e9HjXcCHQOukZpW6jaVSUxq3r1TWV3EqbBs7groqZRuLtpkvo6e1op/kI2GGAk9Gj58F+pmZRdOnuftX7r4GWEVYhymrNkGfgtbApwnPc6JpxU1Ph+8ReoR5HPirmS00s3FpqOf86Kvky2bWOZpWJdaXmdUlhOVzCZMrZX1FX5nPJvS6EqVtGyuhpkRp2b5KqS1t21hp66yytzEzq2lmi4DPCB2DYrcvd88FdgLNKIf1FZs7TBG+GiXzEqZXKjPrS/iP2Dthci9332BmLYFXzexfUY+3MrxHuDbGl2Y2CPgLcDpVZH0RvlL/w90Te/8Vvr7MrD7hP/6t7v5F8uwiXlLh21gpNeW1Scv2VUptadvGUllnVPI25u4Hge5m1hh43swy3T1xX1WFbV9x6tHnACcnPG8DbChheqUxs67AY8BQd9+WN93dN0T/fgY8Txm/jh0Nd/8i76uku88CaplZc6rA+oqMIOkrdUWvLzOrRQiHqe7+5yKaVPo2lkJNadu+SqstXdtYKussUunbWLTsz4HXOXx4L3+9mNlxQCPCMOfRr6/y3ulQkT9Ae4rfuXg5hXeUzYumNwXWEHaSNYkeN63EutoSxtS+ljS9HtAg4fHbwMBKrOtECk6Y6wGsi9bdcYSdiR0o2FHWubLqiubnbeD1Kmt9RZ/9KeCREtpU6jaWYk1p2b5SrK3St7FU6krHNga0ABpHj+sAbwJXJLW5kcI7Y6dHjztTeGfsasq4M7baDN2Y2TOEvfjNzSwHmEjYoYG7/y8wi3BUxCpgDzA2mrfdzO4D5keLutcLf1Wr6Lp+TBhn+5+wX4VcD1enO4Hw9Q3Chv9Hd3+lEusaDlxvZrnAXmCEh60q18xuAmYTjo543N2XVWJdAFcCf3X33QkvrdD1BfQCvg18EI2jAtxNCNJ0bWOp1JSW7SvF2tKxjaVSF1T+NtYKeNLMahJGUqa7+4tmdi+wwN1nAr8DnjazVYQ/QiOimpeZ2XRgOZAL3OhhGChlugSCiEjMxWmMXkREiqCgFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jE3P8HpJybaaURugcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc'] \n",
    "val_acc = history.history['val_acc'] \n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
    "plt.title('Training and validation accuracy') \n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
    "plt.title('Training and validation loss') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 3s 788us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9405134320259094"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating your System\n",
    "\n",
    "You will use the official script to evaluate the performance of your system\n",
    "\n",
    "1. Use the predict method to predict the tags of the whole test set\n",
    "2. Write your results in a file, where the two last columns will be the hand-annotated tag and the predicted tag. The fields must be separated by a space.\n",
    "3. Apply conlleval to your output. Report the F1 result.\n",
    "4. Try to improve your model by modifying some parameters, adding layers, adding Bidirectional and Dropout.\n",
    "5. Evaluate your network again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3684, 150, 11)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "np.shape(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_num = []\n",
    "for sent_nbr, sent_ner_predictions in enumerate(y_pred):\n",
    "    ner_pred_num += [sent_ner_predictions[-len(X_words_test[sent_nbr]):]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert NER indices to symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']]\n",
      "[['O'], ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "ner_pred = []\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence))\n",
    "    ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx))\n",
    "    ner_pred += [ner_pred_cat]\n",
    "\n",
    "print(ner_pred[:2])\n",
    "print(Y_ner_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two last columns will be the hand-annotated tag and the predicted tag, save to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = open('RNN', 'w')\n",
    "for i in range(len(X_words_test)): # For each sentence\n",
    "    for j in range(len(X_words_test[i])): # Fore each word\n",
    "        word = str(X_words_test[i][j])\n",
    "        NER = str(Y_ner_test[i][j]) \n",
    "        PNER = str(ner_pred[i][j])\n",
    "        f_out.write(word + ' ' + NER + ' ' + PNER + '\\n')\n",
    "    f_out.write('\\n')  \n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conlleval applied to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5648 phrases; found: 5362 phrases; correct: 3915.\n",
      "accuracy:  94.05%; precision:  73.01%; recall:  69.32%; FB1:  71.12\n",
      "              LOC: precision:  76.00%; recall:  80.10%; FB1:  77.99  1758\n",
      "             MISC: precision:  63.51%; recall:  50.57%; FB1:  56.30  559\n",
      "              ORG: precision:  63.44%; recall:  56.11%; FB1:  59.55  1469\n",
      "              PER: precision:  81.98%; recall:  79.90%; FB1:  80.93  1576\n"
     ]
    }
   ],
   "source": [
    "!perl ./conlleval.pl <RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a LSTM Network\n",
    "\n",
    "1. Create a simple LSTM network and train a model with the train set. As layers, you will use Embedding, LSTM, and Dense.\n",
    "2. Apply conlleval to your output. Report the F1 result.\n",
    "3. Try to improve your model by modifying some parameters, adding layers, adding Bidirectional, Dropout, possibly mixing SimpleRNN.\n",
    "4. Apply your network to the test set and report the accuracy you obtained. you need to reach a F1 of 82 to pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabulary_size = len(voc) + 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(text_vocabulary_size,\n",
    "                    embedding_dim,\n",
    "                    input_length=150,\n",
    "                    mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(100, return_sequences=True)))\n",
    "model.add(Dense(100*2, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(len(ner_set) + 2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained word embeddings into the Embedding layer (frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/12\n",
      "14987/14987 [==============================] - 206s 14ms/step - loss: 0.0402 - acc: 0.8842 - val_loss: 0.0301 - val_acc: 0.9178\n",
      "Epoch 2/12\n",
      "14987/14987 [==============================] - 208s 14ms/step - loss: 0.0155 - acc: 0.9502 - val_loss: 0.0137 - val_acc: 0.9605\n",
      "Epoch 3/12\n",
      "14987/14987 [==============================] - 198s 13ms/step - loss: 0.0114 - acc: 0.9626 - val_loss: 0.0368 - val_acc: 0.8886\n",
      "Epoch 4/12\n",
      "14987/14987 [==============================] - 203s 14ms/step - loss: 0.0093 - acc: 0.9687 - val_loss: 0.0121 - val_acc: 0.9642\n",
      "Epoch 5/12\n",
      "14987/14987 [==============================] - 204s 14ms/step - loss: 0.0077 - acc: 0.9741 - val_loss: 0.0128 - val_acc: 0.9671\n",
      "Epoch 6/12\n",
      "14987/14987 [==============================] - 201s 13ms/step - loss: 0.0065 - acc: 0.9775 - val_loss: 0.0102 - val_acc: 0.9718\n",
      "Epoch 7/12\n",
      "14987/14987 [==============================] - 200s 13ms/step - loss: 0.0056 - acc: 0.9808 - val_loss: 0.0107 - val_acc: 0.9698\n",
      "Epoch 8/12\n",
      "14987/14987 [==============================] - 199s 13ms/step - loss: 0.0046 - acc: 0.9840 - val_loss: 0.0121 - val_acc: 0.9697\n",
      "Epoch 9/12\n",
      "14987/14987 [==============================] - 198s 13ms/step - loss: 0.0039 - acc: 0.9865 - val_loss: 0.0122 - val_acc: 0.9719\n",
      "Epoch 10/12\n",
      "14987/14987 [==============================] - 200s 13ms/step - loss: 0.0032 - acc: 0.9889 - val_loss: 0.0189 - val_acc: 0.9658\n",
      "Epoch 11/12\n",
      "14987/14987 [==============================] - 192s 13ms/step - loss: 0.0027 - acc: 0.9905 - val_loss: 0.0149 - val_acc: 0.9719\n",
      "Epoch 12/12\n",
      "14987/14987 [==============================] - 195s 13ms/step - loss: 0.0022 - acc: 0.9920 - val_loss: 0.0128 - val_acc: 0.9735\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    epochs=12, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val, Y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c48d82be3f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['acc'] \n",
    "val_acc = history.history['val_acc'] \n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
    "plt.title('Training and validation accuracy') \n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
    "plt.title('Training and validation loss') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicts the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6e78b272c160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "np.shape(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred_num = []\n",
    "for sent_nbr, sent_ner_predictions in enumerate(y_pred):\n",
    "    ner_pred_num += [sent_ner_predictions[-len(X_words_test[sent_nbr]):]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert NER indices to symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pred = []\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence))\n",
    "    ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx))\n",
    "    ner_pred += [ner_pred_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = open('LSTM', 'w')\n",
    "for i in range(len(X_words_test)): # For each sentence\n",
    "    for j in range(len(X_words_test[i])): # Fore each word\n",
    "        word = str(X_words_test[i][j])\n",
    "        NER = str(Y_ner_test[i][j]) \n",
    "        PNER = str(ner_pred[i][j])\n",
    "        f_out.write(word + ' ' + NER + ' ' + PNER + '\\n')\n",
    "    f_out.write('\\n')  \n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conlleval applied to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5648 phrases; found: 5819 phrases; correct: 4721.\n",
      "accuracy:  96.37%; precision:  81.13%; recall:  83.59%; FB1:  82.34\n",
      "              LOC: precision:  83.44%; recall:  89.99%; FB1:  86.59  1799\n",
      "             MISC: precision:  68.73%; recall:  67.95%; FB1:  68.34  694\n",
      "              ORG: precision:  75.06%; recall:  78.27%; FB1:  76.63  1732\n",
      "              PER: precision:  90.53%; recall:  89.24%; FB1:  89.88  1594\n"
     ]
    }
   ],
   "source": [
    "!perl ./conlleval.pl <LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
