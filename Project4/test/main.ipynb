{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting a Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You will use a dataset from the CoNLL conferences that benchmark natural language processing systems and tasks. There were two conferences on named entity recognition: CoNLL 2002 (Spanish and Dutch) and CoNLL 2003 (English and German). In this assignment, you will work on the English dataset. Read the description of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The datasets are protected by a license and you need to obtain it to reconstruct the data. Alternatively, you can use a local copy or try to find one on github (type conll2003 in the search box) or use the Google dataset search: https://toolbox.google.com/datasetsearch. You can find a local copy in the /usr/local/cs/EDAN95/datasets/NER-data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The dataset comes in the form of three files: a training set, a development set, and a test set. You will use the test set to evaluate your models. For this, you will apply the conlleval script that will compute the harmonic mean of the precision and recall: F1. You have a local copy of this script in /usr/local/cs/EDAN95/datasets/ner/bin. conlleval is written in Perl. Be sure to have it on your machine to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the GloVe embeddings 6B from https://nlp.stanford.edu/projects/glove/ and keep the 100d vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '/Users/Marcel/Documents/Python/edan95/project_4/glove.6b'\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using a cosine similarity, compute the 5 closest words to the words table, france, and sweden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8076423, 'belgium'), (0.8004377, 'french'), (0.79505277, 'britain'), (0.7557464, 'spain'), (0.74815863, 'paris')]\n",
      "[(0.8624401, 'denmark'), (0.80732495, 'norway'), (0.7906495, 'finland'), (0.74684644, 'netherlands'), (0.74668366, 'austria')]\n",
      "[(0.80211616, 'tables'), (0.6582379, 'place'), (0.65597206, 'bottom'), (0.65436906, 'room'), (0.6433667, 'side')]\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    \"\"\"Measures the cosine similiarity between two vectors\n",
    "    \"\"\"\n",
    "    den = np.dot(v1, v2)\n",
    "    num = np.linalg.norm(v1)*np.linalg.norm(v2)\n",
    "    return den/num\n",
    "\n",
    "\n",
    "def find_closest_words(dictonary, word, n):\n",
    "    word_list = [(cos_sim(dictonary[word], value), key) for key, value in embeddings_index.items()]\n",
    "    sorted_list = sorted(word_list, key=lambda tup: tup[0], reverse=True)\n",
    "    return sorted_list[0:n-1]\n",
    "\n",
    "closest_2_france = find_closest_words(embeddings_index, 'france', 7)\n",
    "print(closest_2_france[1:])\n",
    "\n",
    "closest_2_sweden = find_closest_words(embeddings_index, 'sweden', 7)\n",
    "print(closest_2_sweden[1:])\n",
    "\n",
    "closest_2_table = find_closest_words(embeddings_index, 'table', 7)\n",
    "print(closest_2_table[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Corpus and Building Indices\n",
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that for each sentence returns the X and Y lists of symbols consisting of words and NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras import Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n",
      "First sentence, words ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "First sentence, NER ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "21010\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '/Users/Marcel/Documents/Python/edan95/project_4/conll003-englishversion/'\n",
    "\n",
    "# Reading the corpus\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + 'train.txt'\n",
    "    dev_file = BASE_DIR + 'valid.txt'\n",
    "    test_file = BASE_DIR + 'test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "# The dictorizer that transforms the CoNLL files into dictionaries¶\n",
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]\n",
    "    \n",
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "print(train_dict[0])\n",
    "print(train_dict[1])\n",
    "\n",
    "# Building the sequences¶\n",
    "\n",
    "## The function to build the sequences\n",
    "def build_sequences(corpus_dict, key_x='form', key_y='pos', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = [word[key_x] for word in sentence]\n",
    "        y = [word[key_y] for word in sentence]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y\n",
    "\n",
    "## Train dataset\n",
    "\n",
    "# We build the words and NER sequence tags¶\n",
    "X_words, Y_ner = build_sequences(train_dict, key_x='form', key_y='ner')\n",
    "print('First sentence, words', X_words[1])\n",
    "print('First sentence, NER', Y_ner[1])\n",
    "\n",
    "\n",
    "# We now extract the list of unique words and NER\n",
    "word_set = sorted(list(set([item for sublist in X_words for item in sublist])))\n",
    "ner_set = sorted(list(set([item for sublist in Y_ner for item in sublist])))\n",
    "print(len(word_set))\n",
    "print(len(ner_set))\n",
    "ner_set\n",
    "\n",
    "\n",
    "## Same but for dev dataset\n",
    "X_words_dev, Y_ner_dev = build_sequences(dev_dict, key_x='form', key_y='ner')\n",
    "\n",
    "# Extract the list of unique words and NER and vocab including glove \n",
    "word_set_dev = sorted(list(set([item for sublist in X_words_dev for item in sublist])))\n",
    "ner_set_dev = sorted(list(set([item for sublist in Y_ner_dev for item in sublist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a vocabulary of all the words observed in the training set and the words in GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'aaa', 'aaaa', 'aaaaa', 'aaah', 'aaahh', 'aaai', 'aaas', 'aab', 'aaba', 'aabar', 'aabb', 'aabel', 'aabenraa', 'aaberg', 'aac', 'aacc', 'aach', 'aachen']\n"
     ]
    }
   ],
   "source": [
    "glove_set = sorted([key for key in embeddings_index.keys() if key.isalpha()])\n",
    "print(glove_set[0:20])\n",
    "\n",
    "glove_set.extend(word_set)\n",
    "#vocab = sorted(list(set(glove_set + word_set)))\n",
    "\n",
    "#print(len(vocab))\n",
    "voc = set(glove_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create indices and inverted indices for the words and the NER: i.e. you will associate each word with a number. You will use index 0 for the padding symbol and 1 for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'B-LOC', 3: 'B-MISC', 4: 'B-ORG', 5: 'B-PER', 6: 'I-LOC', 7: 'I-MISC', 8: 'I-ORG', 9: 'I-PER', 10: 'O'}\n",
      "{'B-LOC': 2, 'B-MISC': 3, 'B-ORG': 4, 'B-PER': 5, 'I-LOC': 6, 'I-MISC': 7, 'I-ORG': 8, 'I-PER': 9, 'O': 10}\n"
     ]
    }
   ],
   "source": [
    "## Train dataset\n",
    "rev_word_idx = dict(enumerate(word_set, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner_set, start=2))\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "\n",
    "rev_word_idx[0] = 0\n",
    "rev_word_idx[1] = 1\n",
    "\n",
    "print(rev_ner_idx)\n",
    "print(ner_idx)\n",
    "\n",
    "## Dev dataset\n",
    "rev_word_idx_dev = dict(enumerate(voc, start=2))\n",
    "rev_ner_idx_dev = dict(enumerate(ner_set_dev, start=2))\n",
    "rev_word_idx_dev[0]=0\n",
    "rev_word_idx_dev[1]=1\n",
    "word_idx_dev = {v: k for k, v in rev_word_idx_dev.items()}\n",
    "ner_idx_dev = {v: k for k, v in rev_ner_idx_dev.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 2,\n",
       " '\"': 3,\n",
       " '$': 4,\n",
       " '%': 5,\n",
       " '&': 6,\n",
       " \"'\": 7,\n",
       " \"'d\": 8,\n",
       " \"'ll\": 9,\n",
       " \"'m\": 10,\n",
       " \"'re\": 11,\n",
       " \"'s\": 12,\n",
       " \"'ve\": 13,\n",
       " '(': 14,\n",
       " ')': 15,\n",
       " '*': 16,\n",
       " '**': 17,\n",
       " '**general': 18,\n",
       " '*indices': 19,\n",
       " '*name': 20,\n",
       " '*note': 21,\n",
       " '+': 22,\n",
       " '++359-2': 23,\n",
       " '+0,2': 24,\n",
       " '+0.0': 25,\n",
       " '+0.05': 26,\n",
       " '+0.1': 27,\n",
       " '+0.2': 28,\n",
       " '+0.4m': 29,\n",
       " '+0.7': 30,\n",
       " '+0.9;+23.6': 31,\n",
       " '+1': 32,\n",
       " '+1,161': 33,\n",
       " '+1.0': 34,\n",
       " '+1.2': 35,\n",
       " '+1.5': 36,\n",
       " '+1.7;+22.0': 37,\n",
       " '+1.9': 38,\n",
       " '+10.8': 39,\n",
       " '+11': 40,\n",
       " '+12': 41,\n",
       " '+12,696': 42,\n",
       " '+15,272': 43,\n",
       " '+16': 44,\n",
       " '+16.4': 45,\n",
       " '+167,330': 46,\n",
       " '+168,130': 47,\n",
       " '+17': 48,\n",
       " '+18.3': 49,\n",
       " '+19': 50,\n",
       " '+2': 51,\n",
       " '+2.3r': 52,\n",
       " '+2.6': 53,\n",
       " '+20': 54,\n",
       " '+22': 55,\n",
       " '+225': 56,\n",
       " '+230.4': 57,\n",
       " '+25': 58,\n",
       " '+27': 59,\n",
       " '+282.1': 60,\n",
       " '+3': 61,\n",
       " '+3,428': 62,\n",
       " '+3,831': 63,\n",
       " '+3.06': 64,\n",
       " '+3.4': 65,\n",
       " '+3.5': 66,\n",
       " '+3.6': 67,\n",
       " '+3.7': 68,\n",
       " '+301': 69,\n",
       " '+31': 70,\n",
       " '+31,230': 71,\n",
       " '+310.4': 72,\n",
       " '+33': 73,\n",
       " '+331': 74,\n",
       " '+34': 75,\n",
       " '+353': 76,\n",
       " '+358': 77,\n",
       " '+361': 78,\n",
       " '+381': 79,\n",
       " '+387-71-663-864': 80,\n",
       " '+4': 81,\n",
       " '+4.2r': 82,\n",
       " '+4.7': 83,\n",
       " '+400.9': 84,\n",
       " '+41': 85,\n",
       " '+431': 86,\n",
       " '+44': 87,\n",
       " '+45': 88,\n",
       " '+46-8-700': 89,\n",
       " '+47': 90,\n",
       " '+48': 91,\n",
       " '+49': 92,\n",
       " '+5': 93,\n",
       " '+5.2': 94,\n",
       " '+525': 95,\n",
       " '+541': 96,\n",
       " '+6': 97,\n",
       " '+6221': 98,\n",
       " '+65': 99,\n",
       " '+65-8703086': 100,\n",
       " '+6613377': 101,\n",
       " '+7': 102,\n",
       " '+7.1': 103,\n",
       " '+7.3;-3.6': 104,\n",
       " '+7095': 105,\n",
       " '+8': 106,\n",
       " '+9': 107,\n",
       " '+9.8': 108,\n",
       " '+90-212-275': 109,\n",
       " '+91-11-3012024': 110,\n",
       " '+91-22-265': 111,\n",
       " ',': 112,\n",
       " '-': 113,\n",
       " '--': 114,\n",
       " '---': 115,\n",
       " '-----': 116,\n",
       " '---------------------': 117,\n",
       " '------------------------': 118,\n",
       " '-------------------------': 119,\n",
       " '--------------------------------': 120,\n",
       " '----------------------------------': 121,\n",
       " '------------------------------------------------------------': 122,\n",
       " '-------------------------------------------------------------': 123,\n",
       " '-docstart-': 124,\n",
       " '.': 125,\n",
       " '..': 126,\n",
       " '...': 127,\n",
       " '....': 128,\n",
       " '...and': 129,\n",
       " '...for': 130,\n",
       " '.01': 131,\n",
       " '.02': 132,\n",
       " '.10': 133,\n",
       " '.22': 134,\n",
       " '.307': 135,\n",
       " '.349': 136,\n",
       " '.353': 137,\n",
       " '.354': 138,\n",
       " '.356': 139,\n",
       " '.357': 140,\n",
       " '.359': 141,\n",
       " '.362': 142,\n",
       " '.367': 143,\n",
       " '.402': 144,\n",
       " '.403': 145,\n",
       " '.405': 146,\n",
       " '.406': 147,\n",
       " '.407': 148,\n",
       " '.408': 149,\n",
       " '.409': 150,\n",
       " '.411': 151,\n",
       " '.413': 152,\n",
       " '.421': 153,\n",
       " '.423': 154,\n",
       " '.424': 155,\n",
       " '.425': 156,\n",
       " '.426': 157,\n",
       " '.427': 158,\n",
       " '.429': 159,\n",
       " '.430': 160,\n",
       " '.431': 161,\n",
       " '.432': 162,\n",
       " '.434': 163,\n",
       " '.435': 164,\n",
       " '.436': 165,\n",
       " '.439': 166,\n",
       " '.443': 167,\n",
       " '.444': 168,\n",
       " '.446': 169,\n",
       " '.447': 170,\n",
       " '.450': 171,\n",
       " '.451': 172,\n",
       " '.453': 173,\n",
       " '.454': 174,\n",
       " '.455': 175,\n",
       " '.456': 176,\n",
       " '.457': 177,\n",
       " '.459': 178,\n",
       " '.460': 179,\n",
       " '.461': 180,\n",
       " '.462': 181,\n",
       " '.463': 182,\n",
       " '.465': 183,\n",
       " '.466': 184,\n",
       " '.467': 185,\n",
       " '.468': 186,\n",
       " '.469': 187,\n",
       " '.470': 188,\n",
       " '.471': 189,\n",
       " '.473': 190,\n",
       " '.474': 191,\n",
       " '.477': 192,\n",
       " '.478': 193,\n",
       " '.481': 194,\n",
       " '.492': 195,\n",
       " '.496': 196,\n",
       " '.500': 197,\n",
       " '.504': 198,\n",
       " '.508': 199,\n",
       " '.511': 200,\n",
       " '.512': 201,\n",
       " '.515': 202,\n",
       " '.516': 203,\n",
       " '.519': 204,\n",
       " '.520': 205,\n",
       " '.522': 206,\n",
       " '.523': 207,\n",
       " '.524': 208,\n",
       " '.526': 209,\n",
       " '.527': 210,\n",
       " '.528': 211,\n",
       " '.530': 212,\n",
       " '.531': 213,\n",
       " '.532': 214,\n",
       " '.533': 215,\n",
       " '.534': 216,\n",
       " '.535': 217,\n",
       " '.536': 218,\n",
       " '.537': 219,\n",
       " '.538': 220,\n",
       " '.539': 221,\n",
       " '.540': 222,\n",
       " '.542': 223,\n",
       " '.543': 224,\n",
       " '.545': 225,\n",
       " '.547': 226,\n",
       " '.549': 227,\n",
       " '.551': 228,\n",
       " '.552': 229,\n",
       " '.561': 230,\n",
       " '.564': 231,\n",
       " '.565': 232,\n",
       " '.568': 233,\n",
       " '.569': 234,\n",
       " '.571': 235,\n",
       " '.573': 236,\n",
       " '.574': 237,\n",
       " '.575': 238,\n",
       " '.576': 239,\n",
       " '.578': 240,\n",
       " '.589': 241,\n",
       " '.592': 242,\n",
       " '.594': 243,\n",
       " '.595': 244,\n",
       " '.598': 245,\n",
       " '.602': 246,\n",
       " '.604': 247,\n",
       " '.606': 248,\n",
       " '.610': 249,\n",
       " '.623': 250,\n",
       " '.626': 251,\n",
       " '.627': 252,\n",
       " '.628': 253,\n",
       " '.630': 254,\n",
       " '.632': 255,\n",
       " '.633': 256,\n",
       " '.8787': 257,\n",
       " '.giuseppe': 258,\n",
       " '.robbie': 259,\n",
       " '/': 260,\n",
       " '0': 261,\n",
       " '0*': 262,\n",
       " '0-0': 263,\n",
       " '0-1': 264,\n",
       " '0-2': 265,\n",
       " '0-50': 266,\n",
       " '0-6': 267,\n",
       " '0.': 268,\n",
       " '0.005': 269,\n",
       " '0.01': 270,\n",
       " '0.02': 271,\n",
       " '0.03': 272,\n",
       " '0.04': 273,\n",
       " '0.042': 274,\n",
       " '0.05': 275,\n",
       " '0.056': 276,\n",
       " '0.08': 277,\n",
       " '0.0817': 278,\n",
       " '0.0822': 279,\n",
       " '0.1': 280,\n",
       " '0.10': 281,\n",
       " '0.100': 282,\n",
       " '0.12': 283,\n",
       " '0.13': 284,\n",
       " '0.14': 285,\n",
       " '0.15': 286,\n",
       " '0.17': 287,\n",
       " '0.19': 288,\n",
       " '0.2': 289,\n",
       " '0.20': 290,\n",
       " '0.20-0.70': 291,\n",
       " '0.200': 292,\n",
       " '0.21': 293,\n",
       " '0.23': 294,\n",
       " '0.25': 295,\n",
       " '0.25-1.00': 296,\n",
       " '0.25-1.30': 297,\n",
       " '0.27': 298,\n",
       " '0.275': 299,\n",
       " '0.3': 300,\n",
       " '0.30': 301,\n",
       " '0.300': 302,\n",
       " '0.31': 303,\n",
       " '0.33': 304,\n",
       " '0.34': 305,\n",
       " '0.35': 306,\n",
       " '0.355': 307,\n",
       " '0.36': 308,\n",
       " '0.37': 309,\n",
       " '0.375': 310,\n",
       " '0.38': 311,\n",
       " '0.39': 312,\n",
       " '0.4': 313,\n",
       " '0.4-0-9-0': 314,\n",
       " '0.40': 315,\n",
       " '0.41': 316,\n",
       " '0.43': 317,\n",
       " '0.44': 318,\n",
       " '0.445': 319,\n",
       " '0.45': 320,\n",
       " '0.46': 321,\n",
       " '0.47': 322,\n",
       " '0.48': 323,\n",
       " '0.5': 324,\n",
       " '0.50': 325,\n",
       " '0.53': 326,\n",
       " '0.55': 327,\n",
       " '0.59': 328,\n",
       " '0.6': 329,\n",
       " '0.60': 330,\n",
       " '0.63': 331,\n",
       " '0.65': 332,\n",
       " '0.68': 333,\n",
       " '0.69': 334,\n",
       " '0.7': 335,\n",
       " '0.70': 336,\n",
       " '0.73': 337,\n",
       " '0.75': 338,\n",
       " '0.79': 339,\n",
       " '0.8': 340,\n",
       " '0.81': 341,\n",
       " '0.85': 342,\n",
       " '0.8584': 343,\n",
       " '0.86': 344,\n",
       " '0.94': 345,\n",
       " '0.95': 346,\n",
       " '0.958': 347,\n",
       " '0.966': 348,\n",
       " '0.98': 349,\n",
       " '00--44-171-542-7947': 350,\n",
       " '00.42': 351,\n",
       " '000': 352,\n",
       " '000s': 353,\n",
       " '0025': 354,\n",
       " '003': 355,\n",
       " '01.77': 356,\n",
       " '0100': 357,\n",
       " '0120': 358,\n",
       " '0172': 359,\n",
       " '02.777': 360,\n",
       " '02.sep.96-06.mar.97': 361,\n",
       " '02.sep.96-21.nov.97': 362,\n",
       " '02/09': 363,\n",
       " '020-504-5040': 364,\n",
       " '03.684': 365,\n",
       " '03/09': 366,\n",
       " '0300': 367,\n",
       " '0330': 368,\n",
       " '04': 369,\n",
       " '04/06/96': 370,\n",
       " '0400': 371,\n",
       " '05/01/97': 372,\n",
       " '05/09': 373,\n",
       " '0515': 374,\n",
       " '05:30': 375,\n",
       " '05:53': 376,\n",
       " '06': 377,\n",
       " '06/01': 378,\n",
       " '0645': 379,\n",
       " '07.00': 380,\n",
       " '08/21/96': 381,\n",
       " '08/23/96': 382,\n",
       " '08/26/96': 383,\n",
       " '08/27/96': 384,\n",
       " '08/28/96': 385,\n",
       " '08/29/1996': 386,\n",
       " '0800': 387,\n",
       " '0815': 388,\n",
       " '0875': 389,\n",
       " '09/01/96': 390,\n",
       " '09/04/96': 391,\n",
       " '09/05/1996': 392,\n",
       " '0900': 393,\n",
       " '0915': 394,\n",
       " '0:04': 395,\n",
       " '0:06': 396,\n",
       " '0:07': 397,\n",
       " '0:08': 398,\n",
       " '0:09': 399,\n",
       " '0:10': 400,\n",
       " '1': 401,\n",
       " '1)266': 402,\n",
       " '1,000': 403,\n",
       " '1,002': 404,\n",
       " '1,003': 405,\n",
       " '1,037': 406,\n",
       " '1,041': 407,\n",
       " '1,044': 408,\n",
       " '1,074': 409,\n",
       " '1,086': 410,\n",
       " '1,100': 411,\n",
       " '1,100.7': 412,\n",
       " '1,113,785': 413,\n",
       " '1,119.0': 414,\n",
       " '1,123': 415,\n",
       " '1,130': 416,\n",
       " '1,143': 417,\n",
       " '1,154': 418,\n",
       " '1,156.79': 419,\n",
       " '1,164.1': 420,\n",
       " '1,184.0': 421,\n",
       " '1,185': 422,\n",
       " '1,190': 423,\n",
       " '1,196': 424,\n",
       " '1,200': 425,\n",
       " '1,236.5': 426,\n",
       " '1,242.9': 427,\n",
       " '1,294.5': 428,\n",
       " '1,305': 429,\n",
       " '1,315.7': 430,\n",
       " '1,328': 431,\n",
       " '1,334.0': 432,\n",
       " '1,380': 433,\n",
       " '1,400': 434,\n",
       " '1,420.9': 435,\n",
       " '1,421.90': 436,\n",
       " '1,429': 437,\n",
       " '1,433.4': 438,\n",
       " '1,452-km': 439,\n",
       " '1,453': 440,\n",
       " '1,456.7': 441,\n",
       " '1,461': 442,\n",
       " '1,466.1': 443,\n",
       " '1,500': 444,\n",
       " '1,558': 445,\n",
       " '1,574,799': 446,\n",
       " '1,600': 447,\n",
       " '1,617': 448,\n",
       " '1,650': 449,\n",
       " '1,683': 450,\n",
       " '1,700': 451,\n",
       " '1,700-hectare': 452,\n",
       " '1,750-hectare': 453,\n",
       " '1,800': 454,\n",
       " '1,803': 455,\n",
       " '1,806': 456,\n",
       " '1,820,000': 457,\n",
       " '1,850-1,900': 458,\n",
       " '1,897.85': 459,\n",
       " '1,900': 460,\n",
       " '1,940': 461,\n",
       " '1-0': 462,\n",
       " '1-1': 463,\n",
       " '1-1/2': 464,\n",
       " '1-1/3': 465,\n",
       " '1-1/4': 466,\n",
       " '1-10-100': 467,\n",
       " '1-106': 468,\n",
       " '1-129': 469,\n",
       " '1-15': 470,\n",
       " '1-16': 471,\n",
       " '1-2': 472,\n",
       " '1-3': 473,\n",
       " '1-3/4': 474,\n",
       " '1-4': 475,\n",
       " '1-48': 476,\n",
       " '1-57': 477,\n",
       " '1-6': 478,\n",
       " '1-64': 479,\n",
       " '1-7': 480,\n",
       " '1-82': 481,\n",
       " '1-96': 482,\n",
       " '1-for-2': 483,\n",
       " '1-mth': 484,\n",
       " '1.': 485,\n",
       " '1.0': 486,\n",
       " '1.00': 487,\n",
       " '1.000': 488,\n",
       " '1.02': 489,\n",
       " '1.03': 490,\n",
       " '1.05': 491,\n",
       " '1.07': 492,\n",
       " '1.09': 493,\n",
       " '1.1': 494,\n",
       " '1.10': 495,\n",
       " '1.12': 496,\n",
       " '1.126': 497,\n",
       " '1.13': 498,\n",
       " '1.16': 499,\n",
       " '1.1875': 500,\n",
       " '1.19': 501,\n",
       " '1.2': 502,\n",
       " '1.20': 503,\n",
       " '1.206': 504,\n",
       " '1.209': 505,\n",
       " '1.210': 506,\n",
       " '1.210-1.236': 507,\n",
       " '1.215': 508,\n",
       " '1.24': 509,\n",
       " '1.26': 510,\n",
       " '1.3': 511,\n",
       " '1.33': 512,\n",
       " '1.343': 513,\n",
       " '1.3438': 514,\n",
       " '1.38': 515,\n",
       " '1.4': 516,\n",
       " '1.43': 517,\n",
       " '1.45': 518,\n",
       " '1.4765': 519,\n",
       " '1.4779': 520,\n",
       " '1.4788': 521,\n",
       " '1.4789': 522,\n",
       " '1.48': 523,\n",
       " '1.4871': 524,\n",
       " '1.4935': 525,\n",
       " '1.5': 526,\n",
       " '1.50': 527,\n",
       " '1.5497': 528,\n",
       " '1.55': 529,\n",
       " '1.5520': 530,\n",
       " '1.56': 531,\n",
       " '1.59': 532,\n",
       " '1.6': 533,\n",
       " '1.60': 534,\n",
       " '1.64': 535,\n",
       " '1.7': 536,\n",
       " '1.70': 537,\n",
       " '1.75': 538,\n",
       " '1.8': 539,\n",
       " '1.80': 540,\n",
       " '1.82': 541,\n",
       " '1.84': 542,\n",
       " '1.85': 543,\n",
       " '1.875': 544,\n",
       " '1.88': 545,\n",
       " '1.9': 546,\n",
       " '1.90': 547,\n",
       " '1.91': 548,\n",
       " '1.94': 549,\n",
       " '1.97': 550,\n",
       " '1/16': 551,\n",
       " '1/2': 552,\n",
       " '1/3': 553,\n",
       " '1/32': 554,\n",
       " '1/9': 555,\n",
       " '1/9/95': 556,\n",
       " '10': 557,\n",
       " '10,000': 558,\n",
       " '10,000-seat': 559,\n",
       " '10,056.4': 560,\n",
       " '10,119,000': 561,\n",
       " '10,204.87': 562,\n",
       " '10,300': 563,\n",
       " '10,650,407': 564,\n",
       " '10,655': 565,\n",
       " '10,663': 566,\n",
       " '10,725': 567,\n",
       " '10,756': 568,\n",
       " '10,760': 569,\n",
       " '10,880,000': 570,\n",
       " '10,896': 571,\n",
       " '10,925': 572,\n",
       " '10-0': 573,\n",
       " '10-0-42-0': 574,\n",
       " '10-0-44-1': 575,\n",
       " '10-0-52-0': 576,\n",
       " '10-0-53-3': 577,\n",
       " '10-0-56-1': 578,\n",
       " '10-0-59-1': 579,\n",
       " '10-1': 580,\n",
       " '10-1-36-2': 581,\n",
       " '10-1-39-1': 582,\n",
       " '10-1-40-0': 583,\n",
       " '10-1-47-2': 584,\n",
       " '10-1-50-1': 585,\n",
       " '10-1-54-0': 586,\n",
       " '10-12': 587,\n",
       " '10-15': 588,\n",
       " '10-2': 589,\n",
       " '10-2-51-0': 590,\n",
       " '10-23': 591,\n",
       " '10-3': 592,\n",
       " '10-3-30-0': 593,\n",
       " '10-3-31-1': 594,\n",
       " '10-3/8': 595,\n",
       " '10-30': 596,\n",
       " '10-4': 597,\n",
       " '10-5': 598,\n",
       " '10-7': 599,\n",
       " '10-8': 600,\n",
       " '10-day': 601,\n",
       " '10-game': 602,\n",
       " '10-session': 603,\n",
       " '10-week': 604,\n",
       " '10-year': 605,\n",
       " '10-year-old': 606,\n",
       " '10.': 607,\n",
       " '10.0': 608,\n",
       " '10.00': 609,\n",
       " '10.01': 610,\n",
       " '10.02': 611,\n",
       " '10.03': 612,\n",
       " '10.06': 613,\n",
       " '10.09': 614,\n",
       " '10.10': 615,\n",
       " '10.11': 616,\n",
       " '10.12': 617,\n",
       " '10.13': 618,\n",
       " '10.14': 619,\n",
       " '10.15': 620,\n",
       " '10.16': 621,\n",
       " '10.17': 622,\n",
       " '10.18': 623,\n",
       " '10.19': 624,\n",
       " '10.28': 625,\n",
       " '10.29': 626,\n",
       " '10.3': 627,\n",
       " '10.30': 628,\n",
       " '10.43': 629,\n",
       " '10.45': 630,\n",
       " '10.5': 631,\n",
       " '10.5-42.3': 632,\n",
       " '10.57:33': 633,\n",
       " '10.6': 634,\n",
       " '10.75': 635,\n",
       " '10.8': 636,\n",
       " '10.84': 637,\n",
       " '10.9': 638,\n",
       " '10.95': 639,\n",
       " '10.982': 640,\n",
       " '10/01': 641,\n",
       " '100': 642,\n",
       " '100,000': 643,\n",
       " '100,600': 644,\n",
       " '100-2': 645,\n",
       " '100-30': 646,\n",
       " '100.6': 647,\n",
       " '100.92': 648,\n",
       " '1000': 649,\n",
       " '1003': 650,\n",
       " '1006': 651,\n",
       " '1009': 652,\n",
       " '100=1990': 653,\n",
       " '100=1992': 654,\n",
       " '100m': 655,\n",
       " '100no': 656,\n",
       " '101': 657,\n",
       " '101-member': 658,\n",
       " '101-strong': 659,\n",
       " '101.40': 660,\n",
       " '101.80': 661,\n",
       " '102': 662,\n",
       " '102-0': 663,\n",
       " '1021': 664,\n",
       " '103-95': 665,\n",
       " '1036.575': 666,\n",
       " '104.625': 667,\n",
       " '1045.56': 668,\n",
       " '105': 669,\n",
       " '105,000': 670,\n",
       " '105-4': 671,\n",
       " '105.07': 672,\n",
       " '106': 673,\n",
       " '106.375': 674,\n",
       " '106.5': 675,\n",
       " '1060.00': 676,\n",
       " '107': 677,\n",
       " '107-06': 678,\n",
       " '107-10': 679,\n",
       " '107-12': 680,\n",
       " '107.0': 681,\n",
       " '107.55': 682,\n",
       " '107.74': 683,\n",
       " '107.78': 684,\n",
       " '10700': 685,\n",
       " '10752.092': 686,\n",
       " '10772': 687,\n",
       " '108': 688,\n",
       " '108,000': 689,\n",
       " '108,288': 690,\n",
       " '108.4**': 691,\n",
       " '108.40': 692,\n",
       " '108.43': 693,\n",
       " '108.50': 694,\n",
       " '108.9': 695,\n",
       " '109': 696,\n",
       " '109.36': 697,\n",
       " '109.4': 698,\n",
       " '109.45': 699,\n",
       " '10no': 700,\n",
       " '10th': 701,\n",
       " '10th-ranked': 702,\n",
       " '11': 703,\n",
       " '11,000': 704,\n",
       " '11,046,000': 705,\n",
       " '11,175,000': 706,\n",
       " '11,244': 707,\n",
       " '11,346,000': 708,\n",
       " '11,424.64': 709,\n",
       " '11,450': 710,\n",
       " '11,500': 711,\n",
       " '11,525': 712,\n",
       " '11,594.99': 713,\n",
       " '11,900-12,100': 714,\n",
       " '11-1': 715,\n",
       " '11-13': 716,\n",
       " '11-15': 717,\n",
       " '11-2': 718,\n",
       " '11-25': 719,\n",
       " '11-3': 720,\n",
       " '11-3/8': 721,\n",
       " '11-4': 722,\n",
       " '11-6': 723,\n",
       " '11-7': 724,\n",
       " '11-8': 725,\n",
       " '11-9': 726,\n",
       " '11-game': 727,\n",
       " '11-year-old': 728,\n",
       " '11.': 729,\n",
       " '11.0': 730,\n",
       " '11.00': 731,\n",
       " '11.04': 732,\n",
       " '11.09': 733,\n",
       " '11.12': 734,\n",
       " '11.16': 735,\n",
       " '11.18': 736,\n",
       " '11.20:33': 737,\n",
       " '11.25': 738,\n",
       " '11.27': 739,\n",
       " '11.28': 740,\n",
       " '11.3': 741,\n",
       " '11.31': 742,\n",
       " '11.34': 743,\n",
       " '11.38': 744,\n",
       " '11.4': 745,\n",
       " '11.45': 746,\n",
       " '11.53': 747,\n",
       " '11.55': 748,\n",
       " '11.60': 749,\n",
       " '11.61': 750,\n",
       " '11.7': 751,\n",
       " '11.776': 752,\n",
       " '11.8': 753,\n",
       " '11.833': 754,\n",
       " '11.959': 755,\n",
       " '110': 756,\n",
       " '110.4': 757,\n",
       " '110.50': 758,\n",
       " '1100': 759,\n",
       " '110th': 760,\n",
       " '111': 761,\n",
       " '111.2': 762,\n",
       " '111.50': 763,\n",
       " '112': 764,\n",
       " '112,200': 765,\n",
       " '112.0-149.0': 766,\n",
       " '112.1-113.4': 767,\n",
       " '112.56': 768,\n",
       " '112.8': 769,\n",
       " '112th-ranked': 770,\n",
       " '113': 771,\n",
       " '113.0': 772,\n",
       " '113.00': 773,\n",
       " '113.02': 774,\n",
       " '1130': 775,\n",
       " '11361.330': 776,\n",
       " '114': 777,\n",
       " '115': 778,\n",
       " '115,000': 779,\n",
       " '115,941': 780,\n",
       " '115.1': 781,\n",
       " '115.259': 782,\n",
       " '115.32': 783,\n",
       " '115.58': 784,\n",
       " '115.62': 785,\n",
       " '116': 786,\n",
       " '116th': 787,\n",
       " '117': 788,\n",
       " '117.00': 789,\n",
       " '117.9': 790,\n",
       " '118': 791,\n",
       " '118.8': 792,\n",
       " '119': 793,\n",
       " '119.3': 794,\n",
       " '119.3**': 795,\n",
       " '119.49': 796,\n",
       " '119.6': 797,\n",
       " '119.6**': 798,\n",
       " '11th': 799,\n",
       " '11th-ranked': 800,\n",
       " '11th-seeded': 801,\n",
       " '12': 802,\n",
       " '12,000': 803,\n",
       " '12,201.09': 804,\n",
       " '12,300': 805,\n",
       " '12,600-12,750': 806,\n",
       " '12,700-12,800': 807,\n",
       " '12,700-12,850': 808,\n",
       " '12,700-12,900': 809,\n",
       " '12,700-12,950': 810,\n",
       " '12,750-12,850': 811,\n",
       " '12,750-12,900': 812,\n",
       " '12,750-12,950': 813,\n",
       " '12,800-13,000': 814,\n",
       " '12,850-13,050': 815,\n",
       " '12,855.7': 816,\n",
       " '12,900-13,100': 817,\n",
       " '12,900-13,150': 818,\n",
       " '12,940': 819,\n",
       " '12-1': 820,\n",
       " '12-1-41-1': 821,\n",
       " '12-1-76-0': 822,\n",
       " '12-11': 823,\n",
       " '12-12': 824,\n",
       " '12-15': 825,\n",
       " '12-17': 826,\n",
       " '12-3': 827,\n",
       " '12-6': 828,\n",
       " '12-7': 829,\n",
       " '12-8': 830,\n",
       " '12-month': 831,\n",
       " '12-mth': 832,\n",
       " '12-week': 833,\n",
       " '12-year': 834,\n",
       " '12-year-old': 835,\n",
       " '12.': 836,\n",
       " '12.0': 837,\n",
       " '12.033': 838,\n",
       " '12.10': 839,\n",
       " '12.112': 840,\n",
       " '12.124': 841,\n",
       " '12.130': 842,\n",
       " '12.177).': 843,\n",
       " '12.19': 844,\n",
       " '12.2': 845,\n",
       " '12.208': 846,\n",
       " '12.211': 847,\n",
       " '12.225': 848,\n",
       " '12.246': 849,\n",
       " '12.3': 850,\n",
       " '12.341': 851,\n",
       " '12.348': 852,\n",
       " '12.4.1996': 853,\n",
       " '12.434': 854,\n",
       " '12.442': 855,\n",
       " '12.5': 856,\n",
       " '12.50': 857,\n",
       " '12.56': 858,\n",
       " '12.60': 859,\n",
       " '12.70': 860,\n",
       " '12.71': 861,\n",
       " '12.77': 862,\n",
       " '12.83': 863,\n",
       " '12.85': 864,\n",
       " '12.88': 865,\n",
       " '12.9': 866,\n",
       " '12.91': 867,\n",
       " '12.92': 868,\n",
       " '12.95': 869,\n",
       " '12.96': 870,\n",
       " '12/01': 871,\n",
       " '12/09': 872,\n",
       " '120': 873,\n",
       " '120,000': 874,\n",
       " '120,500': 875,\n",
       " '120-103': 876,\n",
       " '120.5': 877,\n",
       " '1200': 878,\n",
       " '1203': 879,\n",
       " '122': 880,\n",
       " '122.5': 881,\n",
       " '123,394': 882,\n",
       " '123-4': 883,\n",
       " '123.157': 884,\n",
       " '123.2': 885,\n",
       " '123.89': 886,\n",
       " '1230': 887,\n",
       " '123rd-ranked': 888,\n",
       " '124': 889,\n",
       " '1241': 890,\n",
       " '1249-1286': 891,\n",
       " '125': 892,\n",
       " '125-member': 893,\n",
       " '125.3': 894,\n",
       " '125cc': 895,\n",
       " '125m': 896,\n",
       " '126.0': 897,\n",
       " '1260': 898,\n",
       " '127': 899,\n",
       " '127.10': 900,\n",
       " '127.3': 901,\n",
       " '128': 902,\n",
       " '128-1': 903,\n",
       " '128-4': 904,\n",
       " '128-member': 905,\n",
       " '128.6': 906,\n",
       " '129': 907,\n",
       " '12:01': 908,\n",
       " '12pct': 909,\n",
       " '12th': 910,\n",
       " '12th-ranked': 911,\n",
       " '12th-seeded': 912,\n",
       " '13': 913,\n",
       " '13,000': 914,\n",
       " '13,091,000': 915,\n",
       " '13,102': 916,\n",
       " '13,256.5': 917,\n",
       " '13,500': 918,\n",
       " '13-0': 919,\n",
       " '13-10': 920,\n",
       " '13-11': 921,\n",
       " '13-15': 922,\n",
       " '13-8': 923,\n",
       " '13-9': 924,\n",
       " '13-month-old': 925,\n",
       " '13-times': 926,\n",
       " '13-year': 927,\n",
       " '13-year-old': 928,\n",
       " '13.': 929,\n",
       " '13.0': 930,\n",
       " '13.1': 931,\n",
       " '13.18': 932,\n",
       " '13.24': 933,\n",
       " '13.26': 934,\n",
       " '13.33': 935,\n",
       " '13.353': 936,\n",
       " '13.36': 937,\n",
       " '13.37': 938,\n",
       " '13.38': 939,\n",
       " '13.4': 940,\n",
       " '13.42': 941,\n",
       " '13.45': 942,\n",
       " '13.53': 943,\n",
       " '13.6': 944,\n",
       " '13.64': 945,\n",
       " '13.65': 946,\n",
       " '13.66': 947,\n",
       " '13.67': 948,\n",
       " '13.75': 949,\n",
       " '13.8': 950,\n",
       " '13.80': 951,\n",
       " '13/09': 952,\n",
       " '130': 953,\n",
       " '130.2': 954,\n",
       " '1300': 955,\n",
       " '131': 956,\n",
       " '131.7': 957,\n",
       " '1315': 958,\n",
       " '132': 959,\n",
       " '133': 960,\n",
       " '133.82': 961,\n",
       " '134': 962,\n",
       " '134.40': 963,\n",
       " '134.44': 964,\n",
       " '1345': 965,\n",
       " '1347': 966,\n",
       " '135': 967,\n",
       " '135-9': 968,\n",
       " '135.8': 969,\n",
       " '1360': 970,\n",
       " '137': 971,\n",
       " '138': 972,\n",
       " '138.0-149.0': 973,\n",
       " '138.643': 974,\n",
       " '1381': 975,\n",
       " '139.3': 976,\n",
       " '139.75': 977,\n",
       " '13:12': 978,\n",
       " '13th': 979,\n",
       " '14': 980,\n",
       " '14,000': 981,\n",
       " '14,153': 982,\n",
       " '14,196': 983,\n",
       " '14,278.9': 984,\n",
       " '14,390.7': 985,\n",
       " '14,494': 986,\n",
       " '14,600': 987,\n",
       " '14,668,000': 988,\n",
       " '14,775,000': 989,\n",
       " '14-0-71-0': 990,\n",
       " '14-15': 991,\n",
       " '14-17': 992,\n",
       " '14-28': 993,\n",
       " '14-6': 994,\n",
       " '14-7': 995,\n",
       " '14-9': 996,\n",
       " '14-day': 997,\n",
       " '14-man': 998,\n",
       " '14-month': 999,\n",
       " '14-pct': 1000,\n",
       " '14-year-old': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Embedding Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a matrix of dimensions (M, N), where M will the size of the vocabulary: The unique words in the training set and the words in GloVe, and N, the dimension of the embeddings. Initialize it with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348101\n",
      "332834\n",
      "332836\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(glove_set))\n",
    "\n",
    "#glove_set.extend(word_set)\n",
    "#vocab = sorted(list(set(glove_set + word_set)))\n",
    "\n",
    "#print(len(vocab))\n",
    "#voc = set(glove_set)\n",
    "\n",
    "rev_voc_idx = dict(enumerate(voc, start=2))\n",
    "voc_idx = {v: k for k, v in rev_voc_idx.items()}\n",
    "\n",
    "print(len(voc))\n",
    "\n",
    "M = len(voc) + 2 # max_words\n",
    "N = len(embeddings_index['france']) # embedding_dim\n",
    "#N = len(ner_set)\n",
    "\n",
    "embedding_matrix = np.random.rand(M,N)\n",
    "print(M)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fill the matrix with the GloVe embeddings. You will use the indices from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in voc_idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the X and Y Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert the X and Y list of symbols in a list of numbers using the indices you created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before: We have the symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "First sentence, NER ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, words', X_words[1])\n",
    "print('First sentence, NER', Y_ner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create sequences of numbers, let us convert them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_idx = [list(map(lambda x: word_idx.get(x, 1), x)) for x in X_words]\n",
    "Y_ner_idx = [list(map(lambda x: ner_idx.get(x, 1), x)) for x in Y_ner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words [8780, 16385, 9880, 5996, 19360, 5682, 5783, 12212, 125]\n",
      "First sentence, NER [4, 10, 3, 10, 10, 10, 3, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, words', X_words_idx[1])\n",
    "print('First sentence, NER', Y_ner_idx[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pad the sentences using the pad_sequences function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this because all the sentences needs to have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 150\n",
    "X_words_idx = pad_sequences(X_words_idx,maxlen=maxlen)\n",
    "Y_ner_idx = pad_sequences(Y_ner_idx,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, words [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0  8780 16385  9880\n",
      "  5996 19360  5682  5783 12212   125]\n",
      "First sentence, NER [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4 10  3\n",
      " 10 10 10  3 10 10]\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, words', X_words_idx[1])\n",
    "print('First sentence, NER', Y_ner_idx[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Do the same for the development set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words_idx_dev = [list(map(lambda x: word_idx_dev.get(x, 1), x)) for x in X_words_dev]\n",
    "Y_ner_idx_dev = [list(map(lambda x: ner_idx_dev.get(x, 1), x)) for x in Y_ner_dev]\n",
    "X_words_idx_dev = pad_sequences(X_words_idx_dev,maxlen=maxlen)\n",
    "Y_ner_idx_dev = pad_sequences(Y_ner_idx_dev,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "Y_ner_idx_cat = to_categorical(Y_ner_idx)\n",
    "Y_ner_idx_dev_cat = to_categorical(Y_ner_idx_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a simple recurrent network and train a model with the train set. As layers, you will use Embedding, SimpleRNN, and Dense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "ner_vocab_size=len(ner_idx.keys())+2\n",
    "print(ner_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_vocabulary_size\t 332836\n",
      "embedding_dim\t\t 100\n",
      "maxlen\t\t\t 150\n",
      "ner_vocab_size\t\t 11\n",
      "X\t\t\t (14987, 150)\n",
      "Y\t\t\t (14987, 150)\n",
      "X_val\t\t\t (3466, 150)\n",
      "Y_val\t\t\t (3466, 150)\n"
     ]
    }
   ],
   "source": [
    "text_vocabulary_size = len(voc) + 2\n",
    "print('text_vocabulary_size\\t',text_vocabulary_size)\n",
    "print('embedding_dim\\t\\t',N)\n",
    "print('maxlen\\t\\t\\t',maxlen)\n",
    "print('ner_vocab_size\\t\\t',ner_vocab_size)\n",
    "print('X\\t\\t\\t',X_words_idx.shape)\n",
    "print('Y\\t\\t\\t',Y_ner_idx.shape)\n",
    "print('X_val\\t\\t\\t',X_words_idx_dev.shape)\n",
    "print('Y_val\\t\\t\\t',Y_ner_idx_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 100)          33283600  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 64)           8512      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150, 11)           715       \n",
      "=================================================================\n",
      "Total params: 33,292,827\n",
      "Trainable params: 9,227\n",
      "Non-trainable params: 33,283,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, SimpleRNN,Bidirectional\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "# input här kommer vara emb_mat som vi lägger till som vikter i emb_lay och fryser så att de inte kan förändras\n",
    "model.add(Embedding(text_vocabulary_size, N,input_length=maxlen,mask_zero=False))\n",
    "model.layers[0].set_weights([embedding_matrix]) \n",
    "model.layers[0].trainable = False # Det är här vi fryser\n",
    "# output blir 150 x 100\n",
    "\n",
    "model.add(Bidirectional(SimpleRNN(32,return_sequences=True)))\n",
    "model.add(Dense(ner_vocab_size, activation='softmax')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compile and fit your network. You will report the training and validation losses and accuracies and comment on the possible overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/3\n",
      "14987/14987 [==============================] - 41s 3ms/step - loss: 0.0499 - acc: 0.9875 - val_loss: 0.2512 - val_acc: 0.9341\n",
      "Epoch 2/3\n",
      "14987/14987 [==============================] - 40s 3ms/step - loss: 0.0476 - acc: 0.9878 - val_loss: 0.2116 - val_acc: 0.9434\n",
      "Epoch 3/3\n",
      "14987/14987 [==============================] - 41s 3ms/step - loss: 0.0459 - acc: 0.9881 - val_loss: 0.1842 - val_acc: 0.9530\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_words_idx, Y_ner_idx_cat,\n",
    "          epochs=3, \n",
    "          batch_size=128,\n",
    "          validation_data=(X_words_idx_dev, Y_ner_idx_dev_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de3gV1b3G8e9PbuF+CVgUxIBaBUKAGBEEuSgqar2WKoitWj1UW3tsPe0jB60XrE+pcpTisVXbam1FKUercqyKWLFoPQIBEUSkoAJGrCIQLgJq4Hf+WLOTnfsOJNnJ8H6eZz/sPbP2zNqT4Z01a27m7oiISHwdku4KiIhI3VLQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnopUbMrImZ7TSzHrVZNp3M7Ggzq/XzjM1stJmtS/q82sxOTqXsfszrd2Y2eX+/X8V0f25mf6jt6Ur9apruCkjdMrOdSR9bAV8Ae6PP33P3mTWZnrvvBdrUdtmDgbsfWxvTMbOrgEvdfWTStK+qjWlLPCnoY87di4M2ajFe5e4vVVbezJq6e1F91E1E6oe6bg5y0a75n83scTPbAVxqZkPM7A0zKzSzj81shpk1i8o3NTM3s6zo86PR+OfNbIeZ/Z+Z9axp2Wj8mWb2TzPbZmb3mtk/zOzySuqdSh2/Z2ZrzWyrmc1I+m4TM7vHzDab2XvAmCqWz01mNqvMsPvM7O7o/VVmtir6Pe9Fre3KplVgZiOj963M7E9R3VYCx1cw3/ej6a40s3Oj4f2A/wZOjrrFPktatrcmff/q6LdvNrOnzeywVJZNdczs/Kg+hWb2spkdmzRuspltNLPtZvZu0m8dbGZLo+GfmNldqc5Paom763WQvIB1wOgyw34OfAmcQ9jwtwROAE4k7PH1Av4JXBuVbwo4kBV9fhT4DMgDmgF/Bh7dj7KHAjuA86Jx1wNfAZdX8ltSqeMzQHsgC9iS+O3AtcBKoDuQCSwI/xUqnE8vYCfQOmnanwJ50edzojIGnALsBnKicaOBdUnTKgBGRu+nAa8AHYEjgXfKlL0IOCz6m1wS1eFr0birgFfK1PNR4Nbo/elRHQcAGcCvgZdTWTYV/P6fA3+I3veO6nFK9DeaHC33ZkBfYD3QNSrbE+gVvV8MjI/etwVOTPf/hYPtpRa9ALzm7v/r7vvcfbe7L3b3he5e5O7vAw8CI6r4/hPunu/uXwEzCQFT07LfAJa5+zPRuHsIG4UKpVjHX7j7NndfRwjVxLwuAu5x9wJ33wxMrWI+7wNvEzZAAKcBhe6eH43/X3d/34OXgb8BFR5wLeMi4OfuvtXd1xNa6cnzne3uH0d/k8cIG+m8FKYLMAH4nbsvc/c9wCRghJl1TypT2bKpyjhgjru/HP2NpgLtCBvcIsJGpW/U/fdBtOwgbLCPMbNMd9/h7gtT/B1SSxT0AvBh8gczO87M/mpm/zKz7cAUoHMV3/9X0vtdVH0AtrKyhyfXw92d0AKuUIp1TGlehJZoVR4DxkfvLyFsoBL1+IaZLTSzLWZWSGhNV7WsEg6rqg5mdrmZvRV1kRQCx6U4XQi/r3h67r4d2Ap0SypTk79ZZdPdR/gbdXP31cB/EP4On0ZdgV2jolcAfYDVZrbIzM5K8XdILVHQC4Rd+WQPEFqxR7t7O+BmQtdEXfqY0JUCgJkZpYOprAOp48fAEUmfqzv988/A6KhFfB4h+DGzlsATwC8I3SodgBdTrMe/KquDmfUCfgNcA2RG0303abrVnQq6kdAdlJheW0IX0Ucp1Ksm0z2E8Df7CMDdH3X3oYRumyaE5YK7r3b3cYTuuf8CnjSzjAOsi9SAgl4q0hbYBnxuZr2B79XDPJ8Fcs3sHDNrClwHdKmjOs4GfmRm3cwsE7ihqsLu/gnwGvAwsNrd10SjWgDNgU3AXjP7BnBqDeow2cw6WLjO4NqkcW0IYb6JsM27itCiT/gE6J44+FyBx4ErzSzHzFoQAvdVd690D6kGdT7XzEZG8/4p4bjKQjPrbWajovntjl57CT/g22bWOdoD2Bb9tn0HWBepAQW9VOQ/gMsI/4kfILRo61QUphcDdwObgaOANwnn/dd2HX9D6EtfQThQ+EQK33mMcHD1saQ6FwI/Bp4iHNAcS9hgpeIWwp7FOuB54I9J010OzAAWRWWOA5L7tecBa4BPzCy5Cybx/RcIXShPRd/vQei3PyDuvpKwzH9D2AiNAc6N+utbAHcSjqv8i7AHcVP01bOAVRbO6poGXOzuXx5ofSR1FrpCRRoWM2tC6CoY6+6vprs+Io2ZWvTSYJjZGDNrH+3+/4xwJseiNFdLpNFT0EtDMgx4n7D7PwY4390r67oRkRSp60ZEJObUohcRibkGd1Ozzp07e1ZWVrqrISLSqCxZsuQzd6/wlOQGF/RZWVnk5+enuxoiIo2KmVV6hbe6bkREYk5BLyIScwp6EZGYa3B99CJSv7766isKCgrYs2dPuqsiKcjIyKB79+40a1bZrY7KU9CLHOQKCgpo27YtWVlZhJuGSkPl7mzevJmCggJ69uxZ/Rci6roROcjt2bOHzMxMhXwjYGZkZmbWeO8rpaCP7kGyOnrG5KQKxl9vZu+Y2XIz+5uZJd+zeq+ZLYtec2pUOxGpFwr5xmN//lbVBn10F8H7gDMJT4kZb2Z9yhR7k/AMzRzCLV/vTBq3290HRK9za1zDFO3bBz/9KTz5JGzaVFdzERFpfFJp0Q8C1kbPxfwSmEXJ8zMBcPf57r4r+vgGSU8Kqi8bNsCvfw1jx8Khh0K/fvDDH8Jf/gKfVfrkURFJt82bNzNgwAAGDBhA165d6datW/HnL79M7bb1V1xxBatXr66yzH333cfMmTOrLJOqYcOGsWzZslqZVn1I5WBsN0o/27KA8DDgylxJeJBCQoaZ5RNuOTvV3Z8u+wUzmwhMBOjRo7qnulUsKwu2boUlS2D+fHjlFXjoIfjv6JHL/frByJHhNXw4dE716ZsiUsrMmXDjjaFx1aMH3HEHTDiAx5pkZmYWh+att95KmzZt+MlPflKqjLvj7hxySMVt04cffrja+fzgBz/Y/0o2cqm06CvqEKrwlpdmdinhSfV3JQ3u4e55hIcqTzezo8pNzP1Bd89z97wuXap6elzVmjeHIUNg8mR48cUQ/P/4R1gRu3aF3/0OvvlN6NIF+veH666Dp56CzZv3e5YiB5WZM2HiRFi/HtzDvxMnhuG1be3atWRnZ3P11VeTm5vLxx9/zMSJE8nLy6Nv375MmTKluGyihV1UVESHDh2YNGkS/fv3Z8iQIXz66acA3HTTTUyfPr24/KRJkxg0aBDHHnssr7/+OgCff/453/zmN+nfvz/jx48nLy+v2pb7o48+Sr9+/cjOzmby5MkAFBUV8e1vf7t4+IwZMwC455576NOnD/379+fSSy+t9WVWmVRa9AWUfohxd8KTf0oxs9HAjcCI5HuIu/vG6N/3zewVYCDw3gHUOWXNm8NJJ4XX5Mnw5ZeweHFo7b/yCvz2txAtf3JySlr8I0ZAp071UUORxuXGG2HXrtLDdu0Kww+kVV+Zd955h4cffpj7778fgKlTp9KpUyeKiooYNWoUY8eOpU+f0ocMt23bxogRI5g6dSrXX389Dz30EJMmlTuHBHdn0aJFzJkzhylTpvDCCy9w77330rVrV5588kneeustcnNzq6xfQUEBN910E/n5+bRv357Ro0fz7LPP0qVLFz777DNWrFgBQGFhIQB33nkn69evp3nz5sXD6kMqLfrFwDFm1tPMmgPjgFJnz5jZQMJzO89190+ThneMnhaEmXUGhgLv1Fbla6p5cxg6NKyU8+ZBYSG89hr8/Oehlf/b38KFF4ZunQED4Ec/gqefhi1b0lVjkYZlw4aaDT9QRx11FCeccELx58cff5zc3Fxyc3NZtWoV77xTPk5atmzJmWeeCcDxxx/PunXrKpz2hRdeWK7Ma6+9xrhx4wDo378/ffv2rbJ+Cxcu5JRTTqFz5840a9aMSy65hAULFnD00UezevVqrrvuOubOnUv79u0B6Nu3L5deeikzZ86s0QVPB6raoHf3IsIT6ucCq4DZ7r7SzKaYWeIsmrsIT67/nzKnUfYG8s3sLWA+oY8+bUFfVnLwv/RS6Op59VWYMiWE/QMPwAUXlA7+Z55R8MvBq7JDaPt5aK1arVu3Ln6/Zs0afvWrX/Hyyy+zfPlyxowZU+H55M2bNy9+36RJE4qKiiqcdosWLcqVqemDmCorn5mZyfLlyxk2bBgzZszge9/7HgBz587l6quvZtGiReTl5bF3794azW9/pXQevbs/5+5fd/ej3P2OaNjN7j4nej/a3b9W9jRKd3/d3fu5e//o39/X3U85cC1awLBhcNNNIfgLC0Pw33YbZGaG4D///BD8AwfCj38cgn/r1nTXXKR+3HEHtGpVelirVmF4Xdu+fTtt27alXbt2fPzxx8ydO7fW5zFs2DBmz54NwIoVKyrcY0g2ePBg5s+fz+bNmykqKmLWrFmMGDGCTZs24e5861vf4rbbbmPp0qXs3buXgoICTjnlFO666y42bdrErrL9YHVEt0CoQiL4hw2Dn/0MvvgCFi0q6eO//36YPh3MQos/0cd/8snQsWN66y5SFxL98LV51k2qcnNz6dOnD9nZ2fTq1YuhQ4fW+jx++MMf8p3vfIecnBxyc3PJzs4u7napSPfu3ZkyZQojR47E3TnnnHM4++yzWbp0KVdeeSXujpnxy1/+kqKiIi655BJ27NjBvn37uOGGG2jbtm2t/4aKNLhnxubl5XljefDInj2lg//118PGIDn4R40Kwd+hQ5orK1KJVatW0bt373RXo0EoKiqiqKiIjIwM1qxZw+mnn86aNWto2rRhtYkr+puZ2ZLoDMdyGlbtG5mMjHBO/vDhcPPNpYN//vxwAdc994TgHziwdItfwS/S8OzcuZNTTz2VoqIi3J0HHnigwYX8/lCLvg7t2QMLF5a0+P/v/0KL/5BDygd/FXuHInVKLfrGRy36BiQjI5yTP2IE3HJLSfAnrty99174r/9S8ItI3VLQ16Pk4AfYvbt0iz85+HNzS4J/2DAFv4jsPwV9GrVsWRLmEIL/jTdKgn/GDJg2rXTwjxoVgr9du7RVW0QaGQV9A9KyZQjyUaPC5+Tgnz8ffvWrkuA//vjSLX4Fv4hURk+YasASwX/bbbBgQbiA629/C+cwZ2SEc/jPPjvcl+fEE+GGG+D552HHjnTXXCR1I0eOLHfx0/Tp0/n+979f5ffatGkDwMaNGxk7dmyl067u5I7p06eXunDprLPOqpX70Nx6661MmzbtgKdTGxT0jUirVnDKKeEWDcnB/5//GW7ncM89cNZZ4WItBb80FuPHj2fWrFmlhs2aNYvx48en9P3DDz+cJ554Yr/nXzbon3vuOTrE7PxnBX0jlgj+228Pt2ooLAy3bqgo+AcPhkmT4IUXFPzSsIwdO5Znn32WL74IN71dt24dGzduZNiwYcXntefm5tKvXz+eeeaZct9ft24d2dnZAOzevZtx48aRk5PDxRdfzO7du4vLXXPNNcW3OL7lllsAmDFjBhs3bmTUqFGMivpMs7Ky+Cx6WtHdd99NdnY22dnZxbc4XrduHb179+bf/u3f6Nu3L6effnqp+VRk2bJlDB48mJycHC644AK2RvdNmTFjBn369CEnJ6f4Zmp///vfix+8MnDgQHbUwn9Y9dHHSKtWcOqp4QXw+efh3P3Ewd2774Zf/hKaNIG8vNAtNHJkuLFbtBcsB7kf/Qhq+8FJAwaEbsbKZGZmMmjQIF544QXOO+88Zs2axcUXX4yZkZGRwVNPPUW7du347LPPGDx4MOeee26lz039zW9+Q6tWrVi+fDnLly8vdZvhO+64g06dOrF3715OPfVUli9fzr//+79z9913M3/+fDqXeRrRkiVLePjhh1m4cCHuzoknnsiIESPo2LEja9as4fHHH+e3v/0tF110EU8++WSV95f/zne+w7333suIESO4+eabue2225g+fTpTp07lgw8+oEWLFsXdRdOmTeO+++5j6NCh7Ny5k4yMjBos7YqpRR9jrVvD6NHhNsyvvRZuvjZvXmjZN2kSDuyOGROu0h0yJOwJzJ0LO3emu+ZysEnuvknutnF3Jk+eTE5ODqNHj+ajjz7ik08+qXQ6CxYsKA7cnJwccnJyisfNnj2b3NxcBg4cyMqVK6u9Ydlrr73GBRdcQOvWrWnTpg0XXnghr776KgA9e/ZkwIABQNW3QoZwf/zCwkJGROdVX3bZZSxYsKC4jhMmTODRRx8tvgJ36NChXH/99cyYMYPCwsJauTJXLfqDSCL4R48Onz//PNyfJ9HinzYNpk6Fpk3hhBNKzuo56SS1+A8WVbW869L555/P9ddfz9KlS9m9e3dxS3zmzJls2rSJJUuW0KxZM7Kysiq8NXGyilr7H3zwAdOmTWPx4sV07NiRyy+/vNrpVHXXgMQtjiHc5ri6rpvK/PWvf2XBggXMmTOH22+/nZUrVzJp0iTOPvtsnnvuOQYPHsxLL73Ecccdt1/TT1CL/iDWujWcdlq4++A//hH6+F98EX760zD+rrvgjDNCH3/iKV3z5oUNhEhtatOmDSNHjuS73/1uqYOw27Zt49BDD6VZs2bMnz+f9evXVzmd4cOHFz8A/O2332b58uVAuMVx69atad++PZ988gnPP1/yWOu2bdtW2A8+fPhwnn76aXbt2sXnn3/OU089xcknn1zj39a+fXs6duxYvDfwpz/9iREjRrBv3z4+/PBDRo0axZ133klhYSE7d+7kvffeo1+/ftxwww3k5eXx7rvv1nieZalFL8USwX/aaeHzzp2lW/x33QW/+EVo8Q8aVLrFn/R8CJH9Mn78eC688MJSZ+BMmDCBc845h7y8PAYMGFBty/aaa67hiiuuICcnhwEDBjBo0CAgPC1q4MCB9O3bt9wtjidOnMiZZ57JYYcdxvz584uH5+bmcvnllxdP46qrrmLgwIFVdtNU5pFHHuHqq69m165d9OrVi4cffpi9e/dy6aWXsm3bNtydH//4x3To0IGf/exnzJ8/nyZNmtCnT5/ip2UdCN3UTFK2c2do+SeCf/Fi2LsXmjUrH/xlH04hDZduatb46KZmUmfatAldOWecET7v2FHS4p8/P/Tv33GHgl+koVHQy35r27Z88Ce3+JOD/8QTS4J/yBAFv0h9UtBLrWnbNpyuOWZM+JwI/sRtmX/xi3CqZ3LwjxoVgr9ly3TWXBKPvJOGb3+629VHL/Vm+/bSLf78fNi3L1zFW7bFr+CvPx988AFt27YlMzNTYd/AuTubN29mx44d9OzZs9S4qvroFfSSNtu3hwu5EsG/ZElJ8A8eXBL8gwcr+OvSV199RUFBQbXnlUvDkJGRQffu3WnWrFmp4Qp6aRQSwZ/o6lm6tOLgHzIk3L1TREoo6KVR2ratdIs/EfwtWpRv8Sv45WCnoJdYSAR/osX/5pulgz9xk7YTT1Twy8FHQS+xVFhYvsXvHoJ/yJCSFr+CXw4GCno5KBQWhvvyJ4L/zTdD8GdklA/+pHtSicSCgl4OSongT3T1LFtWPvhHjQpX8Sr4pbFT0IsQ7sef3OJPDv6TTipp8Sv4pTFS0ItUIBH8iRb/W2+VDv7Ewd0TTlDwS8OnoBdJwZYtpVv8ieBv2bKkxT9oEPTrB127gi4ilYZEQS+yH7ZsgQULSgd/QqdOkJ0dQj87u+TVoUO6aisHOwW9SC3YsgWWL4cVK+Dtt0te27eXlOnevfwGoHdv3cJB6p7uRy9SCzp1Kjlgm+AOH34YAj+xAVixAl5+Gb78MpQ55BA4+ujS4d+vHxx1VHhal0hd02omcgDMoEeP8DrrrJLhRUWwdm3p1v/y5fCXv4SNA4QDvL17l98AdO+u/n+pXeq6EalHu3fDqlWlNwArVsBHH5WUadeufPdPv36QmZm+ekvDpz56kQZu61ZYubL8BmDr1pIyXbuW3wD07asHs0ugPnqRBq5jRxg2LLwS3OHjj0v3/7/9Ntx/f9gzSOjVq/wG4Nhjw5O8REBBL9JgmcHhh4fX6aeXDN+7Fz74oPwG4K9/DeMghPyxx5bfAGRlhYPDcnBJqevGzMYAvwKaAL9z96llxl8PXAUUAZuA77r7+mjcZcBNUdGfu/sjVc1LXTci++eLL2D16vIbgHXrSsq0bh26e8puAL72NR0AbuwOqI/ezJoA/wROAwqAxcB4d38nqcwoYKG77zKza4CR7n6xmXUC8oE8wIElwPHuvrXsfBIU9CK1a/t2eOed8qeAbtpUUqZz59IHfhP9/+3bp6/eUjMH2kc/CFjr7u9HE5sFnAcUB727z08q/wZwafT+DGCeu2+JvjsPGAM8XtMfISL7p1278GCWwYNLD//00/Kt/z/8AXbuLCnTo0f5DcBxx+n+/o1NKkHfDfgw6XMBcGIV5a8Enq/iu91qUkERqRuHHgqnnBJeCfv2wYYN5TcA8+bBV1+FMk2awDHHlN8AHHVUGCcNTypBX1HPXYX9PWZ2KaGbZkRNvmtmE4GJAD169EihSiJSFw45JBywzcqCb3yjZPhXX8GaNaU3AMuWwZNPllwAlpEBffqU3wB066b+/3RLJegLgCOSPncHNpYtZGajgRuBEe7+RdJ3R5b57itlv+vuDwIPQuijT6FOIlKPmjULId6nD1x0Ucnwzz8PF4AlbwBeegn++MeSMh06lA//7OxwSwmpH6kcjG1KOBh7KvAR4WDsJe6+MqnMQOAJYIy7r0ka3olwADY3GrSUcDB2S2Xz08FYkcZv8+ZwAVjZA8DbtpWUOfzw0nf+7NcvbEhatUpfvRuzAzoY6+5FZnYtMJdweuVD7r7SzKYA+e4+B7gLaAP8j4V9tA3ufq67bzGz2wkbB4ApVYW8iMRDZiYMHx5eCe7hVg/JV/6+/Tb8+tewZ08oYxYuACt7+4djjtEFYAdCt0AQkbTauxfee6/8BuCf/wwHhyGE/HHHld8A9OihC8ASdK8bEWl09uyBd98tfwbQhg0lZdq0Cef7JzYAiX8PPTR99U4XBb2IxMa2bRX3/2/eXFKmS5fyrf++faFt2/TVu67ppmYiEhvt24dn+J50Uskwd/jkk/LdP7//fTgzKOHII8tvAI49Nv4Pf1fQi0ijZxZu49y1K4weXTJ8375wr5+yG4AXXggPh4FwkdfXv15+A9CzZ3wuAFPQi0hsHXJIOIunVy8499yS4V9+GQ72Jm8A8vNh9uySMi1bhtM9y24ADjus8V0ApqAXkYNO8+Yl4Z1s586SG8AlNgAvvBDuAZTQsWP5i7+ys8PwhkpBLyISadMGBg0Kr2SffVYS/okNwKOPhjuDJnTrVn4D0KdP2DNINwW9iEg1OneGkSPDK8EdCgrKP/7xlVfCswEgdPEcfXT5DcAxx0DTekxfBb2IyH4wgyOOCK+zzioZXlQEa9eWPwD8zDMlF4A1bw69e5ffAPToUTf9/zqPXkSkHuzeXXIDuOQNQEFBSZkTToBFi/Zv+jqPXkQkzVq2hNzc8EpWWFgS/nV1Px8FvYhIGnXoAMOGhVdd0e2ARERiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMwp6EVEYk5BLyIScwp6EZGYU9CLiMScgl5EJOYU9CIiMaegFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxFxKQW9mY8xstZmtNbNJFYwfbmZLzazIzMaWGbfXzJZFrzm1VXEREUlN0+oKmFkT4D7gNKAAWGxmc9z9naRiG4DLgZ9UMInd7j6gFuoqIiL7odqgBwYBa939fQAzmwWcBxQHvbuvi8btq4M6iojIAUil66Yb8GHS54JoWKoyzCzfzN4ws/MrKmBmE6My+Zs2barBpEVEpDqpBL1VMMxrMI8e7p4HXAJMN7Ojyk3M/UF3z3P3vC5dutRg0iIiUp1Ugr4AOCLpc3dgY6ozcPeN0b/vA68AA2tQPxEROUCpBP1i4Bgz62lmzYFxQEpnz5hZRzNrEb3vDAwlqW9fRETqXrVB7+5FwLXAXGAVMNvdV5rZFDM7F8DMTjCzAuBbwANmtjL6em8g38zeAuYDU8ucrSMiInXM3GvS3V738vLyPD8/P93VEBFpVMxsSXQ8tBxdGSsiEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMwp6EVEYk5BLyIScwp6EZGYU9CLiMScgl5EJOYU9CIiMaegFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMylFPRmNsbMVpvZWjObVMH44Wa21MyKzGxsmXGXmdma6HVZbVVcRERSU23Qm1kT4D7gTKAPMN7M+pQptgG4HHiszHc7AbcAJwKDgFvMrOOBV1tERFKVSot+ELDW3d939y+BWcB5yQXcfZ27Lwf2lfnuGcA8d9/i7luBecCYWqi3iIikKJWg7wZ8mPS5IBqWipS+a2YTzSzfzPI3bdqU4qRFRCQVqQS9VTDMU5x+St919wfdPc/d87p06ZLipEVEJBWpBH0BcETS5+7AxhSnfyDfFRGRWpBK0C8GjjGznmbWHBgHzElx+nOB082sY3QQ9vRomIiI1JNqg97di4BrCQG9Cpjt7ivNbIqZnQtgZieYWQHwLeABM1sZfXcLcDthY7EYmBINExGRemLuqXa314+8vDzPz89PdzVERBoVM1vi7nkVjdOVsSIiMaegFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMwp6EVEYk5BLyIScy9l0EYAAAf4SURBVAp6EZGYU9CLiMScgl5EJOYU9CIiMaegFxGJOQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURiTkEvIhJzCnoRkZhT0IuIxFxKQW9mY8xstZmtNbNJFYxvYWZ/jsYvNLOsaHiWme02s2XR6/7arX6JmTMhKwsOOST8O3NmXc1JRKRxaVpdATNrAtwHnAYUAIvNbI67v5NU7Epgq7sfbWbjgF8CF0fj3nP3AbVc71JmzoSJE2HXrvB5/frwGWDChLqcs4hIw5dKi34QsNbd33f3L4FZwHllypwHPBK9fwI41cys9qpZtRtvLAn5hF27wnCR2qA9RmnMUgn6bsCHSZ8LomEVlnH3ImAbkBmN62lmb5rZ383s5IpmYGYTzSzfzPI3bdpUox8AsGFDzYaL1ERij3H9enAv2WNU2EtjkUrQV9Qy9xTLfAz0cPeBwPXAY2bWrlxB9wfdPc/d87p06ZJClUrr0aNmw0VqQnuMUtfqeo8xlaAvAI5I+twd2FhZGTNrCrQHtrj7F+6+GcDdlwDvAV8/0EqXdccd0KpV6WGtWoXhIgdKe4xSl+pjjzGVoF8MHGNmPc2sOTAOmFOmzBzgsuj9WOBld3cz6xIdzMXMegHHAO/XTtVLTJgADz4IRx4JZuHfBx/UgVipHdpjlLpUH3uM1QZ91Od+LTAXWAXMdveVZjbFzM6Niv0eyDSztYQumsQpmMOB5Wb2FuEg7dXuvqX2ql9iwgRYtw727Qv/KuSltmiPUepSfewxmnvZ7vb0ysvL8/z8/HRXQ6SUmTNDC2vDhtCSv+MONSakdmRlhe6aso48MjRaU2VmS9w9r6JxujJWJAXaY5S6Uh97jAp6EZE0qo9jjNVeGSsiInVrwoS63UtUi15EJOYU9CIiMaegFxGJOQW9iEjMKehFRGKuwV0wZWabgAouH0hZZ+CzWqpObVK9akb1qhnVq2biWK8j3b3Cu0I2uKA/UGaWX9nVYemketWM6lUzqlfNHGz1UteNiEjMKehFRGIujkH/YLorUAnVq2ZUr5pRvWrmoKpX7ProRUSktDi26EVEJImCXkQk5hpN0JvZQ2b2qZm9Xcl4M7MZZrbWzJabWW7SuMvMbE30uqyi79dhvSZE9VluZq+bWf+kcevMbIWZLTOzWn3aSgr1Gmlm26J5LzOzm5PGjTGz1dGynFTR9+uwXj9NqtPbZrbXzDpF4+pyeR1hZvPNbJWZrTSz6yooU6/rWIp1Stf6lUrd6n0dS7Fe9b6OmVmGmS0ys7eiet1WQZkWZvbnaJksNLOspHH/GQ1fbWZn1LgC7t4oXoTHEuYCb1cy/izgecCAwcDCaHgnwnNqOwEdo/cd67FeJyXmB5yZqFf0eR3QOU3LayTwbAXDmxAe4t4LaA68BfSpr3qVKXsO4fnD9bG8DgNyo/dtgX+W/d31vY6lWKd0rV+p1K3e17FU6pWOdSxaZ9pE75sBC4HBZcp8H7g/ej8O+HP0vk+0jFoAPaNl16Qm8280LXp3XwBU9bzZ84A/evAG0MHMDgPOAOa5+xZ33wrMA8bUV73c/fVovgBvAN1ra94HUq8qDALWuvv77v4lMIuwbNNRr/HA47U176q4+8fuvjR6v4PwfORuZYrV6zqWSp3SuH6lsrwqU2fr2H7Uq17WsWid2Rl9bBa9yp4Jcx7wSPT+CeBUM7No+Cx3/8LdPwDWEpZhyhpN0KegG/Bh0ueCaFhlw9PhSkKLMMGBF81siZlNTEN9hkS7ks+bWd9oWINYXmbWihCWTyYNrpflFe0yDyS0upKlbR2rok7J0rJ+VVO3tK1j1S2z+l7HzKyJmS0DPiU0DCpdv9y9CNgGZFILyytOT5iyCoZ5FcPrlZmNIvxHHJY0eKi7bzSzQ4F5ZvZu1OKtD0sJ98bYaWZnAU8Dx9BAlhdhl/of7p7c+q/z5WVmbQj/8X/k7tvLjq7gK3W+jlVTp0SZtKxf1dQtbetYKsuMel7H3H0vMMDMOgBPmVm2uycfq6qz9StOLfoC4Iikz92BjVUMrzdmlgP8DjjP3Tcnhrv7xujfT4GnqOHu2IFw9+2JXUl3fw5oZmadaQDLKzKOMrvUdb28zKwZIRxmuvtfKihS7+tYCnVK2/pVXd3StY6lsswi9b6ORdMuBF6hfPde8XIxs6ZAe0I354Evr9o+6FCXLyCLyg8unk3pA2WLouGdgA8IB8k6Ru871WO9ehD61E4qM7w10Dbp/evAmHqsV1dKLpgbBGyIll1TwsHEnpQcKOtbX/WKxidW8Nb1tbyi3/5HYHoVZep1HUuxTmlZv1KsW72vY6nUKx3rGNAF6BC9bwm8CnyjTJkfUPpg7OzofV9KH4x9nxoejG00XTdm9jjhKH5nMysAbiEc0MDd7weeI5wVsRbYBVwRjdtiZrcDi6NJTfHSu2p1Xa+bCf1svw7HVSjycHe6rxF23yCs+I+5+wv1WK+xwDVmVgTsBsZ5WKuKzOxaYC7h7IiH3H1lPdYL4ALgRXf/POmrdbq8gKHAt4EVUT8qwGRCkKZrHUulTmlZv1KsWzrWsVTqBfW/jh0GPGJmTQg9KbPd/VkzmwLku/sc4PfAn8xsLWEjNC6q80ozmw28AxQBP/DQDZQy3QJBRCTm4tRHLyIiFVDQi4jEnIJeRCTmFPQiIjGnoBcRiTkFvYhIzCnoRURi7v8Bj0fE998ddQQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc'] \n",
    "val_acc = history.history['val_acc'] \n",
    "loss = history.history['loss'] \n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
    "plt.title('Training and validation accuracy') \n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
    "plt.title('Training and validation loss') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply your network to the test set and report the accuracy you obtained. You will use the evaluate method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your System\n",
    "You will use the official script to evaluate the performance of your system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the predict method to predict the tags of the whole test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write your results in a file, where the two last columns will be the hand-annotated tag and the predicted tag. The fields must be separated by a space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply conlleval to your output. Report the F1 result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Try to improve your model by modifying some parameters, adding layers, adding Bidirectional and Dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate your network again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = conll_dict.transform(test_sentences)\n",
    "X_words, Y_ner = build_sequences(test_dict, key_x='form', key_y='ner')\n",
    "X_words_test, Y_ner_test = build_sequences(test_dict, key_x='form', key_y='ner')\n",
    "\n",
    "# Extract the list of unique words and NER and vocab including glove \n",
    "word_set_test = sorted(list(set([item for sublist in X_words_test for item in sublist])))\n",
    "ner_set_test = sorted(list(set([item for sublist in Y_ner_test for item in sublist])))\n",
    "\n",
    "# Building the indices \n",
    "rev_word_idx_test = dict(enumerate(voc, start=2))\n",
    "rev_ner_idx_test = dict(enumerate(ner_set_test, start=2))\n",
    "rev_word_idx_test[0]=0\n",
    "rev_word_idx_test[1]='-unknown-'\n",
    "word_idx_test = {v: k for k, v in rev_word_idx_test.items()}\n",
    "ner_idx_test = {v: k for k, v in rev_ner_idx_test.items()}\n",
    "\n",
    "# Converting sequences to indicies\n",
    "X_words_idx_test = [list(map(lambda x: word_idx_test.get(x, 1), x)) for x in X_words_test]\n",
    "X_words_idx_test = pad_sequences(X_words_idx_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_words_idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_output(predicted, ner_idx,X_words_test,Y_ner_test,filename):\n",
    "    Y_out_pad= np.argmax(predicted,axis=2)\n",
    "    inv_ner_idx = {v: k for k, v in ner_idx.items()}\n",
    "    Y_out = []\n",
    "    inv_ner_idx[0]='O'\n",
    "    inv_ner_idx[1]='wtf'\n",
    "    for i in range(len(Y_out_pad)):\n",
    "        temp_old = Y_out_pad[i][-(len(X_words_test[i])):]\n",
    "        temp_new = []\n",
    "        for j in temp_old:\n",
    "            temp_new.append(inv_ner_idx[j])\n",
    "        Y_out.append(temp_new)\n",
    "\n",
    "    f_out = open(filename, 'w')\n",
    "    for i in range(len(X_words_test)): # For each sentence\n",
    "        for j in range(len(X_words_test[i])): # Fore each word\n",
    "            word = X_words_test[i][j]\n",
    "            NER = Y_ner_test[i][j]\n",
    "            PNER = Y_out[i][j]\n",
    "            f_out.write(word + ' ' + NER + ' ' + PNER + '\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()\n",
    "    return Y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5648 phrases; found: 723 phrases; correct: 13.\n",
      "accuracy:  77.61%; precision:   1.80%; recall:   0.23%; FB1:   0.41\n",
      "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  43\n",
      "              LOC: precision:   6.12%; recall:   0.18%; FB1:   0.35  49\n",
      "             MISC: precision:   5.88%; recall:   0.28%; FB1:   0.54  34\n",
      "              ORG: precision:   4.31%; recall:   0.30%; FB1:   0.56  116\n",
      "              PER: precision:   0.62%; recall:   0.19%; FB1:   0.29  481\n"
     ]
    }
   ],
   "source": [
    "Y_new = creat_output(predicted, ner_idx,X_words_test,Y_ner_test,'new_out')\n",
    "!perl ./conlleval.pl <new_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a LSTM Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a simple LSTM network and train a model with the train set. As layers, you will use Embedding, LSTM, and Dense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Apply conlleval to your output. Report the F1 result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try to improve your model by modifying some parameters, adding layers, adding Bidirectional, Dropout, possibly mixing SimpleRNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply your network to the test set and report the accuracy you obtained. you need to reach a F1 of 82 to pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 150, 100)          33283600  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 150, 200)          160800    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 150, 200)          60200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150, 200)          40200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 200)          0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150, 11)           2211      \n",
      "=================================================================\n",
      "Total params: 33,547,011\n",
      "Trainable params: 263,411\n",
      "Non-trainable params: 33,283,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dropout\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "# input här kommer vara emb_mat som vi lägger till som vikter i emb_lay och fryser så att de inte kan förändras\n",
    "model.add(Embedding(text_vocabulary_size,N,input_length=maxlen,mask_zero=False))\n",
    "# output blir 150 x 100\n",
    "\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(Bidirectional(SimpleRNN(100,return_sequences=True)))\n",
    "model.add(Dense(200, activation='relu')) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(ner_vocab_size, activation='softmax')) \n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix]) \n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/10\n",
      "14987/14987 [==============================] - 379s 25ms/step - loss: 0.1182 - acc: 0.9730 - val_loss: 0.3262 - val_acc: 0.9179\n",
      "Epoch 2/10\n",
      "14987/14987 [==============================] - 310s 21ms/step - loss: 0.0570 - acc: 0.9868 - val_loss: 0.3145 - val_acc: 0.9227\n",
      "Epoch 3/10\n",
      "14987/14987 [==============================] - 328s 22ms/step - loss: 0.0471 - acc: 0.9878 - val_loss: 0.2504 - val_acc: 0.9264\n",
      "Epoch 4/10\n",
      "14987/14987 [==============================] - 433s 29ms/step - loss: 0.0407 - acc: 0.9888 - val_loss: 0.2397 - val_acc: 0.9269\n",
      "Epoch 5/10\n",
      "14987/14987 [==============================] - 371s 25ms/step - loss: 0.0356 - acc: 0.9899 - val_loss: 0.2926 - val_acc: 0.9195\n",
      "Epoch 6/10\n",
      "14987/14987 [==============================] - 353s 24ms/step - loss: 0.0319 - acc: 0.9908 - val_loss: 0.2081 - val_acc: 0.9388\n",
      "Epoch 7/10\n",
      "14987/14987 [==============================] - 316s 21ms/step - loss: 0.0287 - acc: 0.9915 - val_loss: 0.1974 - val_acc: 0.9443\n",
      "Epoch 8/10\n",
      "14987/14987 [==============================] - 317s 21ms/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.2365 - val_acc: 0.9378\n",
      "Epoch 9/10\n",
      "14987/14987 [==============================] - 347s 23ms/step - loss: 0.0239 - acc: 0.9928 - val_loss: 0.2481 - val_acc: 0.9388\n",
      "Epoch 10/10\n",
      "14987/14987 [==============================] - 317s 21ms/step - loss: 0.0218 - acc: 0.9934 - val_loss: 0.2232 - val_acc: 0.9498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x698134438>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc']) \n",
    "model.fit(X_words_idx, Y_ner_idx_cat,\n",
    "          epochs=3, \n",
    "          batch_size=128,\n",
    "          validation_data=(X_words_idx_dev, Y_ner_idx_dev_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_words_idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_new = creat_output(predicted, ner_idx,X_words_test,Y_ner_test,'BILSTM100_BISRNN100_200_DO_8ep_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46666 tokens with 5648 phrases; found: 13558 phrases; correct: 87.\n",
      "accuracy:  49.69%; precision:   0.64%; recall:   1.54%; FB1:   0.91\n",
      "              LOC: precision:   5.44%; recall:   2.04%; FB1:   2.97  625\n",
      "             MISC: precision:   0.42%; recall:   2.42%; FB1:   0.71  4058\n",
      "              ORG: precision:   0.51%; recall:   0.90%; FB1:   0.65  2930\n",
      "              PER: precision:   0.35%; recall:   1.30%; FB1:   0.56  5945\n"
     ]
    }
   ],
   "source": [
    "!perl ./conlleval.pl <BILSTM100_BISRNN100_200_DO_8ep_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
