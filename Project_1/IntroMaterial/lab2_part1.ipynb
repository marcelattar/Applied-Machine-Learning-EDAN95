{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the main file for the first part of lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Importing the necesarry modules and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, tree, metrics\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inspect the data. What is in there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split your data set into 70% training data (features and labels), and 30% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(digits.data, digits.target, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  1257\n",
      "Number of test examples:  540\n",
      "Number of total examples: 1797\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: \",len(train_features))\n",
    "print(\"Number of test examples: \",len(test_features))\n",
    "print(\"Number of total examples:\", len(train_features)+len(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set up a DecisionTreeClassifier as it comes in SciKitLearn. Use it with default parameters to train a decision tree classifier for the digits dataset based on the training data. Follow the tutorial (or the respective documentation) and produce a plot of the tree with graphviz. What can you learn from this about how the used algorithm handles the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf1.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tree.export_graphviz(clf1, out_file=\"tree.dot\")\n",
    "\n",
    "import graphviz\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph).view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test the classifier with the remaining test data and analyse it using the metrics packages of SciKitLearn (classification report, confusion matrix). What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=0, splitter='best'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93        53\n",
      "           1       0.80      0.70      0.74        50\n",
      "           2       0.80      0.74      0.77        47\n",
      "           3       0.83      0.89      0.86        54\n",
      "           4       0.82      0.85      0.84        60\n",
      "           5       0.90      0.91      0.90        66\n",
      "           6       0.89      0.94      0.92        53\n",
      "           7       0.92      0.84      0.88        55\n",
      "           8       0.77      0.86      0.81        43\n",
      "           9       0.82      0.85      0.83        59\n",
      "\n",
      "    accuracy                           0.85       540\n",
      "   macro avg       0.85      0.85      0.85       540\n",
      "weighted avg       0.85      0.85      0.85       540\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[48  0  1  0  1  1  0  0  1  1]\n",
      " [ 0 35  5  0  3  0  2  1  3  1]\n",
      " [ 1  2 35  3  0  0  1  1  3  1]\n",
      " [ 0  1  0 48  1  0  0  0  1  3]\n",
      " [ 0  3  0  0 51  1  3  1  0  1]\n",
      " [ 0  0  1  1  1 60  0  0  1  2]\n",
      " [ 0  0  0  0  1  2 50  0  0  0]\n",
      " [ 0  1  1  3  2  0  0 46  1  1]\n",
      " [ 1  2  0  1  1  0  0  0 37  1]\n",
      " [ 0  0  1  2  1  3  0  1  1 50]]\n"
     ]
    }
   ],
   "source": [
    "predicted1 = clf1.predict(test_features) \n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (clf1, metrics.classification_report(test_labels, predicted1)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(test_labels, predicted1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Change the parameters of the classifier, e.g., the minimum number of samples in a leaf / for a split, to see how the tree and the results are affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=3, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=0, splitter='best'):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        53\n",
      "           1       0.84      0.84      0.84        50\n",
      "           2       0.84      0.77      0.80        47\n",
      "           3       0.78      0.87      0.82        54\n",
      "           4       0.81      0.85      0.83        60\n",
      "           5       0.93      0.85      0.89        66\n",
      "           6       0.89      0.94      0.92        53\n",
      "           7       0.82      0.84      0.83        55\n",
      "           8       0.81      0.88      0.84        43\n",
      "           9       0.86      0.81      0.83        59\n",
      "\n",
      "    accuracy                           0.86       540\n",
      "   macro avg       0.86      0.86      0.86       540\n",
      "weighted avg       0.86      0.86      0.86       540\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[49  0  0  0  1  1  1  0  1  0]\n",
      " [ 0 42  4  0  2  0  0  1  1  0]\n",
      " [ 0  2 36  3  0  0  1  1  3  1]\n",
      " [ 0  1  1 47  1  0  0  0  1  3]\n",
      " [ 0  3  0  0 51  0  2  2  0  2]\n",
      " [ 0  0  2  1  3 56  2  2  0  0]\n",
      " [ 0  0  0  0  2  0 50  1  0  0]\n",
      " [ 0  0  0  4  2  0  0 46  1  2]\n",
      " [ 0  2  0  1  0  1  0  1 38  0]\n",
      " [ 0  0  0  4  1  2  0  2  2 48]]\n"
     ]
    }
   ],
   "source": [
    "# Here I create the model object, DecisionTreeClassifier\n",
    "clf2 = DecisionTreeClassifier(random_state=0, min_samples_split = 2, min_samples_leaf = 3) \n",
    "# min_sample_leaf : default 1, min_sample_split : default 2\n",
    "\n",
    "# Here I train the model with the training data\n",
    "clf2.fit(train_features, train_labels)\n",
    "\n",
    "# This is to visualize the tree\n",
    "tree.export_graphviz(clf2, out_file=\"tree.dot\")\n",
    "import graphviz\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph).view()\n",
    "\n",
    "\n",
    "# Calculate the prediction using the test data\n",
    "predicted2 = clf2.predict(test_features) \n",
    "\n",
    "# Printing the Classification report and the Confusion matrix\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (clf2, metrics.classification_report(test_labels, predicted2)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(test_labels, predicted2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ID3\n",
    "import ToyData as td\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODE CREATED\n",
      "['Attributes: ', dict_keys(['color', 'size', 'shape'])]\n",
      "[0 1 0]\n",
      "['A: ', 'size']\n",
      "['sub_attributes: ', {'color': ['y', 'g', 'b'], 'shape': ['r', 'i']}]\n",
      "NODE CREATED\n",
      "['Attributes: ', dict_keys(['color', 'shape'])]\n",
      "[0 0 1]\n",
      "['A: ', 'shape']\n",
      "['sub_attributes: ', {'color': ['y', 'g', 'b']}]\n",
      "NODE CREATED\n",
      "['Attributes: ', dict_keys(['color'])]\n",
      "[1 0 0]\n",
      "['A: ', 'color']\n",
      "['sub_attributes: ', {}]\n",
      "NODE CREATED\n",
      "NODE CREATED\n",
      "----new iteration------\n",
      "NODE CREATED\n",
      "----new iteration------\n",
      "NODE CREATED\n",
      "['Attributes: ', dict_keys(['color', 'shape'])]\n",
      "[0 0 1]\n",
      "['A: ', 'shape']\n",
      "['sub_attributes: ', {'color': ['y', 'g', 'b']}]\n",
      "NODE CREATED\n",
      "['Attributes: ', dict_keys(['color'])]\n",
      "[1 0 0]\n",
      "['A: ', 'color']\n",
      "['sub_attributes: ', {}]\n",
      "NODE CREATED\n",
      "----new iteration------\n",
      "NODE CREATED\n",
      "['Attributes: ', dict_keys(['color'])]\n",
      "[1 0 0]\n",
      "['A: ', 'color']\n",
      "['sub_attributes: ', {}]\n",
      "NODE CREATED\n",
      "NODE CREATED\n",
      "----new iteration------\n",
      "----new iteration------\n",
      "----new iteration------\n",
      "{'id': 0, 'label': None, 'attribute': 'size', 'entropy': 0.9886994082884974, 'samples': 16, 'classCounts': [['+', 9], ['-', 7]], 'nodes': [{'id': 1, 'label': None, 'attribute': 'shape', 'entropy': 0.8112781244591328, 'samples': 8, 'classCounts': [['+', 6], ['-', 2]], 'nodes': [{'id': 2, 'label': None, 'attribute': 'color', 'entropy': 0.9182958340544896, 'samples': 6, 'classCounts': [['+', 4], ['-', 2]], 'nodes': [{'id': 3, 'label': array(['+'], dtype='<U1'), 'attribute': None, 'entropy': 0, 'samples': 5, 'classCounts': [['+', 4], ['-', 1]], 'nodes': []}, {'id': 4, 'label': array(['-'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 1, 'classCounts': [['-', 1]], 'nodes': []}, {'id': 5, 'label': array(['+'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 0, 'classCounts': None, 'nodes': None}]}, {'id': 6, 'label': array(['+'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 2, 'classCounts': [['+', 2]], 'nodes': []}]}, {'id': 7, 'label': None, 'attribute': 'shape', 'entropy': 0.954434002924965, 'samples': 8, 'classCounts': [['+', 5], ['-', 3]], 'nodes': [{'id': 8, 'label': None, 'attribute': 'color', 'entropy': 0.9182958340544896, 'samples': 6, 'classCounts': [['+', 2], ['-', 4]], 'nodes': [{'id': 9, 'label': array(['-'], dtype='<U1'), 'attribute': None, 'entropy': 0, 'samples': 6, 'classCounts': [['+', 2], ['-', 4]], 'nodes': []}, {'id': 10, 'label': array(['-'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 0, 'classCounts': None, 'nodes': None}, {'id': 11, 'label': array(['-'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 0, 'classCounts': None, 'nodes': None}]}, {'id': 12, 'label': None, 'attribute': 'color', 'entropy': 1.0, 'samples': 2, 'classCounts': [['+', 1], ['-', 1]], 'nodes': [{'id': 13, 'label': array(['+'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 1, 'classCounts': [['+', 1]], 'nodes': []}, {'id': 14, 'label': array(['-'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 1, 'classCounts': [['-', 1]], 'nodes': []}, {'id': 15, 'label': array(['+', '-'], dtype='<U1'), 'attribute': None, 'entropy': None, 'samples': 0, 'classCounts': None, 'nodes': None}]}]}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'testTree.pdf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes, classes, data, target, data2, target2 = td.ToyData().get_data()\n",
    "id3 = ID3.ID3DecisionTreeClassifier()\n",
    "myTree = id3.fit(data, target, attributes, classes)\n",
    "print(myTree)\n",
    "plot = id3.make_dot_data()\n",
    "plot.render(\"testTree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (clf, metrics.classification_report(target2, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(target2, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
