{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "\n",
    "## Objective\n",
    "In and after this lab session you will\n",
    "\n",
    "1. train a Gaussian NBC with the EM algorithm\n",
    "2. compare the results you get to those of the k-Means clustering provided in SciKitLearn\n",
    "3. discuss the classifiers from this lab session and those from the previous session (supervised learning of NBCs) in a brief report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Tools\n",
    "The EM (Expectation - Maximisation) algorithm solves the problem of not being able to compute the Maximum Likelihood Estimates for unknown classes directly by iterating over two steps until there is no significant change in step 2 observable:\n",
    "\n",
    "1. Compute the expected outcome for each example / sample given estimates for priors and distribution (essentially, the likelihoods for observing the sample assuming an estimated distribution).  \n",
    "2. Compute new estimates for your priors and distributions (in the case of a Gaussian NBC, new means and variances are needed) based on the estimated expected values for how much each sample belongs to the respective distribution.\n",
    "\n",
    "You can find the algorithm stated explicitly as given in Murphy, \"Machine Learning - A probabilistic perspective\", pp 352 - 353 HERE (Link to come).\n",
    "\n",
    "One special case of the EM algorithm is k-Means clustering, for which an implementation can be found in SciKitLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your implementation task\n",
    "1. Implement the EM-algorithm to find a Gaussian NBC for the digits dataset from SciKitLearn (you can of course also use the MNIST_Light set from Lab 5, but for initial tests the digits data set is more convenient, since it is smaller in various aspects). You may assume (conditional) independence between the attributes, i.e., the covariances can be assumed to be simply the variances over each attribute. Split the data set in 70% training and 30% test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SciKitLearn Digits <a name='sklearn1'></a>\n",
    "1. SciKitLearn digits: just load it and use it as is. Use 70% of the data for training and 30% for testing. Run all of the four classifiers and compare and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X, y = load_digits(n_class=10, return_X_y=True)\n",
    "X= min_max_scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principal_df = pd.DataFrame(data = principalComponents, \n",
    "                            columns = ['PC1', 'PC2', 'PC3'])\n",
    "target_df = pd.DataFrame(data=y, columns=['Target'])\n",
    "final_df = pd.concat([principal_df, target_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b31619775782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"browser\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PC1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PC2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PC3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "pio.renderers.default = \"browser\"\n",
    "fig = px.scatter_3d(final_df, x='PC1', y='PC2', z='PC3', color='Target')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def EM_algorithm(X, k_classes, N=200,  eps=5e-2):\n",
    "    \n",
    "    def E_step(X, k_classes, mu, sig, prior):\n",
    "        r = np.zeros([len(X), k_classes])\n",
    "        for i, xi in enumerate(X):\n",
    "            num = np.prod([norm.pdf(xi, mu[kcl], np.sqrt(sig[kcl])) for kcl in range(k_classes)], axis=1)\n",
    "            num = [prior[kcl]*num[kcl] for kcl in range(k_classes)]\n",
    "            den = np.sum(num)\n",
    "            r[i, :] = num/den            \n",
    "        return r\n",
    "\n",
    "    def M_step(X, r):\n",
    "        N, k_classes = r.shape\n",
    "        prior = {kcl:np.sum(r[:, kcl])/N for kcl in range(k_classes)}\n",
    "        mu = {}\n",
    "        sig = {}\n",
    "        \n",
    "        for kcl in range(k_classes):\n",
    "            mu[kcl] = np.sum([r[i, kcl]*xi for i, xi in enumerate(X)], axis=0)/np.sum(r[:, kcl])\n",
    "            sig[kcl] = np.sum([r[i, kcl]*np.diag(np.outer(xi, xi)) for i, xi in enumerate(X)], axis= 0)/np.sum(r[:, kcl]) - np.diag(np.outer(mu[kcl], mu[kcl])) + eps\n",
    "        return mu, sig, prior\n",
    "    \n",
    "    \n",
    "    # Initial values\n",
    "    mu = {kcl:np.random.uniform(1, size=X[0].size) for kcl in range(k_classes)}\n",
    "    sig = {kcl:np.random.uniform(0.5, size=X[0].size) for kcl in range(k_classes)}\n",
    "    prior = {kcl:1/k_classes for kcl in range(k_classes)} #Assuming equal distribution\n",
    "    \n",
    "    for iteration in range(N):\n",
    "        r = E_step(X_train, k_classes, mu, sig, prior)\n",
    "        mu, sig, prior = M_step(X_train, r)\n",
    "        \n",
    "    class GNB_classifier:\n",
    "        def __init__(self, mu, sig, prior, k_classes, eps):\n",
    "            self.mu = mu\n",
    "            self.sig = sig\n",
    "            self.prior = prior\n",
    "            self.eps = eps\n",
    "            self.classes = k_classes\n",
    "        \n",
    "        \n",
    "        def predict(self, X):\n",
    "            y_pred = []\n",
    "            for x in X:\n",
    "                prob = [self._posterior(x, kcl) for kcl in range(self.classes)]\n",
    "                y_pred.append(np.argmax(prob))\n",
    "            return np.asarray(y_pred)\n",
    "    \n",
    "    \n",
    "        def _posterior(self, x, ci):\n",
    "            return self.prior[ci]*np.prod([norm.pdf(x[i], self.mu[ci][i], self.sig[ci][i] + self.eps) for i in range(len(x))])\n",
    "    \n",
    "    return GNB_classifier(mu, sig, prior, k_classes, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the result of the EM-algorithm (the found distribution parameters) to cluster the training data (essentially, using the resulting classifier to do a prediction over the training data). Produce a confusion matrix over the known labels for the training data and your EM-generated clusters. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity Score:  0.7268138846521567\n",
      "Completeness Score:  0.7409117116451881\n",
      "V-measure:  0.7337950917850213\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "gnb_clf = EM_algorithm(X_train, k_classes=10, N=200)\n",
    "gnb_pred = gnb_clf.predict(X_test)\n",
    "measure = metrics.homogeneity_completeness_v_measure(y_test, gnb_pred)\n",
    "\n",
    "print(\"Homogeneity Score: \", measure[0])\n",
    "print(\"Completeness Score: \", measure[1])\n",
    "print(\"V-measure: \", measure[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If necessary, find a way to \"repair\" the cluster assignments so that you can do a prediction run over the test data, from which you can compare the results with your earlier implementation of the Gaussian NBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(X_train, y_train, y_pred):\n",
    "    k_classes = list(set(y_train))\n",
    "    c_true = {}\n",
    "    c_pred = {}\n",
    "    for kcl in k_classes:\n",
    "        c_true[kcl] = np.mean([X for i, X in enumerate(X_train) if y_train[i] == kcl], axis=0)\n",
    "        c_pred[kcl] = np.mean([X for i, X in enumerate(X_train) if y_pred[i] == kcl], axis=0)\n",
    "\n",
    "    pred2true = {}\n",
    "    for kcl in k_classes:\n",
    "        pred2true[kcl] = np.argmin([np.linalg.norm(c_pred[kcl] - c_true_v) for c_true_v in c_true.values()])\n",
    "    \n",
    "    y_new = []\n",
    "    for y in y_pred:\n",
    "        y_new.append(pred2true[y])\n",
    "    return np.asarray(y_new)\n",
    "\n",
    "\n",
    "def repair_ind(X_train, y_train, y_pred):\n",
    "    k_classes = list(set(y_train))\n",
    "    pred2true = {}\n",
    "    for kcl in k_classes:\n",
    "        indices = [i for i, x in enumerate(X_train) if y_pred[i] == kcl]\n",
    "        unique, counts= np.unique(y_train[indices], return_counts=True)\n",
    "        pred2true[kcl] = unique[np.argmax(counts)]\n",
    "        \n",
    "    y_new = []\n",
    "    for y in y_pred:\n",
    "        y_new.append(pred2true[y])\n",
    "    return np.asarray(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_pred = repair(X_test, y_test, gnb_pred)\n",
    "em_pred2 = repair_ind(X_test, y_test, gnb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity Score:  0.7237721908319448\n",
      "Completeness Score:  0.7752544056226649\n",
      "V-measure:  0.748629251724719\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        53\n",
      "           1       0.54      0.80      0.65        50\n",
      "           2       0.80      0.87      0.84        47\n",
      "           3       0.00      0.00      0.00        54\n",
      "           4       0.95      0.90      0.92        60\n",
      "           5       0.94      0.68      0.79        66\n",
      "           6       0.98      0.98      0.98        53\n",
      "           7       0.88      0.93      0.90        55\n",
      "           8       0.61      0.51      0.56        43\n",
      "           9       0.42      0.80      0.55        59\n",
      "\n",
      "    accuracy                           0.74       540\n",
      "   macro avg       0.71      0.74      0.72       540\n",
      "weighted avg       0.72      0.74      0.72       540\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[50  0  0  0  2  1  0  0  0  0]\n",
      " [ 0 40 10  0  0  0  0  0  0  0]\n",
      " [ 0  0 41  0  0  0  0  0  6  0]\n",
      " [ 0  1  0  0  0  0  0  2  5 46]\n",
      " [ 0  4  0  0 54  0  0  2  0  0]\n",
      " [ 0  0  0  0  1 45  1  0  0 19]\n",
      " [ 0  0  0  0  0  0 52  0  1  0]\n",
      " [ 0  4  0  0  0  0  0 51  0  0]\n",
      " [ 0 19  0  0  0  1  0  0 22  1]\n",
      " [ 0  6  0  0  0  1  0  3  2 47]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinma/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "measure = metrics.homogeneity_completeness_v_measure(y_test, em_pred)\n",
    "\n",
    "print(\"Homogeneity Score: \", measure[0])\n",
    "print(\"Completeness Score: \", measure[1])\n",
    "print(\"V-measure: \", measure[2])\n",
    "print('\\n')\n",
    "print(\"Classification report:\\n%s\\n\"\n",
    "      % (metrics.classification_report(y_test, em_pred)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, em_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity Score:  0.7237721908319448\n",
      "Completeness Score:  0.7752544056226649\n",
      "V-measure:  0.748629251724719\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        53\n",
      "           1       0.54      0.80      0.65        50\n",
      "           2       0.80      0.87      0.84        47\n",
      "           3       0.00      0.00      0.00        54\n",
      "           4       0.95      0.90      0.92        60\n",
      "           5       0.94      0.68      0.79        66\n",
      "           6       0.98      0.98      0.98        53\n",
      "           7       0.88      0.93      0.90        55\n",
      "           8       0.61      0.51      0.56        43\n",
      "           9       0.42      0.80      0.55        59\n",
      "\n",
      "    accuracy                           0.74       540\n",
      "   macro avg       0.71      0.74      0.72       540\n",
      "weighted avg       0.72      0.74      0.72       540\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[50  0  0  0  2  1  0  0  0  0]\n",
      " [ 0 40 10  0  0  0  0  0  0  0]\n",
      " [ 0  0 41  0  0  0  0  0  6  0]\n",
      " [ 0  1  0  0  0  0  0  2  5 46]\n",
      " [ 0  4  0  0 54  0  0  2  0  0]\n",
      " [ 0  0  0  0  1 45  1  0  0 19]\n",
      " [ 0  0  0  0  0  0 52  0  1  0]\n",
      " [ 0  4  0  0  0  0  0 51  0  0]\n",
      " [ 0 19  0  0  0  1  0  0 22  1]\n",
      " [ 0  6  0  0  0  1  0  3  2 47]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinma/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "measure = metrics.homogeneity_completeness_v_measure(y_test, em_pred2)\n",
    "\n",
    "print(\"Homogeneity Score: \", measure[0])\n",
    "print(\"Completeness Score: \", measure[1])\n",
    "print(\"V-measure: \", measure[2])\n",
    "print('\\n')\n",
    "print(\"Classification report:\\n%s\\n\"\n",
    "      % (metrics.classification_report(y_test, em_pred2)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, em_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use now also the k-Means implementation from SciKitLearn and compare the results to yours (they should be similar at least in the sense that there are classes that are more clearly separated from the rest than others for both approaches). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity Score:  0.7561055338287734\n",
      "Completeness Score:  0.8081139384915079\n",
      "V-measure:  0.7812451278991431\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        53\n",
      "           1       0.77      0.80      0.78        50\n",
      "           2       0.79      0.87      0.83        47\n",
      "           3       0.00      0.00      0.00        54\n",
      "           4       0.96      0.90      0.93        60\n",
      "           5       0.94      0.68      0.79        66\n",
      "           6       0.96      0.98      0.97        53\n",
      "           7       0.87      1.00      0.93        55\n",
      "           8       0.82      0.86      0.84        43\n",
      "           9       0.41      0.81      0.55        59\n",
      "\n",
      "    accuracy                           0.79       540\n",
      "   macro avg       0.75      0.79      0.76       540\n",
      "weighted avg       0.75      0.79      0.76       540\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[52  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 40 10  0  0  0  0  0  0  0]\n",
      " [ 0  0 41  0  0  0  0  0  4  2]\n",
      " [ 0  0  1  0  0  0  0  2  4 47]\n",
      " [ 0  4  0  0 54  0  0  2  0  0]\n",
      " [ 0  0  0  0  1 45  2  0  0 18]\n",
      " [ 1  0  0  0  0  0 52  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 55  0  0]\n",
      " [ 0  3  0  0  0  1  0  0 37  2]\n",
      " [ 0  5  0  0  0  2  0  4  0 48]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinma/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_clf = KMeans(n_clusters=10, random_state=42).fit(X_train)\n",
    "km_pred = km_clf.predict(X_test)\n",
    "km_pred = repair(X_test, y_test, km_pred)\n",
    "\n",
    "measure = metrics.homogeneity_completeness_v_measure(y_test, km_pred)\n",
    "\n",
    "print(\"Homogeneity Score: \", measure[0])\n",
    "print(\"Completeness Score: \", measure[1])\n",
    "print(\"V-measure: \", measure[2])\n",
    "print('\\n')\n",
    "print(\"Classification report:\\n%s\\n\"\n",
    "      % (metrics.classification_report(y_test, km_pred)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, km_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
